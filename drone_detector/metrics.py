# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/10_metrics.ipynb (unless otherwise specified).

__all__ = ['adjusted_R2Score', 'rrmse', 'bias', 'bias_pct', 'label_ranking_average_precision_score',
           'label_ranking_loss', 'one_error', 'coverage_error', 'poly_IoU', 'is_true_positive', 'is_false_positive',
           'average_precision', 'average_recall']

# Cell
from .imports import *
from fastai.learner import Metric
from fastai.torch_core import *
from fastai.metrics import *
from fastai.losses import BaseLoss
import sklearn.metrics as skm
import torch
import torch.nn.functional as F

# Cell
mk_class('ActivationType', **{o:o.lower() for o in ['No', 'Sigmoid', 'Softmax', 'BinarySoftmax']},
         doc="All possible activation classes for `AccumMetric")

# Cell

def adjusted_R2Score(r2_score, n, k):
    "Calculates adjusted_R2Score based on r2_score, number of observations (n) and number of predictor variables(k)"
    return 1 - (((n-1)/(n-k-1)) * (1 - r2_score))


# Cell

def _rrmse(inp, targ):
    "RMSE normalized with mean of the target"
    return torch.sqrt(F.mse_loss(inp, targ)) / targ.mean() * 100

rrmse = AccumMetric(_rrmse)
rrmse.__doc__ = "Target mean weighted rmse"

# Cell
def _bias(inp, targ):
    "Average bias of predictions"
    inp, targ = flatten_check(inp, targ)
    return (inp - targ).sum() / len(targ)

bias = AccumMetric(_bias)
bias.__doc__ = "Average bias of predictions"

# Cell
def _bias_pct(inp, targ):
    "Mean weighted bias"
    inp, targ = flatten_check(inp, targ)
    return 100 * ((inp-targ).sum()/len(targ)) / targ.mean()

bias_pct = AccumMetric(_bias_pct)
bias_pct.__doc__ = 'Mean weighted bias'

# Cell

def label_ranking_average_precision_score(sigmoid=True, sample_weight=None):
    """Label ranking average precision (LRAP) is the average over each ground truth label assigned to each sample,
    of the ratio of true vs. total labels with lower score."""
    activation = ActivationType.Sigmoid if sigmoid else ActivationType.No
    return skm_to_fastai(skm.label_ranking_average_precision_score, sample_weight=None, flatten=False, thresh=None,
                         activation=activation)

# Cell

def label_ranking_loss(sigmoid=True, sample_weight=None):
    """Compute the average number of label pairs that are incorrectly ordered given y_score
    weighted by the size of the label set and the number of labels not in the label set."""
    activation = ActivationType.Sigmoid if sigmoid else ActivationType.No
    return skm_to_fastai(skm.label_ranking_loss, sample_weight=None, flatten=False, thresh=None,
                         activation=activation)

# Cell

def _one_error(inp, targ):
    max_ranks = inp.argmax(axis=1)
    faults = 0
    for i in range_of(max_ranks):
        faults += targ[i,max_ranks[i]]
    return 1 - torch.true_divide(faults, len(max_ranks))

one_error = AccumMetric(_one_error, flatten=False)
one_error.__doc__ = "Rate for which the top ranked label is not among ground truth"

# Cell

def coverage_error(sigmoid=True, sample_weight=None):
    """Compute how far we need to go through the ranked scores to cover all true labels.
    The best value is equal to the average number of labels in y_true per sample."""

    activation = ActivationType.Sigmoid if sigmoid else ActivationType.No
    return skm_to_fastai(skm.coverage_error, sample_weight=None, flatten=False, thresh=None, activation=activation)

# Cell

def poly_IoU(poly_1:Polygon, poly_2:Polygon) -> float:
    "IoU for polygons"
    area_intersection = poly_1.intersection(poly_2).area
    area_union = poly_1.union(poly_2).area
    iou = area_intersection / area_union
    return iou

# Cell
def is_true_positive(row, results:gpd.GeoDataFrame, res_sindex:gpd.sindex):
    "Check if a single ground truth mask is TP or FN with 11 different IoU thresholds"
    iou_threshs = np.arange(0.5, 1.04, 0.05)

    # Matching predictions using spatial index
    c = list(res_sindex.intersection(row.geometry.bounds))
    possible_matches = results.iloc[c].copy()

    # No masks -> False negative
    if len(possible_matches) == 0: return ['FN'] * len(iou_threshs)

    possible_matches['iou'] = possible_matches.apply(lambda pred: poly_IoU(pred.geometry, row.geometry), axis=1)

    retvals = []

    for i, iou_thresh in enumerate(iou_threshs):
        iou_thresh = np.round(iou_thresh, 2)
        possible_matches = possible_matches[possible_matches.iou >= iou_thresh]
        if len(possible_matches) == 0: return retvals + ['FN'] * (len(iou_threshs)-len(retvals))

        possible_matches.reset_index(inplace=True, drop=True)
        max_iou_ix = possible_matches['iou'].idxmax()
        max_score_id = possible_matches['score'].idxmax()

        if possible_matches.iloc[max_iou_ix].iou < iou_thresh: return ['FN'] * (len(iou_threshs) - len(retvals))

        if possible_matches.iloc[max_iou_ix].label != row.label: return ['FN'] * (len(iou_threshs) - len(retvals))

        retvals.append('TP')
    return retvals

# Cell

def is_false_positive(row, ground_truth:gpd.GeoDataFrame, gt_sindex:gpd.sindex,
                      results:gpd.GeoDataFrame, res_sindex:gpd.sindex):
    "Check if prediction is FP or TP for 11 different IoU thresholds"

    iou_threshs = np.arange(0.5, 1.04, 0.05)

    # First find out the matching ground truth masks
    c = list(gt_sindex.intersection(row.geometry.bounds))
    possible_gt_matches = ground_truth.iloc[c].copy()
    #possible_gt_matches = possible_matches[possible_matches.label == row.label].copy()
    possible_gt_matches = possible_gt_matches.loc[possible_gt_matches.intersects(row.geometry)]
    possible_gt_matches.reset_index(inplace=True)

    # No ground truth masks -> false positive
    if len(possible_gt_matches) == 0: return ['FP'] * len(iou_threshs)

    retvals = []

    # Count IoU for all possible_gt_matches
    possible_gt_matches['iou'] = possible_gt_matches.apply(lambda gt: poly_IoU(gt.geometry, row.geometry), axis=1)

    # Assume that largest IoU is the corresponding label
    gt_ix = possible_gt_matches['iou'].idxmax()

    for i, iou_thresh in enumerate(iou_threshs):
        iou_thresh = np.round(iou_thresh, 2)
        # If IoU-threshold is too low -> false positive
        if possible_gt_matches.iloc[gt_ix].iou < iou_thresh:
            return retvals + ['FP'] * (len(iou_threshs)-len(retvals))


        # If labels don't match -> false positive:
        if possible_gt_matches.iloc[gt_ix].label != row.label:
            return retvals + ['FP'] * (len(iou_threshs)-len(retvals))

        # Then check whether there are other predictions
        c = list(res_sindex.intersection(row.geometry.bounds))
        possible_pred_matches = results.iloc[c].copy()

        # Remove examined row from possible_matches. Assume that scores are always different (spoiler: they are not)
        possible_pred_matches = possible_pred_matches[possible_pred_matches.score != row.score]

        # No other possibilities -> not FP
        if len(possible_pred_matches) == 0:
            retvals.append('TP')
            continue

        possible_pred_matches['iou'] = possible_pred_matches.apply(lambda pred: poly_IoU(pred.geometry,
                                                                                         possible_gt_matches.iloc[gt_ix].geometry),
                                                                   axis=1)

        possible_pred_matches = possible_pred_matches[possible_pred_matches.iou > iou_thresh]
        possible_pred_matches.reset_index(inplace=True)

        if len(possible_pred_matches) == 0:
            retvals.append('TP')
            continue

        pred_max_iou_ix = possible_pred_matches['iou'].idxmax()
        pred_max_score_ix = possible_pred_matches['score'].idxmax()

        # Do any other possible predictions have larger score? If yes -> FP
        if possible_pred_matches.iloc[pred_max_score_ix].score > row.score:
            return retvals + ['FP'] * (len(iou_threshs)-len(retvals))

        retvals.append('TP')

    return retvals

# Cell

def average_precision(ground_truth:gpd.GeoDataFrame, preds:gpd.GeoDataFrame) -> dict:
    "Get 11-point AP score for each label separately and with all iou_thresholds"

    # Clip geodataframes so that they cover the same area
    preds = gpd.clip(preds, box(*ground_truth.total_bounds), keep_geom_type=True)
    ground_truth = gpd.clip(ground_truth, box(*preds.total_bounds), keep_geom_type=True)

    gt_sindex = ground_truth.sindex
    pred_sindex = preds.sindex
    fp_cols = [f'FP_{np.round(i, 2)}' for i in np.arange(0.5, 1.04, 0.05)]
    preds[fp_cols] = preds.apply(lambda row: is_false_positive(row, ground_truth, gt_sindex, preds, pred_sindex),
                                 axis=1, result_type='expand')
    iou_threshs = np.arange(0.5, 1.04, 0.05)

    res_dict = {}
    for l in preds.label.unique():
        for iou_thresh in iou_threshs:
            iou_thresh = np.round(iou_thresh, 2)
            res_dict[f'{l}_pre_{iou_thresh}'] = []
            temp_preds = preds[preds.label == l].copy()
            num_correct = len(ground_truth[ground_truth.label == l])
            temp_preds.sort_values(by='score', ascending=False, inplace=True)
            temp_preds.reset_index(inplace=True)
            temp_preds['cumul_TP'] = 0.
            temp_preds['precision'] = 0.
            temp_preds['recall'] = 0.
            temp_preds.loc[0, 'cumul_TP'] = 0 if temp_preds.loc[0, f'FP_{iou_thresh}'] == 'FP' else 1
            temp_preds.loc[0, 'precision'] = temp_preds.loc[0,'cumul_TP'] / 1
            temp_preds.loc[0, 'recall'] = temp_preds.loc[0,'cumul_TP'] / num_correct
            for i in range(1, len(temp_preds)):
                row_tp = 0 if temp_preds.loc[i, f'FP_{iou_thresh}'] == 'FP' else 1
                temp_preds.loc[i, 'cumul_TP'] = temp_preds.loc[i-1, 'cumul_TP'] + row_tp
                temp_preds.loc[i, 'precision'] = temp_preds.loc[i,'cumul_TP'] / (i+1)
                temp_preds.loc[i, 'recall'] = temp_preds.loc[i,'cumul_TP'] / num_correct
            recall_threshs = np.arange(0,1.04, 0.1)
            for rec_thresh in recall_threshs:
                pre = temp_preds[temp_preds.recall >= rec_thresh].precision.max()
                res_dict[f'{l}_pre_{iou_thresh}'].append(0 if not np.isfinite(pre) else pre)
    return res_dict

# Cell

def average_recall(ground_truth:gpd.GeoDataFrame, preds:gpd.GeoDataFrame, max_detections:int=None) -> dict:
    """Get 11-point AR score for each label separately and with all iou_thresholds.
    If `max_detections` is not `None` evaluate with only that most confident predictions
    Seems to be still bugged, needs fixing
    """

    # Clip geodataframes so that they cover the same area
    preds = gpd.clip(preds, box(*ground_truth.total_bounds), keep_geom_type=True)
    ground_truth = gpd.clip(ground_truth, box(*preds.total_bounds), keep_geom_type=True)

    tp_cols = [f'TP_{np.round(i, 2)}' for i in np.arange(0.5, 1.03, 0.05)]
    if max_detections is not None:
        preds.sort_values(by='score', ascending=False, inplace=True)
        preds = preds[:max_detections]
        preds.reset_index(inplace=True)
    pred_sindex = preds.sindex
    ground_truth[tp_cols] = ground_truth.apply(lambda row: is_true_positive(row, preds, pred_sindex),
                                               axis=1, result_type='expand')
    iou_threshs = np.arange(0.5, 1.04, 0.05)
    res_dict = {}
    for l in ground_truth.label.unique():
        res_dict[f'{l}_rec'] = []
        for iou_thresh in iou_threshs:
            iou_thresh = np.round(iou_thresh, 2)
            temp_gt = ground_truth[ground_truth.label == l].copy()
            res_dict[f'{l}_rec'].append(len(temp_gt[temp_gt[f'TP_{iou_thresh}'] == 'TP']) / len(temp_gt))
    return res_dict