[
  {
    "objectID": "14_processing.yolo.html",
    "href": "14_processing.yolo.html",
    "title": "yolo utilities",
    "section": "",
    "text": "Conversion between geospatial data and yolo-format\n\nSpecifications of yolo format:\n\nThe folder structure must be so that images are in a folder called images and annotations in a folder labels\nEach image must have a corresponding annotation file with a same name aside from file type (txt)\nEach txt contains all annotations in separate rows\n\nBounding box annotation format is classid x_center y_center width height\nPolygon annotation format classid x y x y…\n\nCoordinates are normalized between 0 and 1, so that origin is at upper left and (1,1) in bottom right\nTrain/val/test -sets are collated in separate files, with the paths to image files\nInformation is collated on a yaml file, where\n\npath: &lt;path&gt; is the dataset root dir\ntrain:, val: and test: are either:\n\ndirectories\ntxt-files containing images\nlist containing paths\n\nclass names are saved like\n\nnames: \n    0: person\n    1: bicycle\n\n\nsource\n\nYOLOProcessor\n\n YOLOProcessor (data_path:str, outpath:str, names:list)\n\nHandles transformations from GIS-polygons to YOLOv8-format and backwards\nYOLOProcessor converts data from either shp or geojson to YOLOv5 format.\nFirst bounding boxes.\n\ncats = ['Standing', 'Fallen']\n\noutpath = Path('example_data/tiles/')\nyolo_proc = YOLOProcessor(outpath, outpath, cats)\nyolo_proc.from_shp('label', ann_format='box')\n\n\n\n\n\ndef plot_bounding_box(image, annotation_list, classes):\n    annotations = np.array(annotation_list)\n    w, h = image.size\n    plotted_image = ImageDraw.Draw(image)\n\n    transformed_annotations = np.copy(annotations)\n    transformed_annotations[:,[1,3]] = annotations[:,[1,3]] * w\n    transformed_annotations[:,[2,4]] = annotations[:,[2,4]] * h \n    \n    transformed_annotations[:,1] = transformed_annotations[:,1] - (transformed_annotations[:,3] / 2)\n    transformed_annotations[:,2] = transformed_annotations[:,2] - (transformed_annotations[:,4] / 2)\n    transformed_annotations[:,3] = transformed_annotations[:,1] + transformed_annotations[:,3]\n    transformed_annotations[:,4] = transformed_annotations[:,2] + transformed_annotations[:,4]\n        \n    for ann in transformed_annotations:\n        obj_cls, x0, y0, x1, y1 = ann\n        plotted_image.rectangle(((x0,y0), (x1,y1)))\n        \n        plotted_image.text((x0, y0 - 10), classes[int(obj_cls)])\n    \n    plt.imshow(np.array(image))\n    plt.show()\n\n    \n# Get any random annotation file \nannotation_file = random.choice(os.listdir(outpath/'labels'))\nwith open(outpath/'labels'/annotation_file, \"r\") as file:\n    annotation_list = file.read().split(\"\\n\")[:-1]\n    annotation_list = [x.split(\" \") for x in annotation_list]\n    annotation_list = [[float(y) for y in x ] for x in annotation_list]\n\n#Get the corresponding image file\nimage_file = annotation_file.replace(\"txt\", \"tif\")\nassert os.path.exists(outpath/'images'/image_file)\n\n#Load the image\nimage = Image.open(outpath/'images'/image_file)\n\n#Plot the Bounding Box\nplot_bounding_box(image, annotation_list, {0:'Standing', 1:'Fallen'})\n\n\n\n\nThen to polygon.\n\ncats = ['Standing', 'Fallen']\n\noutpath = Path('example_data/tiles/')\nyolo_proc = YOLOProcessor(outpath, outpath, cats)\nyolo_proc.from_shp('label', ann_format='polygon')\n\n\n\n\n\ndef plot_polygon(image, annotation_list, classes):\n    w, h = image.size\n    plotted_image = ImageDraw.Draw(image)\n        \n    for ann in annotation_list:\n        obj_cls = ann[0]\n        xcoords = [ann[i]*w for i in range(1, len(ann), 2)]\n        ycoords = [ann[i]*h for i in range(2, len(ann)+1, 2)]\n        xcoords.append(xcoords[0])\n        ycoords.append(ycoords[0])\n        plotted_image.polygon(list(zip(xcoords, ycoords)))\n        \n        plotted_image.text((xcoords[0], ycoords[0] - 10), classes[int(obj_cls)])\n    \n    plt.imshow(np.array(image))\n    plt.show()\n\n\n# Get any random annotation file \nannotation_file = random.choice(os.listdir(outpath/'labels'))\nwith open(outpath/'labels'/annotation_file, \"r\") as file:\n    annotation_list = file.read().split(\"\\n\")[:-1]\n    annotation_list = [x.split(\" \") for x in annotation_list]\n    annotation_list = [[float(y) for y in x ] for x in annotation_list]\n\n#Get the corresponding image file\nimage_file = annotation_file.replace(\"txt\", \"tif\")\nassert os.path.exists(outpath/'images'/image_file)\n\n#Load the image\nimage = Image.open(outpath/'images'/image_file)\n\n#Plot the Bounding Box\nplot_polygon(image, annotation_list, {0:'Standing', 1:'Fallen'})\n\n\n\n\nRotated bounding boxes are saved as polygon-style annotations.\n\ncats = ['Standing', 'Fallen']\n\noutpath = Path('example_data/tiles/')\nyolo_proc = YOLOProcessor(outpath, outpath, cats)\nyolo_proc.from_shp('label', ann_format='rotated box')\n\n\n\n\n\n# Get any random annotation file \nannotation_file = random.choice(os.listdir(outpath/'labels'))\nwith open(outpath/'labels'/annotation_file, \"r\") as file:\n    annotation_list = file.read().split(\"\\n\")[:-1]\n    annotation_list = [x.split(\" \") for x in annotation_list]\n    annotation_list = [[float(y) for y in x ] for x in annotation_list]\n\n#Get the corresponding image file\nimage_file = annotation_file.replace(\"txt\", \"tif\")\nassert os.path.exists(outpath/'images'/image_file)\n\n#Load the image\nimage = Image.open(outpath/'images'/image_file)\n\n#Plot the Bounding Box\nplot_polygon(image, annotation_list, {0:'Standing', 1:'Fallen'})"
  },
  {
    "objectID": "processing.postprocessing.html",
    "href": "processing.postprocessing.html",
    "title": "Postprocessing",
    "section": "",
    "text": "First the commonly used NMS with bounding boxes, that prioritizes either confidence score (default) or bounding box area.\n\nsource\n\n\n\n non_max_suppression_fast (boxes, scores, overlap_thresh:float,\n                           sort_criterion:str='score')\n\nPossibility to sort boxes by score (default) or area\nNon-max suppression can in theory be applied also on polygons, but it hasn’t been used in any publications as far as I know.\nIf non_max_suppression_poly is used to eliminate polygons, threshold might need to be smaller than typical value of 0.7 that is used.\n\nsource\n\n\n\n\n non_max_suppression_poly (geoms, scores, overlap_thresh:float,\n                           sort_criterion:str='score')\n\nPossibility to sort geoms by score (default) or area\nSome utils to run above functions to GeoDataFrames\n\nsource\n\n\n\n\n do_min_rot_rectangle_nms (gdf:geopandas.geodataframe.GeoDataFrame,\n                           nms_thresh=0.7, crit='score')\n\n\nsource\n\n\n\n\n do_poly_nms (gdf:geopandas.geodataframe.GeoDataFrame, nms_thresh=0.1,\n              crit='score')\n\n\nsource\n\n\n\n\n do_nms (gdf:geopandas.geodataframe.GeoDataFrame, nms_thresh=0.7,\n         crit='score')"
  },
  {
    "objectID": "processing.postprocessing.html#non-maximum-suppression",
    "href": "processing.postprocessing.html#non-maximum-suppression",
    "title": "Postprocessing",
    "section": "",
    "text": "First the commonly used NMS with bounding boxes, that prioritizes either confidence score (default) or bounding box area.\n\nsource\n\n\n\n non_max_suppression_fast (boxes, scores, overlap_thresh:float,\n                           sort_criterion:str='score')\n\nPossibility to sort boxes by score (default) or area\nNon-max suppression can in theory be applied also on polygons, but it hasn’t been used in any publications as far as I know.\nIf non_max_suppression_poly is used to eliminate polygons, threshold might need to be smaller than typical value of 0.7 that is used.\n\nsource\n\n\n\n\n non_max_suppression_poly (geoms, scores, overlap_thresh:float,\n                           sort_criterion:str='score')\n\nPossibility to sort geoms by score (default) or area\nSome utils to run above functions to GeoDataFrames\n\nsource\n\n\n\n\n do_min_rot_rectangle_nms (gdf:geopandas.geodataframe.GeoDataFrame,\n                           nms_thresh=0.7, crit='score')\n\n\nsource\n\n\n\n\n do_poly_nms (gdf:geopandas.geodataframe.GeoDataFrame, nms_thresh=0.1,\n              crit='score')\n\n\nsource\n\n\n\n\n do_nms (gdf:geopandas.geodataframe.GeoDataFrame, nms_thresh=0.7,\n         crit='score')"
  },
  {
    "objectID": "processing.postprocessing.html#smoothing-and-filling-holes",
    "href": "processing.postprocessing.html#smoothing-and-filling-holes",
    "title": "Postprocessing",
    "section": "Smoothing and filling holes",
    "text": "Smoothing and filling holes\nBelow functions are run before converting IceVision preds to COCO or shapefile format.\n\nsource\n\ndilate_erode\n\n dilate_erode (preds:list)\n\nRun dilation followed by erosion in order to smooth masks\n\nsource\n\n\nfill_holes\n\n fill_holes (preds:list)\n\nRun binary_fill_holes to predicted binary masks"
  },
  {
    "objectID": "metrics.html",
    "href": "metrics.html",
    "title": "Custom losses and metrics",
    "section": "",
    "text": "source\n\n\n\n adjusted_R2Score (r2_score, n, k)\n\nCalculates adjusted_R2Score based on r2_score, number of observations (n) and number of predictor variables(k)"
  },
  {
    "objectID": "metrics.html#classification-metrics",
    "href": "metrics.html#classification-metrics",
    "title": "Custom losses and metrics",
    "section": "",
    "text": "source\n\n\n\n adjusted_R2Score (r2_score, n, k)\n\nCalculates adjusted_R2Score based on r2_score, number of observations (n) and number of predictor variables(k)"
  },
  {
    "objectID": "metrics.html#regression-metrics",
    "href": "metrics.html#regression-metrics",
    "title": "Custom losses and metrics",
    "section": "Regression metrics",
    "text": "Regression metrics\n\n\nrrmse\n\n rrmse (preds, targs)\n\nRelative RMSE. Normalized with mean of the target\n\n\n\nbias\n\n bias (preds, targs)\n\nAverage bias of predictions\n\n\n\nbias_pct\n\n bias_pct (preds, targs)\n\nMean weighted bias, normalized with mean of the target"
  },
  {
    "objectID": "metrics.html#bigearthnet-metrics",
    "href": "metrics.html#bigearthnet-metrics",
    "title": "Custom losses and metrics",
    "section": "BigEarthNet metrics",
    "text": "BigEarthNet metrics\n\nMetrics used in BigEarthNet paper to evaluate multi-label classification\n\n\nsource\n\nlabel_ranking_average_precision_score\n\n label_ranking_average_precision_score (sigmoid=True, sample_weight=None)\n\nLabel ranking average precision (LRAP) is the average over each ground truth label assigned to each sample, of the ratio of true vs. total labels with lower score.\n\nfrom fastai.learner import Learner\n\n\nclass TstLearner(Learner):\n    def __init__(self,dls=None,model=None,**kwargs): self.pred,self.xb,self.yb = None,None,None\n\ndef compute_val(met, x1, x2):\n    met.reset()\n    vals = [0,6,15,20]\n    learn = TstLearner()\n    for i in range(3):\n        learn.pred,learn.yb = x1[vals[i]:vals[i+1]],(x2[vals[i]:vals[i+1]],)\n        met.accumulate(learn)\n    return met.value\n\n\nx_1 = torch.randn(10,3)\nx_2 = torch.randint(2,(10,3))\nx_1, torch.sigmoid(x_1), x_2\n\n(tensor([[-0.0175, -1.1692,  0.4717],\n         [-0.1019,  1.7057,  0.8773],\n         [-0.6633,  0.0593,  1.3823],\n         [-0.6155, -0.4767, -1.4756],\n         [-1.0687,  0.0246, -0.3407],\n         [ 0.8646, -0.5164,  0.7274],\n         [-0.7253,  0.6507,  0.2994],\n         [-0.8993,  0.8574,  0.3334],\n         [ 0.5395,  0.5471,  0.7207],\n         [ 0.5087, -0.7967,  0.5395]]),\n tensor([[0.4956, 0.2370, 0.6158],\n         [0.4745, 0.8463, 0.7063],\n         [0.3400, 0.5148, 0.7994],\n         [0.3508, 0.3830, 0.1861],\n         [0.2557, 0.5061, 0.4156],\n         [0.7036, 0.3737, 0.6742],\n         [0.3262, 0.6572, 0.5743],\n         [0.2892, 0.7021, 0.5826],\n         [0.6317, 0.6335, 0.6728],\n         [0.6245, 0.3107, 0.6317]]),\n tensor([[1, 0, 0],\n         [1, 1, 0],\n         [1, 1, 1],\n         [0, 0, 1],\n         [1, 1, 1],\n         [1, 0, 0],\n         [0, 0, 0],\n         [0, 1, 0],\n         [1, 0, 1],\n         [0, 1, 0]]))\n\n\n\nlrap = label_ranking_average_precision_score()\ncompute_val(lrap, x_1, x_2)\n\n0.7833333333333332\n\n\n\nsource\n\n\nlabel_ranking_loss\n\n label_ranking_loss (sigmoid=True, sample_weight=None)\n\nCompute the average number of label pairs that are incorrectly ordered given y_score weighted by the size of the label set and the number of labels not in the label set.\n\nlrl = label_ranking_loss()\ncompute_val(lrl, x_1, x_2)\n\n0.35\n\n\n\n\n\none_error\n\n one_error (preds, targs)\n\nRate for which the top ranked label is not among ground truth\n\none_error(x_1, x_2)\n\ntensor(0.4000)\n\n\n\nsource\n\n\ncoverage_error\n\n coverage_error (sigmoid=True, sample_weight=None)\n\nCompute how far we need to go through the ranked scores to cover all true labels. The best value is equal to the average number of labels in y_true per sample.\n\ncov = coverage_error()\ncompute_val(cov, x_1, x_2)\n\n2.2"
  },
  {
    "objectID": "metrics.html#segmentation-metrics",
    "href": "metrics.html#segmentation-metrics",
    "title": "Custom losses and metrics",
    "section": "Segmentation metrics",
    "text": "Segmentation metrics\n\nsource\n\nJaccardCoeffMulti\n\n JaccardCoeffMulti (axis=1)\n\nAveraged Jaccard coefficient for multiclass target in segmentation. Excludes background class\n\nx1 = torch.randn(20,6,3,3)\nx2 = torch.randint(0, 6, (20, 3, 3))\npred = x1.argmax(1)\n\n\ncompute_val(JaccardCoeffMulti(), x1, x2)\n\n0.08437537158034053"
  },
  {
    "objectID": "metrics.html#object-detection-metrics-and-evaluation-for-shapefiles-mostly-obsolete-use-giscocoeval-instead",
    "href": "metrics.html#object-detection-metrics-and-evaluation-for-shapefiles-mostly-obsolete-use-giscocoeval-instead",
    "title": "Custom losses and metrics",
    "section": "Object detection metrics and evaluation for shapefiles MOSTLY OBSOLETE, USE GisCOCOEval instead",
    "text": "Object detection metrics and evaluation for shapefiles MOSTLY OBSOLETE, USE GisCOCOEval instead\nTo evaluate our collection of predicted masks, we’ll compare each of our predicted masks with each of the available target masks for a given input.\n\nA true positive, when a prediction-target mask pair has an IoU score which exceeds some predefined threshold\nA false positive, when a predicted object had no associated ground truth object mask\nA false negative indicates a ground truth object mask had no associated predicted object mask\n\nIn the case of multiple detections, the one with the highest confidence is considered to be “correct” and others are FP.\nFrom these, we can get Precision and Recall\n\\(Precision = \\frac{TP}{TP + FP} = \\frac{TP}{all \\: detections}, Recall = \\frac{TP}{TP+FN} = \\frac{TP}{all \\: ground \\: truths}\\)\nAnd use these to derive other metrics.\nTypical metrics include Average Precision (AP) and mean Average Precision (mAP). From these several metrics can be derived:\n\nAP50, AP75, AP[.50:.05:.95] are the most common, with AP[.50:.05:.95] being the primary challenge metric in COCO\nAP Across scales: APsmall, APmedium, APlarge, where small, medium and large have specified areas\n\nScales for COCO are less than 32² for small, between 32² and 96² for medium and more than 96² for large, sizes in pixels\nOur data has variable resolution sizes, but on average the resolution is around 0.05m, so small is less than 2.56m², medium is between 2.56m² and 23.04m², and large is more than 23.04m²\n\nAverage Recall (AR) is also sometimes used similarly, but with restrictions for the number of detections per image\n\nIt is computed as the area under Recall-IoU -curve for IoU thresholds from [0.5, 1]\n\nAll of these can be applied to bounding boxes and masks\n\nAll the following functions assume that you have two GeoDataFrames that have same CRS and matching column label. Usage example is the following:\n\nground_truth = gpd.read_file(&lt;path_to_ground_truth&gt;)\nresults = gpd.read_file(&lt;path_to_results&gt;)\n\n# clip the geodataframes to have same extent\nresults = gpd.clip(results, box(*ground_truth.total_bounds), keep_geom_type=True)\nground_truth = gpd.clip(ground_truth, box(*results.total_bounds), keep_geom_type=True)\n\n# create spatial index for faster queries                         \nres_sindex = results.sindex\ngt_sindex = ground_truth.sindex\n\n# TP/FN check with different thresholds, applied to ground truth\ntp_cols = [f'TP_{np.round(i, 2)}' for i in np.arange(0.5, 1.04, 0.05)]\nground_truth[tp_cols] = ground_truth.apply(lambda row: is_true_positive(row, results, res_sindex), \n                                           axis=1, result_type='expand')\n\n# TP/FP check with different thresholds, applied to predictions\nfp_cols = [f'FP_{np.round(i, 2)}' for i in np.arange(0.5, 1.01, 0.05)]\nresults[fp_cols] = results.apply(lambda row: is_false_positive(row, ground_truth, gt_sindex, results, res_sindex), \n                                 axis=1, result_type='expand')\n\nsource\n\npoly_IoU\n\n poly_IoU (poly_1:shapely.geometry.polygon.Polygon,\n           poly_2:shapely.geometry.polygon.Polygon)\n\nIoU for polygons\n\nsource\n\n\npoly_dice\n\n poly_dice (poly_1:shapely.geometry.polygon.Polygon,\n            poly_2:shapely.geometry.polygon.Polygon)\n\nDice for polygons\n\nsource\n\n\nis_true_positive\n\n is_true_positive (row, results:geopandas.geodataframe.GeoDataFrame, res_s\n                   index:&lt;module'geopandas.sindex'from'/usr/share/minicond\n                   a3/envs/test/lib/python3.10/site-\n                   packages/geopandas/sindex.py'&gt;)\n\nCheck if a single ground truth mask is TP or FN with 11 different IoU thresholds\n\nsource\n\n\nis_false_positive\n\n is_false_positive (row, ground_truth:geopandas.geodataframe.GeoDataFrame,\n                    gt_sindex:&lt;module'geopandas.sindex'from'/usr/share/min\n                    iconda3/envs/test/lib/python3.10/site-\n                    packages/geopandas/sindex.py'&gt;,\n                    results:geopandas.geodataframe.GeoDataFrame, res_sinde\n                    x:&lt;module'geopandas.sindex'from'/usr/share/miniconda3/\n                    envs/test/lib/python3.10/site-\n                    packages/geopandas/sindex.py'&gt;)\n\nCheck if prediction is FP or TP for 11 different IoU thresholds\naverage_precision and average_recall both return dict of the results, with each label and each IoU threshold separately. Each item is 11 item list where each item correspond to a different recall threshold in the range of [0:.1:1] in the case of average_precision, or IoU threshold in the range of [.50:.05:1] in for average_recall.\n\nsource\n\n\naverage_precision\n\n average_precision (ground_truth:geopandas.geodataframe.GeoDataFrame,\n                    preds:geopandas.geodataframe.GeoDataFrame)\n\nGet 11-point AP score for each label separately and with all iou_thresholds\n\nsource\n\n\naverage_recall\n\n average_recall (ground_truth:geopandas.geodataframe.GeoDataFrame,\n                 preds:geopandas.geodataframe.GeoDataFrame,\n                 max_detections:int=None)\n\nGet 11-point AR score for each label separately and with all iou_thresholds. If max_detections is not None evaluate with only that most confident predictions Seems to be still bugged, needs fixing"
  },
  {
    "objectID": "metrics.html#object-detection-metrics-with-pycocotools-and-gis-data",
    "href": "metrics.html#object-detection-metrics-with-pycocotools-and-gis-data",
    "title": "Custom losses and metrics",
    "section": "Object detection metrics with pycocotools and gis-data",
    "text": "Object detection metrics with pycocotools and gis-data\nRun predict_instance_masks or predict_bboxes for each scene separately, and save the resulting files in data_path containing * folder raster_tiles that contain the corresponding raster data. Required for transforming shapefiles to pixel coordinates * folder vector_tiles that contain ground truth masks * folder predicted_vectors that contain predictions\nAll files corresponding to the same scene should have the same name, e.g. raster_tiles/1053_Hiidenportti_Chunk9_orto.tif for raster image, vector_tiles/1053_Hiidenportti_Chunk9_orto.geojson for ground truth and predicted_vectors/1053_Hiidenportti_Chunk9_orto.geojson for predictions.\n\nsource\n\nGisCOCOeval\n\n GisCOCOeval (data_path:str, outpath:str, coco_info:dict,\n              coco_licenses:list, coco_categories:list)\n\nInitialize evaluator with data path and coco information\n\nsource\n\n\nGisCOCOeval.prepare_data\n\n GisCOCOeval.prepare_data (gt_label_col:str='label',\n                           res_label_col:str='label',\n                           rotated_bbox:bool=False, min_bbox_area:int=0)\n\nConvert GIS-data predictions to COCO-format for evaluation, and save resulting files to self.outpath\n\nsource\n\n\nGisCOCOeval.prepare_eval\n\n GisCOCOeval.prepare_eval (eval_type:str='segm')\n\nPrepare COCOeval to evaluate predictions with 100 and 1000 detections. AP metrics are evaluated with 1000 detections and AR with 100\n\nsource\n\n\nGisCOCOeval.evaluate\n\n GisCOCOeval.evaluate (classes_separately:bool=True)\n\nRun evaluation and print metrics\n\nsource\n\n\nGisCOCOeval.save_results\n\n GisCOCOeval.save_results (outpath, iou_thresh:float=0.5)\n\nSaves correctly detected ground truths, correct detections missed ground truths and misclassifications with specified iou_threshold in separate files for each scene"
  },
  {
    "objectID": "engines.detectron2.training.html",
    "href": "engines.detectron2.training.html",
    "title": "Helpers for detectron2 training",
    "section": "",
    "text": "Simple Trainer for basic training\n\nsource\n\nTrainer\n\n Trainer (cfg)\n\nTrainer class for training detectron2 models, using default augmentations\n\nsource\n\n\nRotatedDatasetMapper\n\n RotatedDatasetMapper (is_train:bool, augmentations:List[Union[detectron2.\n                       data.transforms.augmentation.Augmentation,fvcore.tr\n                       ansforms.transform.Transform]], image_format:str,\n                       use_instance_mask:bool=False,\n                       use_keypoint:bool=False,\n                       instance_mask_format:str='polygon', keypoint_hflip_\n                       indices:Optional[numpy.ndarray]=None,\n                       precomputed_proposal_topk:Optional[int]=None,\n                       recompute_boxes:bool=False)\n\nA callable which takes a dataset dict in Detectron2 Dataset format, and map it into a format used by the model.\nThis is the default callable to be used to map your dataset dict into training data. You may need to follow it to implement your own one for customized logic, such as a different way to read or transform images. See :doc:/tutorials/data_loading for details.\nThe callable currently does the following:\n\nRead the image from “file_name”\nApplies cropping/geometric transforms to the image and annotations\nPrepare data and annotations to Tensor and :class:Instances\n\n\nsource\n\n\ntransform_rotated_annotations\n\n transform_rotated_annotations (annotation, transforms, image_size,\n                                keypoint_hflip_indices=None)\n\n\nsource\n\n\nRotatedTrainer\n\n RotatedTrainer (cfg)\n\nA trainer with default training logic. It does the following:\n\nCreate a :class:SimpleTrainer using model, optimizer, dataloader defined by the given config. Create a LR scheduler defined by the config.\nLoad the last checkpoint or cfg.MODEL.WEIGHTS, if exists, when resume_or_load is called.\nRegister a few common hooks defined by the config.\n\nIt is created to simplify the standard model training workflow and reduce code boilerplate for users who only need the standard training workflow, with standard features. It means this class makes many assumptions about your training logic that may easily become invalid in a new research. In fact, any assumptions beyond those made in the :class:SimpleTrainer are too much for research.\nThe code of this class has been annotated about restrictive assumptions it makes. When they do not work for you, you’re encouraged to:\n\nOverwrite methods of this class, OR:\nUse :class:SimpleTrainer, which only does minimal SGD training and nothing else. You can then add your own hooks if needed. OR:\nWrite your own training loop similar to tools/plain_train_net.py.\n\nSee the :doc:/tutorials/training tutorials for more details.\nNote that the behavior of this class, like other functions/classes in this file, is not stable, since it is meant to represent the “common default behavior”. It is only guaranteed to work well with the standard models and training workflow in detectron2. To obtain more stable behavior, write your own training logic with other public APIs.\nExamples: :: trainer = DefaultTrainer(cfg) trainer.resume_or_load() # load last checkpoint or MODEL.WEIGHTS trainer.train()\nAttributes: scheduler: checkpointer (DetectionCheckpointer): cfg (CfgNode):"
  },
  {
    "objectID": "overview.engines.html",
    "href": "overview.engines.html",
    "title": "Deep learning engines and helpers",
    "section": "",
    "text": "This project contains helpers, scripts and classes for three different engines: fastai, icevision and detectron2. fastai is a good choice for semantic segmentation, classification and regression tasks, whereas icevision and detectron2 are good options for object detection and instance segmentation."
  },
  {
    "objectID": "overview.engines.html#fastai",
    "href": "overview.engines.html#fastai",
    "title": "Deep learning engines and helpers",
    "section": "fastai",
    "text": "fastai\n\ndrone_detector.engines.fastai.data contains helpers for working with:\n\ngeotiff files\nmultichannel images\nsemantic segmentation where the target mask is contiunuous\nTime series of (multichannel) images\n\ndrone_detector.engines.fastai.losses contains some custom losses to use for semantic segmentation\ndrone_detector.engines.fastai.augmentations contains helpers for using Albumentations with fastai models\nCLI utilities for inference"
  },
  {
    "objectID": "overview.engines.html#detectron2",
    "href": "overview.engines.html#detectron2",
    "title": "Deep learning engines and helpers",
    "section": "detectron2",
    "text": "detectron2\n\nCLI utilities for batch inference\nHelpers for training\nCustom TTA modifications that implement also VFlip\nData augmentation for rotated bounding boxes"
  },
  {
    "objectID": "engines.fastai.losses.html",
    "href": "engines.fastai.losses.html",
    "title": "Custom losses for segmentation and object detection",
    "section": "",
    "text": "From https://github.com/bermanmaxim/LovaszSoftmax\n\n\nsource\n\n\n\n xloss (logits, labels, ignore=None)\n\nCross entropy loss\n\nsource\n\n\n\n\n flatten_probas (probas, labels, ignore=None)\n\nFlattens predictions in the batch\n\nsource\n\n\n\n\n lovasz_softmax_flat (probas, labels, classes='present')\n\nMulti-class Lovasz-Softmax loss probas: [P, C] Variable, class probabilities at each prediction (between 0 and 1) labels: [P] Tensor, ground truth labels (between 0 and C - 1) classes: ‘all’ for all, ‘present’ for classes present in labels, or a list of classes to average.\n\nsource\n\n\n\n\n lovasz_softmax (probas, labels, classes='present', per_image=False,\n                 ignore=None)\n\nMulti-class Lovasz-Softmax loss probas: [B, C, H, W] Variable, class probabilities at each prediction (between 0 and 1). Interpreted as binary (sigmoid) output with outputs of size [B, H, W]. labels: [B, H, W] Tensor, ground truth labels (between 0 and C - 1) classes: ‘all’ for all, ‘present’ for classes present in labels, or a list of classes to average. per_image: compute the loss per image instead of per batch ignore: void class labels\n\nsource\n\n\n\n\n flatten_binary_scores (scores, labels, ignore=None)\n\nFlattens predictions in the batch (binary case) Remove labels equal to ‘ignore’\n\nsource\n\n\n\n\n lovasz_hinge_flat (logits, labels)\n\nBinary Lovasz hinge loss logits: [P] Variable, logits at each prediction (between -and +) labels: [P] Tensor, binary ground truth labels (0 or 1) ignore: label to ignore\n\nsource\n\n\n\n\n lovasz_hinge (logits, labels, per_image=True, ignore=None)\n\nBinary Lovasz hinge loss logits: [B, H, W] Variable, logits at each pixel (between -and +) labels: [B, H, W] Tensor, binary ground truth masks (0 or 1) per_image: compute the loss per image instead of per batch ignore: void class id\n\nsource\n\n\n\n\n mean (l, ignore_nan=False, empty=0)\n\nnanmean compatible with generators.\n\nsource\n\n\n\n\n isnan (x)\n\n\nsource\n\n\n\n\n iou (preds, labels, C, EMPTY=1.0, ignore=None, per_image=False)\n\nArray of IoU for each (non ignored) class\n\nsource\n\n\n\n\n iou_binary (preds, labels, EMPTY=1.0, ignore=None, per_image=True)\n\nIoU for foreground class binary: 1 foreground, 0 background\n\nsource\n\n\n\n\n lovasz_grad (gt_sorted)\n\nComputes gradient of the Lovasz extension w.r.t sorted errors See Alg. 1 in paper\n\nsource\n\n\n\n\n LovaszHingeLoss (ignore=None)\n\nLovasz-Hinge loss from https://arxiv.org/abs/1705.08790, with per_image=True\nTodo\nBinary Lovasz hinge loss logits: [P] Variable, logits at each prediction (between -and +) labels: [P] Tensor, binary ground truth labels (0 or 1) ignore: label to ignore\n\nsource\n\n\n\n\n LovaszHingeLossFlat (*args, axis=-1, ignore=None, **kwargs)\n\nSame as LovaszHingeLoss but flattens input and target\n\nlov_hinge = LovaszHingeLossFlat()\noutp = torch.randn(4,1,128,128)\ntarget = torch.randint(0, 2, (4,1,128,128))\n\n\nlov_hinge(outp, target)\n\ntensor(1.4331)\n\n\n\nlovasz_hinge(outp, target)\n\ntensor(1.4331)\n\n\n\nsource\n\n\n\n\n LovaszSigmoidLoss (ignore=None)\n\nLovasz-Sigmoid loss from https://arxiv.org/abs/1705.08790, with per_image=False\nTodo\nprobas: [P, C] Variable, logits at each prediction (between -and +) labels: [P] Tensor, binary ground truth labels (0 or 1) ignore: label to ignore\n\nsource\n\n\n\n\n LovaszSigmoidLossFlat (*args, axis=-1, ignore=None, **kwargs)\n\nSame as LovaszSigmoidLoss but flattens input and target\n\nlov_sigmoid = LovaszSigmoidLossFlat()\n\n\nlov_sigmoid(outp, target)\n\ntensor(0.5823)\n\n\n\nlovasz_softmax(torch.sigmoid(outp), target, classes=[1])\n\ntensor(0.5823)\n\n\n\nsource\n\n\n\n\n LovaszSoftmaxLoss (classes='present', ignore=None)\n\nLovasz-Sigmoid loss from https://arxiv.org/abs/1705.08790, with per_image=False\n\nsource\n\n\n\n\n LovaszSoftmaxLossFlat (*args, axis=1, classes='present', ignore=None,\n                        **kwargs)\n\nSame as LovaszSigmoidLoss but flattens input and target\n\nlov_softmax = LovaszSoftmaxLossFlat()\noutp_multi = torch.randn(4,3,128,128)\ntarget_multi = torch.randint(0, 3, (4,1,128,128))\nlov_softmax(outp_multi, target_multi)\n\ntensor(0.7045)\n\n\n\nlovasz_softmax(F.softmax(outp_multi, dim=1), target_multi)\n\ntensor(0.7045)\n\n\n\nlov_softmax_subset = LovaszSoftmaxLossFlat(classes=[1,2])\nlov_softmax_subset(outp_multi, target_multi)\n\ntensor(0.7039)\n\n\n\nlovasz_softmax(F.softmax(outp_multi, dim=1), target_multi, classes=[1,2])\n\ntensor(0.7039)\n\n\n\nsource\n\n\n\n\n FocalDice (axis=1, smooth=1.0, alpha=1.0)\n\nCombines Focal loss with dice loss"
  },
  {
    "objectID": "engines.fastai.losses.html#lovász-losses",
    "href": "engines.fastai.losses.html#lovász-losses",
    "title": "Custom losses for segmentation and object detection",
    "section": "",
    "text": "From https://github.com/bermanmaxim/LovaszSoftmax\n\n\nsource\n\n\n\n xloss (logits, labels, ignore=None)\n\nCross entropy loss\n\nsource\n\n\n\n\n flatten_probas (probas, labels, ignore=None)\n\nFlattens predictions in the batch\n\nsource\n\n\n\n\n lovasz_softmax_flat (probas, labels, classes='present')\n\nMulti-class Lovasz-Softmax loss probas: [P, C] Variable, class probabilities at each prediction (between 0 and 1) labels: [P] Tensor, ground truth labels (between 0 and C - 1) classes: ‘all’ for all, ‘present’ for classes present in labels, or a list of classes to average.\n\nsource\n\n\n\n\n lovasz_softmax (probas, labels, classes='present', per_image=False,\n                 ignore=None)\n\nMulti-class Lovasz-Softmax loss probas: [B, C, H, W] Variable, class probabilities at each prediction (between 0 and 1). Interpreted as binary (sigmoid) output with outputs of size [B, H, W]. labels: [B, H, W] Tensor, ground truth labels (between 0 and C - 1) classes: ‘all’ for all, ‘present’ for classes present in labels, or a list of classes to average. per_image: compute the loss per image instead of per batch ignore: void class labels\n\nsource\n\n\n\n\n flatten_binary_scores (scores, labels, ignore=None)\n\nFlattens predictions in the batch (binary case) Remove labels equal to ‘ignore’\n\nsource\n\n\n\n\n lovasz_hinge_flat (logits, labels)\n\nBinary Lovasz hinge loss logits: [P] Variable, logits at each prediction (between -and +) labels: [P] Tensor, binary ground truth labels (0 or 1) ignore: label to ignore\n\nsource\n\n\n\n\n lovasz_hinge (logits, labels, per_image=True, ignore=None)\n\nBinary Lovasz hinge loss logits: [B, H, W] Variable, logits at each pixel (between -and +) labels: [B, H, W] Tensor, binary ground truth masks (0 or 1) per_image: compute the loss per image instead of per batch ignore: void class id\n\nsource\n\n\n\n\n mean (l, ignore_nan=False, empty=0)\n\nnanmean compatible with generators.\n\nsource\n\n\n\n\n isnan (x)\n\n\nsource\n\n\n\n\n iou (preds, labels, C, EMPTY=1.0, ignore=None, per_image=False)\n\nArray of IoU for each (non ignored) class\n\nsource\n\n\n\n\n iou_binary (preds, labels, EMPTY=1.0, ignore=None, per_image=True)\n\nIoU for foreground class binary: 1 foreground, 0 background\n\nsource\n\n\n\n\n lovasz_grad (gt_sorted)\n\nComputes gradient of the Lovasz extension w.r.t sorted errors See Alg. 1 in paper\n\nsource\n\n\n\n\n LovaszHingeLoss (ignore=None)\n\nLovasz-Hinge loss from https://arxiv.org/abs/1705.08790, with per_image=True\nTodo\nBinary Lovasz hinge loss logits: [P] Variable, logits at each prediction (between -and +) labels: [P] Tensor, binary ground truth labels (0 or 1) ignore: label to ignore\n\nsource\n\n\n\n\n LovaszHingeLossFlat (*args, axis=-1, ignore=None, **kwargs)\n\nSame as LovaszHingeLoss but flattens input and target\n\nlov_hinge = LovaszHingeLossFlat()\noutp = torch.randn(4,1,128,128)\ntarget = torch.randint(0, 2, (4,1,128,128))\n\n\nlov_hinge(outp, target)\n\ntensor(1.4331)\n\n\n\nlovasz_hinge(outp, target)\n\ntensor(1.4331)\n\n\n\nsource\n\n\n\n\n LovaszSigmoidLoss (ignore=None)\n\nLovasz-Sigmoid loss from https://arxiv.org/abs/1705.08790, with per_image=False\nTodo\nprobas: [P, C] Variable, logits at each prediction (between -and +) labels: [P] Tensor, binary ground truth labels (0 or 1) ignore: label to ignore\n\nsource\n\n\n\n\n LovaszSigmoidLossFlat (*args, axis=-1, ignore=None, **kwargs)\n\nSame as LovaszSigmoidLoss but flattens input and target\n\nlov_sigmoid = LovaszSigmoidLossFlat()\n\n\nlov_sigmoid(outp, target)\n\ntensor(0.5823)\n\n\n\nlovasz_softmax(torch.sigmoid(outp), target, classes=[1])\n\ntensor(0.5823)\n\n\n\nsource\n\n\n\n\n LovaszSoftmaxLoss (classes='present', ignore=None)\n\nLovasz-Sigmoid loss from https://arxiv.org/abs/1705.08790, with per_image=False\n\nsource\n\n\n\n\n LovaszSoftmaxLossFlat (*args, axis=1, classes='present', ignore=None,\n                        **kwargs)\n\nSame as LovaszSigmoidLoss but flattens input and target\n\nlov_softmax = LovaszSoftmaxLossFlat()\noutp_multi = torch.randn(4,3,128,128)\ntarget_multi = torch.randint(0, 3, (4,1,128,128))\nlov_softmax(outp_multi, target_multi)\n\ntensor(0.7045)\n\n\n\nlovasz_softmax(F.softmax(outp_multi, dim=1), target_multi)\n\ntensor(0.7045)\n\n\n\nlov_softmax_subset = LovaszSoftmaxLossFlat(classes=[1,2])\nlov_softmax_subset(outp_multi, target_multi)\n\ntensor(0.7039)\n\n\n\nlovasz_softmax(F.softmax(outp_multi, dim=1), target_multi, classes=[1,2])\n\ntensor(0.7039)\n\n\n\nsource\n\n\n\n\n FocalDice (axis=1, smooth=1.0, alpha=1.0)\n\nCombines Focal loss with dice loss"
  },
  {
    "objectID": "engines.fastai.augmentations.html",
    "href": "engines.fastai.augmentations.html",
    "title": "Augmentations",
    "section": "",
    "text": "source\n\n\n\n RegressionMask.affine_coord\n                              (x:drone_detector.engines.fastai.data.Regres\n                              sionMask, mat=None, coord_tfm=None, sz=None,\n                              mode='nearest', pad_mode='reflection',\n                              align_corners=True)\n\nRegressionMask can’t be long type"
  },
  {
    "objectID": "engines.fastai.augmentations.html#patching-for-drone_detector.data-classes",
    "href": "engines.fastai.augmentations.html#patching-for-drone_detector.data-classes",
    "title": "Augmentations",
    "section": "",
    "text": "source\n\n\n\n RegressionMask.affine_coord\n                              (x:drone_detector.engines.fastai.data.Regres\n                              sionMask, mat=None, coord_tfm=None, sz=None,\n                              mode='nearest', pad_mode='reflection',\n                              align_corners=True)\n\nRegressionMask can’t be long type"
  },
  {
    "objectID": "engines.fastai.augmentations.html#albumentationstransform",
    "href": "engines.fastai.augmentations.html#albumentationstransform",
    "title": "Augmentations",
    "section": "AlbumentationsTransform",
    "text": "AlbumentationsTransform\n\nsource\n\nAlbumentationsTransform\n\n AlbumentationsTransform (train_aug, valid_aug=None)\n\nA transform handler for multiple Albumentation transforms in simple classification or regression tasks."
  },
  {
    "objectID": "engines.fastai.augmentations.html#segmentationalbumentationstransform",
    "href": "engines.fastai.augmentations.html#segmentationalbumentationstransform",
    "title": "Augmentations",
    "section": "SegmentationAlbumentationsTransform",
    "text": "SegmentationAlbumentationsTransform\n\nsource\n\nSegmentationAlbumentationsTransform\n\n SegmentationAlbumentationsTransform (aug)\n\nA transform handler for Albumentation transforms for segmentation tasks.\n\npath = Path('example_data/monthly_data')\nmonth='july'\n\nfnames = [path/f'{month}/{f}' for f in os.listdir(path/f'{month}') if f.endswith('.tif')][:2]\n\n\ntransform_list = [A.ToFloat(max_value=65535.0, always_apply=True),\n                  A.RandomBrightnessContrast(p=0.5,brightness_limit=.05, brightness_by_max=False),\n                  A.RandomRotate90(p=1),\n                  A.HorizontalFlip(p=.5),\n                  A.VerticalFlip(p=.5),\n                  A.CoarseDropout(),\n                  A.FromFloat(max_value=65535.0, dtype=np.int16, always_apply=True)\n                 ]\n\n\nused_tfms =  SegmentationAlbumentationsTransform(A.Compose(transform_list))\n\n\nsegm = TifSegmentationDataLoaders.from_label_func(path/f'{month}_2018', fnames, y_block=RegressionMaskBlock,\n                                                  label_func=partial(label_with_matching_fname, \n                                                                     path=path/'masks'),\n                                                  batch_tfms = [\n                                                      #Normalize.from_stats(*stats)\n                                                  ],\n                                                  item_tfms = [used_tfms], \n                                                  bs=1)\n\n\nsegm.show_batch(channels=[3,2,1])\n\n\n\n\n\nfiles = get_image_timeseries(path, \n                             months=['may', 'june', 'july', 'august', 'september', 'october'], masks='masks')\n\n\ndblock = DataBlock(blocks=(MultiChannelImageTupleBlock, \n                           RegressionMaskBlock),\n                   get_items=lambda x: x,\n                   get_x=get_all_but_last, get_y=get_last,\n                   item_tfms=[\n                       used_tfms\n                   ],\n                   batch_tfms=[ \n                   ])\n\n\ndls = dblock.dataloaders(files[:1], bs=1)\n\n\ndls.show_batch(channels=[3,2,1])"
  },
  {
    "objectID": "engines.fastai.augmentations.html#utils-functions",
    "href": "engines.fastai.augmentations.html#utils-functions",
    "title": "Augmentations",
    "section": "Utils functions",
    "text": "Utils functions"
  },
  {
    "objectID": "processing.tiling.html",
    "href": "processing.tiling.html",
    "title": "Tiling",
    "section": "",
    "text": "source\n\nTiler\n\n Tiler (outpath, gridsize_x:int=400, gridsize_y:int=400,\n        overlap:Tuple[int,int]=(100, 100))\n\nSimilar functions than ´solaris.tile.raster_tile’ but with more recent dependencies.\n\nsource\n\n\nTiler.tile_raster\n\n Tiler.tile_raster (path_to_raster:str, allow_partial_data:bool=False)\n\nTiles specified raster to self.gridsize_x times self.gridsize_y grid, with self.overlap pixel overlap\n\nsource\n\n\nTiler.tile_vector\n\n Tiler.tile_vector (path_to_vector:str, min_area_pct:float=0.0)\n\nTiles a vector data file into smaller tiles. Converts all multipolygons to a regular polygons. min_area_pct is be used to specify the minimum area for partial masks to keep. Default value 0.0 keeps all masks.\n\nsource\n\n\nTiler.tile_and_rasterize_vector\n\n Tiler.tile_and_rasterize_vector (path_to_raster:str, path_to_vector:str,\n                                  column:str, keep_bg_only:bool=False)\n\nRasterizes vectors based on tiled rasters. Requires that shapefile has numeric data in column. By default only keeps the patches that contain polygon data, by specifying keep_bg_only=True saves also masks for empty patches.\n\nsource\n\n\nuntile_vector\n\n untile_vector (path_to_targets:str, outpath:str,\n                non_max_suppression_thresh:float=0.0,\n                nms_criterion:str='score')\n\nCreate single shapefile from a directory of predicted shapefiles\n\nsource\n\n\ncopy_sum\n\n copy_sum (merged_data, new_data, merged_mask, new_mask, **kwargs)\n\nMake new pixels have the sum of two overlapping pixels as their value. Useful with prediction data\n\nsource\n\n\nuntile_raster\n\n untile_raster (path_to_targets:str, outfile:str, method:str='first')\n\nMerge multiple patches from path_to_targets into a single raster`\n\nf = gpd.read_file('example_data/R70C21.shp')\nf.head()\nf['label_id'] = f.apply(lambda row: 2 if row.label == 'Standing' else 1, axis=1)\nf.to_file('example_data/R70C21.shp')\n\nExample area looks like this\n\nfrom rasterio import plot as rioplot\n\n\nraster = rio.open('example_data/R70C21.tif')\nrioplot.show(raster)\n\n\n\n\n&lt;Axes: &gt;\n\n\nWith rasterio.plot it is a lot easier to visualize shapefile and raster simultaneously\n\nfig, ax = plt.subplots(1,1)\nrioplot.show(raster, ax=ax)\nf.plot(ax=ax, column='label_id')\n\n&lt;Axes: &gt;\n\n\n\n\n\nLet’s tile image into 240x180 sized patches using overlap of half patch size.\n\ntiler = Tiler(outpath='example_data/tiles', gridsize_x=240, gridsize_y=180, overlap=(120, 90))\n\n\ntiler.tile_raster('example_data/R70C21.tif')\n\n\n\n\n\ntiler.tile_vector('example_data/R70C21.shp', min_area_pct=.2)\n\n\n\n\nUntile shapefiles and check how they look\n\nuntile_vector(f'example_data/tiles/vectors', outpath='example_data/untiled.geojson')\n\n\n\n\n81 polygons before non-max suppression\n81 polygons after non-max suppression\n\n\nPlot with the tiled grid.\n\nuntiled = gpd.read_file('example_data/untiled.geojson')\nfig, ax = plt.subplots(1,1)\nrioplot.show(raster, ax=ax)\ntiler.grid.exterior.plot(ax=ax)\nuntiled.plot(ax=ax, column='label_id', facecolor='none', edgecolor='black')\n\n&lt;Axes: &gt;\n\n\n\n\n\nIf allow_partial_data=False as is the default behaviour, tiling is done only for the area from which full sized patch can be extracted. With allow_partial_data=True, windows can “extend” to empty areas. This is useful with inference, when predicted areas can have wonky dimensions.\n\ntiler.tile_raster('example_data/R70C21.tif', allow_partial_data=True)\n\n\n\n\n\ntiler.tile_vector('example_data/R70C21.shp', min_area_pct=.2)\n\n\n\n\nUntile shapefiles and check how they look\n\nuntile_vector(f'example_data/tiles/vectors', outpath='example_data/untiled.geojson')\n\n\n\n\n81 polygons before non-max suppression\n81 polygons after non-max suppression\n\n\n\nuntiled = gpd.read_file('example_data/untiled.geojson')\nfig, ax = plt.subplots(1,1)\nrioplot.show(raster, ax=ax)\n#tiler.grid.exterior.plot(ax=ax)\nuntiled.plot(ax=ax, column='label_id', facecolor='none', edgecolor='black')\n\n&lt;Axes: &gt;\n\n\n\n\n\nPlot with the tiled grid.\n\nuntiled = gpd.read_file('example_data/untiled.geojson')\nfig, ax = plt.subplots(1,1)\nrioplot.show(raster, ax=ax)\ntiler.grid.exterior.plot(ax=ax)\nuntiled.plot(ax=ax, column='label_id')\n\n&lt;Axes: &gt;\n\n\n\n\n\n\ntiler.tile_and_rasterize_vector('example_data/R70C21.tif', 'example_data/R70C21.shp', column='label_id')\n\n\n\n\n\ntiler.tile_and_rasterize_vector('example_data/R70C21.tif', 'example_data/R70C21.shp', \n                                column='label_id', keep_bg_only=True)\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\n\nwith rio.open('example_data/tiles/rasterized_vectors/R1C3.tif') as i: im = i.read()\nplt.imshow(im[0])\n\n&lt;matplotlib.image.AxesImage&gt;\n\n\n\n\n\nuntile_raster can be used to mosaic all patches into one.\n\nuntile_raster('example_data/tiles/rasterized_vectors/', 'example_data/tiles/mosaic_first.tif', \n              method='first')\n\n\nwith rio.open('example_data/tiles/mosaic_first.tif') as mos: mosaic = mos.read()\nplt.imshow(mosaic[0])\n\n&lt;matplotlib.image.AxesImage&gt;\n\n\n\n\n\nBy specifying method as sum it’s possible to collate predictions and get the most likely label for pixels\n\nuntile_raster('example_data/tiles/rasterized_vectors/', 'example_data/tiles/mosaic_sum.tif',\n              method='sum')\n\n\nwith rio.open('example_data/tiles/mosaic_sum.tif') as mos: mosaic = mos.read()\nplt.imshow(mosaic[0])\n\n&lt;matplotlib.image.AxesImage&gt;"
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "utils",
    "section": "",
    "text": "source\n\n\n\n rangeof (iterable)\n\nEquivalent for range(len(iterable))\n\nsource\n\n\n\n\n fix_multipolys (multipoly:shapely.geometry.multipolygon.MultiPolygon)\n\nConvert MultiPolygon to a single Polygon. The resulting Polygon has the exterior boundaries of the largest geometry of the MultiPolygon"
  },
  {
    "objectID": "utils.html#utilities",
    "href": "utils.html#utilities",
    "title": "utils",
    "section": "",
    "text": "source\n\n\n\n rangeof (iterable)\n\nEquivalent for range(len(iterable))\n\nsource\n\n\n\n\n fix_multipolys (multipoly:shapely.geometry.multipolygon.MultiPolygon)\n\nConvert MultiPolygon to a single Polygon. The resulting Polygon has the exterior boundaries of the largest geometry of the MultiPolygon"
  },
  {
    "objectID": "utils.html#math-formulae-for-fallen-deadwood-volume-estimation",
    "href": "utils.html#math-formulae-for-fallen-deadwood-volume-estimation",
    "title": "utils",
    "section": "Math formulae (for fallen deadwood volume estimation)",
    "text": "Math formulae (for fallen deadwood volume estimation)\n\nsource\n\ncut_cone_v\n\n cut_cone_v (r_1:float, r_2:float, h:float)\n\nV = (h(A + sqrt(A*A’) + A))/3\n\nsource\n\n\ncone_v\n\n cone_v (r:float, h:float)\n\nV = (Ah)/3"
  },
  {
    "objectID": "engines.detectron2.tta.html",
    "href": "engines.detectron2.tta.html",
    "title": "Custom TTA modifications for detectron2",
    "section": "",
    "text": "source\n\nDatasetMapperTTAFlip\n\n DatasetMapperTTAFlip (min_sizes:List[int], max_size:int, flip:bool)\n\nImplement test-time augmentation for detection data. Modified to implement both horizontal and vertical flip It is a callable which takes a dataset dict from a detection dataset, and returns a list of dataset dicts where the images are augmented from the input image by the transformations defined in the config. This is used for test-time augmentation.\n\nsource\n\n\nTTAPredictor\n\n TTAPredictor (cfg)\n\nDefaultPredictor that implements TTA"
  },
  {
    "objectID": "processing.coordinates.html",
    "href": "processing.coordinates.html",
    "title": "Coordinate transformations for vector data",
    "section": "",
    "text": "source\n\ngeoregister_px_df\n\n georegister_px_df (df:pandas.core.frame.DataFrame, im_path=None,\n                    affine_obj:affine.Affine=None, crs=None,\n                    geom_col:str='geometry', precision:int=None,\n                    output_path=None)\n\n\nsource\n\n\ngdf_to_px\n\n gdf_to_px (gdf:geopandas.geodataframe.GeoDataFrame, im_path,\n            geom_col:str='geometry', precision:int=None, outpath=None,\n            override_crs=False)\n\nAdapted from https://solaris.readthedocs.io/en/latest/_modules/solaris/vector/polygon.html#geojson_to_px_gdf\n\nsource\n\n\naffine_transform_gdf\n\n affine_transform_gdf (gdf:geopandas.geodataframe.GeoDataFrame,\n                       affine_obj:affine.Affine, inverse:bool=False,\n                       geom_col:str='geometry', precision:int=None)\n\nAdapted from solaris, transforms all geometries in GeoDataFrame to pixel coordinates from Georeferced coordinates and vice versa\n\nsource\n\n\nconvert_poly_coords\n\n convert_poly_coords (geom:&lt;function shape&gt;, raster_src:str=None,\n                      affine_obj:affine.Affine=None, inverse:bool=False,\n                      precision=None)\n\nAdapted from solaris. Converts georeferenced coordinates to pixel coordinates and vice versa\n\nsource\n\n\nget_geo_transform\n\n get_geo_transform (src)\n\nExtract geotransform for a raster image source\n\nsource\n\n\nlist_to_affine\n\n list_to_affine (xform_mat:list)\n\nAdapted from Solaris.geo. Creates an affine object from array-formatted list\n\ngdf = gpd.read_file('example_data/R70C21.shp')\nim_path = 'example_data/R70C21.tif'\n\nwith rio.open(im_path) as im: \n    im_data = im.read()\nplt.imshow(im_data.swapaxes(0,2).swapaxes(0,1))\nplt.show()\n\n\n\n\n\ngdf.plot(column='label', #facecolor='none', \n         edgecolor='black', lw=0.7, cmap='viridis')\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\ntfmd_gdf = gdf_to_px(gdf, im_path)\ntfmd_gdf.plot(column='label', #facecolor='none', \n              edgecolor='black', lw=0.7, cmap='viridis')\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\nOrigin in lower left for this data.\n\nfig, ax = plt.subplots()\nax.imshow(im_data.swapaxes(0,2).swapaxes(0,1))\ntfmd_gdf.plot(ax=ax, column='label', facecolor='none', edgecolor='black', lw=0.7, cmap='viridis')\nplt.show()\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\nOverlaid on image coordinates are correct\n\ntfmd_gdf = georegister_px_df(tfmd_gdf, im_path)\ntfmd_gdf.plot(column='label', facecolor='none', edgecolor='black', lw=0.7, cmap='viridis')\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\nBackwards transformation works"
  },
  {
    "objectID": "engines.fastai.data.html",
    "href": "engines.fastai.data.html",
    "title": "Custom data used in remote sensing",
    "section": "",
    "text": "source\n\n\n\n open_geotiff (fn, chans=None, max_val=None)\n\nOpen geotiff and read as numpy array, then convert into tensor\n\nsource\n\n\n\n\n open_npy (fn, chans=None, max_val=None)\n\nOpen npy-file"
  },
  {
    "objectID": "engines.fastai.data.html#opening-functions",
    "href": "engines.fastai.data.html#opening-functions",
    "title": "Custom data used in remote sensing",
    "section": "",
    "text": "source\n\n\n\n open_geotiff (fn, chans=None, max_val=None)\n\nOpen geotiff and read as numpy array, then convert into tensor\n\nsource\n\n\n\n\n open_npy (fn, chans=None, max_val=None)\n\nOpen npy-file"
  },
  {
    "objectID": "engines.fastai.data.html#show_-functions",
    "href": "engines.fastai.data.html#show_-functions",
    "title": "Custom data used in remote sensing",
    "section": "show_* functions",
    "text": "show_* functions\n\nsource\n\nnorm\n\n norm (vals, norm_min=None, norm_max=None, axis=(0, 1))\n\nFor visualization purposes scale image with `(vals-norm_min)/(norm_max-norm_min), with norm_min and norm_max either specified or within 0.01 and 0.99 quantiles of all values\n\nsource\n\n\nshow_mean_spectra\n\n show_mean_spectra (img, ax=None, figsize=(3, 3), ctx=None, title=None,\n                    **kwargs)\n\nShow average spectra graph\n\nsource\n\n\nshow_normalized_spectral_index\n\n show_normalized_spectral_index (img, channels, ax=None, figsize=(3, 3),\n                                 ctx=None, title=None, **kwargs)\n\nShow normalized spectral index such as NDVI\n\nsource\n\n\nshow_single_channel\n\n show_single_channel (img, channel, ax=None, figsize=(3, 3), ctx=None,\n                      title=None, **kwargs)\n\nVisualize only channel band\n\nsource\n\n\nshow_composite\n\n show_composite (img, channels, ax=None, figsize=(3, 3), title=None,\n                 scale=True, ctx=None, norm_min=None, norm_max=None,\n                 scale_axis=(0, 1), **kwargs)\n\nShow three channel composite so that channels correspond to R, G and B\nBoth show_results and show_batch are patched with @typedispatch to work with both MultiChannelTensorImages and RegressionMasks"
  },
  {
    "objectID": "engines.fastai.data.html#regressionmask",
    "href": "engines.fastai.data.html#regressionmask",
    "title": "Custom data used in remote sensing",
    "section": "RegressionMask",
    "text": "RegressionMask\n\nMask for continuous segmentation targets\n\n\nsource\n\nRegressionMaskBlock\n\n RegressionMaskBlock (cls=&lt;class '__main__.RegressionMask'&gt;, **kwargs)\n\nDefault behaviour: use all channels\n\nsource\n\n\nRegressionMask\n\n RegressionMask (x, **kwargs)\n\nClass for regression segmentation tasks"
  },
  {
    "objectID": "engines.fastai.data.html#multichanneltensorimage",
    "href": "engines.fastai.data.html#multichanneltensorimage",
    "title": "Custom data used in remote sensing",
    "section": "MultiChannelTensorImage",
    "text": "MultiChannelTensorImage\n\nTensorImage subclass for multichannel images. Works with .npy and .tif files so far.\n\n\nsource\n\nMultiChannelTensorImage\n\n MultiChannelTensorImage (x, **kwargs)\n\nA Tensor which support subclass pickling, and maintains metadata when casting or after methods\n\nsource\n\n\nMultiChannelImageBlock\n\n MultiChannelImageBlock (cls=&lt;class '__main__.MultiChannelTensorImage'&gt;,\n                         chans=None, max_val=None)\n\nDefault behaviour: use all channels\n\nsource\n\n\nusing_attr\n\n using_attr (f, attr)\n\nChange function f to operate on attr"
  },
  {
    "objectID": "engines.fastai.data.html#multichanneltensorimagetuple",
    "href": "engines.fastai.data.html#multichanneltensorimagetuple",
    "title": "Custom data used in remote sensing",
    "section": "MultiChannelTensorImageTuple",
    "text": "MultiChannelTensorImageTuple\n\nsource\n\nMultiChannelTensorImageTuple\n\n MultiChannelTensorImageTuple (x=None, *rest)\n\nA tuple with elementwise ops and more friendly init behavior\n\nsource\n\n\nMultiChannelImageTupleBlock\n\n MultiChannelImageTupleBlock (cls=&lt;class\n                              '__main__.MultiChannelTensorImageTuple'&gt;,\n                              chans=None, max_val=None)\n\nDefault behaviour: use all channels\nWith MultiChannelTensorImageTuple it’s easy to process time series of images:\n\nfns = ['example_data/monthly_data/may/R10C15.tif',\n       'example_data/monthly_data/june/R10C15.tif',\n       'example_data/monthly_data/july/R10C15.tif',\n       'example_data/monthly_data/august/R10C15.tif',\n       'example_data/monthly_data/september/R10C15.tif',\n       'example_data/monthly_data/october/R10C15.tif']\nfns = [Path(f) for f in fns]\n\n\ntemporaltuple = MultiChannelTensorImageTuple.create(fns)\ntemporaltuple\n\nMultiChannelTensorImageTuple of 6 images\n\n\n\ntemporaltuple.show(channels=[3,2,1], figsize=(14,10), cmap='RdYlGn_r')\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\nWith this kind of transform MultiChannelTensorImageTuple can be transformed into sequence of individual images\n\nclass TupleToTemporal(Transform):\n    order = 0\n    \n    def __init__(self, split_idx=None): store_attr()\n        \n    def encodes(self, o:MultiChannelTensorImageTuple):\n        return torch.stack(o)\n    \n    def decodes(self, o:MultiChannelTensorImage):\n        return torch.unbind(o)\n\n\ntfm = TupleToTemporal()\n\n\nstacked = tfm.encodes(temporaltuple)\nstacked\n\nMultiChannelTensorImage size=6x12x96x96\n\n\n\nunstacked = tfm.decodes(stacked)\nunstacked\n\n(MultiChannelTensorImage size=12x96x96,\n MultiChannelTensorImage size=12x96x96,\n MultiChannelTensorImage size=12x96x96,\n MultiChannelTensorImage size=12x96x96,\n MultiChannelTensorImage size=12x96x96,\n MultiChannelTensorImage size=12x96x96)\n\n\n\nMultiChannelTensorImageTuple(unstacked).show(channels=[3,2,1], figsize=(12,6))\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\nMost of fastai transforms work out of the box\n\n_, axs = subplots(8,1, figsize=(8,14))\nfor k, ax in zip(range(0,8), axs.flatten()):\n    DihedralItem(p=1)(temporaltuple, split_idx=0).show(channels=[7,3,2], ctx=ax, figsize=(1,6))\n\n\n\n\n\nsource\n\n\nget_last\n\n get_last (t)\n\n\nsource\n\n\nget_all_but_last\n\n get_all_but_last (t)\n\n\nsource\n\n\nget_image_timeseries\n\n get_image_timeseries (path, months, masks)\n\n\npath = Path('example_data/monthly_data/')\nfiles = get_image_timeseries(path, ['may', 'june', 'july', 'august', 'september', 'october'], 'masks')\n\ndblock = DataBlock(blocks=(MultiChannelImageTupleBlock(max_val=10000), \n                           RegressionMaskBlock),\n                   splitter=RandomSplitter(0),\n                   get_items=lambda x: x,\n                   get_x=get_all_but_last, get_y=get_last,\n                   item_tfms=[\n                   ],\n                   batch_tfms=[ \n                   ])\n\n\ndls = dblock.dataloaders(files[:1], bs=1)\n\n\ndls.show_batch(channels=[3,2,1])"
  },
  {
    "objectID": "engines.fastai.data.html#multichannelimagedataloaders",
    "href": "engines.fastai.data.html#multichannelimagedataloaders",
    "title": "Custom data used in remote sensing",
    "section": "MultiChannelImageDataLoaders",
    "text": "MultiChannelImageDataLoaders\n\nDataLoaders for MultiChannelImages\n\n\nsource\n\nMultiChannelImageDataLoaders\n\n MultiChannelImageDataLoaders (*loaders, path:str|pathlib.Path='.',\n                               device=None)\n\nBasic wrapper around several DataLoaders.\n\nsource\n\n\nMultiChannelImageDataLoaders.from_folder\n\n MultiChannelImageDataLoaders.from_folder (path, chans=None, max_val=None,\n                                           extensions=['.tif'],\n                                           train='train', valid='valid',\n                                           valid_pct=None, seed=None,\n                                           vocab=None, item_tfms=None,\n                                           batch_tfms=None, bs:int=64,\n                                           val_bs:int=None,\n                                           shuffle:bool=True, device=None)\n\nCreate from imagenet style dataset in path with train, valid, test subfolders (or provide valid_pct). Optionally list the channels to use.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath\nstr | Path\n.\nPath to put in DataLoaders\n\n\nchans\nNoneType\nNone\n\n\n\nmax_val\nNoneType\nNone\n\n\n\nextensions\nlist\n[‘.tif’]\n\n\n\ntrain\nstr\ntrain\n\n\n\nvalid\nstr\nvalid\n\n\n\nvalid_pct\nNoneType\nNone\n\n\n\nseed\nNoneType\nNone\n\n\n\nvocab\nNoneType\nNone\n\n\n\nitem_tfms\nNoneType\nNone\n\n\n\nbatch_tfms\nNoneType\nNone\n\n\n\nbs\nint\n64\nSize of batch\n\n\nval_bs\nint\nNone\nSize of batch for validation DataLoader\n\n\nshuffle\nbool\nTrue\nWhether to shuffle data\n\n\ndevice\nNoneType\nNone\nDevice to put DataLoaders\n\n\n\n\nsource\n\n\nMultiChannelImageDataLoaders.from_path_func\n\n MultiChannelImageDataLoaders.from_path_func (path, fnames, label_func,\n                                              chans=None, max_val=None,\n                                              extensions=['.tif'],\n                                              valid_pct=0.2, seed=None,\n                                              item_tfms=None,\n                                              batch_tfms=None, bs:int=64,\n                                              val_bs:int=None,\n                                              shuffle:bool=True,\n                                              device=None)\n\nCreate from list of fnames in paths with label_func.\nOptionally list the channels to use.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath\nstr | Path\n.\nPath to put in DataLoaders\n\n\nfnames\n\n\n\n\n\nlabel_func\n\n\n\n\n\nchans\nNoneType\nNone\n\n\n\nmax_val\nNoneType\nNone\n\n\n\nextensions\nlist\n[‘.tif’]\n\n\n\nvalid_pct\nfloat\n0.2\n\n\n\nseed\nNoneType\nNone\n\n\n\nitem_tfms\nNoneType\nNone\n\n\n\nbatch_tfms\nNoneType\nNone\n\n\n\nbs\nint\n64\nSize of batch\n\n\nval_bs\nint\nNone\nSize of batch for validation DataLoader\n\n\nshuffle\nbool\nTrue\nWhether to shuffle data\n\n\ndevice\nNoneType\nNone\nDevice to put DataLoaders\n\n\n\n\nsource\n\n\nMultiChannelImageDataLoaders.from_name_func\n\n MultiChannelImageDataLoaders.from_name_func (path, fnames, label_func,\n                                              chans=None, max_val=None,\n                                              extensions=['.tif'],\n                                              valid_pct=0.2, seed=None,\n                                              item_tfms=None,\n                                              batch_tfms=None, bs:int=64,\n                                              val_bs:int=None,\n                                              shuffle:bool=True,\n                                              device=None)\n\nCreate from name attrs in list of fnames in paths with label_func. Optionally list the channels to use.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath\nstr | Path\n.\nPath to put in DataLoaders\n\n\nfnames\n\n\n\n\n\nlabel_func\n\n\n\n\n\nchans\nNoneType\nNone\n\n\n\nmax_val\nNoneType\nNone\n\n\n\nextensions\nlist\n[‘.tif’]\n\n\n\nvalid_pct\nfloat\n0.2\n\n\n\nseed\nNoneType\nNone\n\n\n\nitem_tfms\nNoneType\nNone\n\n\n\nbatch_tfms\nNoneType\nNone\n\n\n\nbs\nint\n64\nSize of batch\n\n\nval_bs\nint\nNone\nSize of batch for validation DataLoader\n\n\nshuffle\nbool\nTrue\nWhether to shuffle data\n\n\ndevice\nNoneType\nNone\nDevice to put DataLoaders\n\n\n\n\nsource\n\n\nMultiChannelImageDataLoaders.from_shapefile\n\n MultiChannelImageDataLoaders.from_shapefile (path, chans=None,\n                                              max_val=None,\n                                              shp_fname='labels.shp',\n                                              valid_pct=0.2, seed=None,\n                                              fn_col=0, folder=None,\n                                              suff='', label_col=1,\n                                              label_delim=None,\n                                              y_block=None,\n                                              valid_col=None,\n                                              item_tfms=None,\n                                              batch_tfms=None, bs:int=64,\n                                              val_bs:int=None,\n                                              shuffle:bool=True,\n                                              device=None)\n\nCreate from shapefile shp_fname in path readable with geopandas (GeoJSON, ESRI Shapefile) Optionally list the channels to use.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath\nstr | Path\n.\nPath to put in DataLoaders\n\n\nchans\nNoneType\nNone\n\n\n\nmax_val\nNoneType\nNone\n\n\n\nshp_fname\nstr\nlabels.shp\n\n\n\nvalid_pct\nfloat\n0.2\n\n\n\nseed\nNoneType\nNone\n\n\n\nfn_col\nint\n0\n\n\n\nfolder\nNoneType\nNone\n\n\n\nsuff\nstr\n\n\n\n\nlabel_col\nint\n1\n\n\n\nlabel_delim\nNoneType\nNone\n\n\n\ny_block\nNoneType\nNone\n\n\n\nvalid_col\nNoneType\nNone\n\n\n\nitem_tfms\nNoneType\nNone\n\n\n\nbatch_tfms\nNoneType\nNone\n\n\n\nbs\nint\n64\nSize of batch\n\n\nval_bs\nint\nNone\nSize of batch for validation DataLoader\n\n\nshuffle\nbool\nTrue\nWhether to shuffle data\n\n\ndevice\nNoneType\nNone\nDevice to put DataLoaders\n\n\n\n\nsource\n\n\nMultiChannelImageDataLoaders.from_lists\n\n MultiChannelImageDataLoaders.from_lists (path, fnames, labels,\n                                          chans=None, max_val=None,\n                                          valid_pct=0.2, seed:int=None,\n                                          y_block=None, item_tfms=None,\n                                          batch_tfms=None, bs:int=64,\n                                          val_bs:int=None,\n                                          shuffle:bool=True, device=None)\n\nCreate from list of fnames in path. Optionally list the channels to use.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath\nstr | Path\n.\nPath to put in DataLoaders\n\n\nfnames\n\n\n\n\n\nlabels\n\n\n\n\n\nchans\nNoneType\nNone\n\n\n\nmax_val\nNoneType\nNone\n\n\n\nvalid_pct\nfloat\n0.2\n\n\n\nseed\nint\nNone\n\n\n\ny_block\nNoneType\nNone\n\n\n\nitem_tfms\nNoneType\nNone\n\n\n\nbatch_tfms\nNoneType\nNone\n\n\n\nbs\nint\n64\nSize of batch\n\n\nval_bs\nint\nNone\nSize of batch for validation DataLoader\n\n\nshuffle\nbool\nTrue\nWhether to shuffle data\n\n\ndevice\nNoneType\nNone\nDevice to put DataLoaders"
  },
  {
    "objectID": "engines.fastai.data.html#tifsegmentationdataloaders",
    "href": "engines.fastai.data.html#tifsegmentationdataloaders",
    "title": "Custom data used in remote sensing",
    "section": "TifSegmentationDataLoaders",
    "text": "TifSegmentationDataLoaders\n\nsource\n\nTifSegmentationDataLoaders\n\n TifSegmentationDataLoaders (*loaders, path:str|pathlib.Path='.',\n                             device=None)\n\nNeeds a better name\n\nsource\n\n\nlabel_from_different_folder\n\n label_from_different_folder (fn, original_folder, new_folder)\n\n\nsource\n\n\nlabel_with_matching_fname\n\n label_with_matching_fname (fn, path)\n\nUtility to match image and mask that have different folder but identical filename\n\nsource\n\n\nTifSegmentationDataLoaders.from_label_func\n\n TifSegmentationDataLoaders.from_label_func (path, fnames, label_func,\n                                             y_block=&lt;function MaskBlock&gt;,\n                                             chans=None, max_val=None,\n                                             extensions=['.tif'],\n                                             valid_pct=0.2, seed=None,\n                                             splitter=None, codes=None,\n                                             item_tfms=None,\n                                             batch_tfms=None, bs:int=64,\n                                             val_bs:int=None,\n                                             shuffle:bool=True,\n                                             device=None)\n\nCreate from list of fnames in paths with label_func.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath\nstr | Path\n.\nPath to put in DataLoaders\n\n\nfnames\n\n\n\n\n\nlabel_func\n\n\n\n\n\ny_block\nfunction\nMaskBlock\n\n\n\nchans\nNoneType\nNone\n\n\n\nmax_val\nNoneType\nNone\n\n\n\nextensions\nlist\n[‘.tif’]\n\n\n\nvalid_pct\nfloat\n0.2\n\n\n\nseed\nNoneType\nNone\n\n\n\nsplitter\nNoneType\nNone\n\n\n\ncodes\nNoneType\nNone\n\n\n\nitem_tfms\nNoneType\nNone\n\n\n\nbatch_tfms\nNoneType\nNone\n\n\n\nbs\nint\n64\nSize of batch\n\n\nval_bs\nint\nNone\nSize of batch for validation DataLoader\n\n\nshuffle\nbool\nTrue\nWhether to shuffle data\n\n\ndevice\nNoneType\nNone\nDevice to put DataLoaders"
  },
  {
    "objectID": "engines.fastai.data.html#transforms",
    "href": "engines.fastai.data.html#transforms",
    "title": "Custom data used in remote sensing",
    "section": "Transforms",
    "text": "Transforms\n\nsource\n\nScaleToFloatTensor\n\n ScaleToFloatTensor (div=100.0, div_mask=1, split_idx=None)\n\nScale image values to interval 0-1"
  },
  {
    "objectID": "processing.coco.html",
    "href": "processing.coco.html",
    "title": "COCO utilities",
    "section": "",
    "text": "Utility to transform geospatial data to different COCO formats.\nNotes:\n\nIt is possible to specify min_bbox_area to shp_to_coco function to exclude too small polygons. Default value is 16 pixels\nIf a detection is a multipart polygon, only the polygon with the largest area is converted to a shapefile.\n\n\nsource\n\n\n\n nor_theta (theta)\n\nConvert angle to simpler format\n\nsource\n\n\n\n\n calc_bearing (point1, point2)\n\nGet the angle of the rotated bounding box in radians\n\nsource\n\n\n\n\n COCOProcessor (data_path:str, outpath:str, coco_info:dict,\n                coco_licenses:list, coco_categories:list)\n\nHandles Transformations from shapefiles to COCO-format and backwards\n\nsource\n\n\n\n\n detectron2_mask_preds_to_coco_anns (images:list, preds:list)\n\nProcess detectron2 prediction to COCO-annotation polygon format. Returns a dict with COCO-style images and annotations\n\nsource\n\n\n\n\n detectron2_bbox_preds_to_coco_anns (images:list, preds:list)\n\nProcess detectron2 prediction to COCO-annotation polygon format. Returns a dict with COCO-style images and annotations\nAfter tiling the data, COCOProcessor can be used to convert it into a coco style dataset. You need to manually set the coco-info, categories and licenses.\n\ndeadwood_categories = [\n        {'supercategory':'deadwood', 'id':1, 'name': 'Standing'},\n        {'supercategory':'deadwood', 'id':2, 'name': 'Fallen'},\n    ]\n\nfrom datetime import date\n\ncoco_info = {'description': 'dummydataset for example purposes',\n             'version': 0.1,\n             'year': 2022,\n             'contributor': 'Janne Mäyrä',\n             'date_created': date.today().strftime(\"%Y/%m/%d\")\n}\n\ncoco_licenses = {}\n\nBase version with raw original bounding boxes.\n\noutpath = Path('example_data/tiles/')\ncoco_proc = COCOProcessor(outpath, outpath, coco_info, coco_licenses, deadwood_categories)\ncoco_proc.from_shp('label', outfile='coco_norm.json', rotated_bbox=False)\n\n\n\n\nYou can also do rotated bounding boxes.\n\noutpath = Path('example_data/tiles/')\ncoco_proc = COCOProcessor(outpath, outpath, coco_info, coco_licenses, deadwood_categories)\ncoco_proc.from_shp('label', outfile='coco_rot.json', rotated_bbox=True)\n\n\n\n\nUse detectron2 to visualize the differences.\n\nfrom detectron2.data.datasets import register_coco_instances\nfrom detectron2.utils.visualizer import Visualizer\nfrom detectron2.data import MetadataCatalog, DatasetCatalog, build_detection_train_loader, DatasetMapper\nimport random\nimport cv2\n\n\noutpath = Path('example_data/tiles/')\n\nregister_coco_instances('dummy_norm', {}, outpath/'coco_norm.json', outpath/'images')\nregister_coco_instances('dummy_rot', {}, outpath/'coco_rot.json', outpath/'images')\n\n\nfig, axs = plt.subplots(3,4, figsize=(8,4), dpi=150)\nmetadata = MetadataCatalog.get('dummy_norm')\ndataset_dicts = DatasetCatalog.get('dummy_norm')\nddicts = dataset_dicts[:12]\nfor d, ax in zip(ddicts, axs.flatten()):\n    ax.set_xticks([])\n    ax.set_yticks([])\n    img = cv2.imread(d['file_name'])\n    visualizer = Visualizer(img, metadata=metadata, scale=0.9)\n    truth = visualizer.draw_dataset_dict(d)\n    \n    ax.imshow(truth.get_image()[...,::-1])\nplt.tight_layout()\nplt.show()\n\n\n\n\ndetectron2 doesn’t yet support masks and rotated bounding boxes at the same time.\n\nfig, axs = plt.subplots(3,4, figsize=(8,4), dpi=150)\nmetadata = MetadataCatalog.get('dummy_rot')\ndataset_dicts = DatasetCatalog.get('dummy_rot')\nddicts = dataset_dicts[0:12]\nfor d, ax in zip(ddicts, axs.flatten()):\n    ax.set_xticks([])\n    ax.set_yticks([])\n    img = cv2.imread(d['file_name'])\n    visualizer = Visualizer(img, metadata=metadata, scale=0.9)\n    truth = visualizer.draw_dataset_dict(d)\n    \n    ax.imshow(truth.get_image()[...,::-1])\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "processing.coco.html#cocoprocessor",
    "href": "processing.coco.html#cocoprocessor",
    "title": "COCO utilities",
    "section": "",
    "text": "Utility to transform geospatial data to different COCO formats.\nNotes:\n\nIt is possible to specify min_bbox_area to shp_to_coco function to exclude too small polygons. Default value is 16 pixels\nIf a detection is a multipart polygon, only the polygon with the largest area is converted to a shapefile.\n\n\nsource\n\n\n\n nor_theta (theta)\n\nConvert angle to simpler format\n\nsource\n\n\n\n\n calc_bearing (point1, point2)\n\nGet the angle of the rotated bounding box in radians\n\nsource\n\n\n\n\n COCOProcessor (data_path:str, outpath:str, coco_info:dict,\n                coco_licenses:list, coco_categories:list)\n\nHandles Transformations from shapefiles to COCO-format and backwards\n\nsource\n\n\n\n\n detectron2_mask_preds_to_coco_anns (images:list, preds:list)\n\nProcess detectron2 prediction to COCO-annotation polygon format. Returns a dict with COCO-style images and annotations\n\nsource\n\n\n\n\n detectron2_bbox_preds_to_coco_anns (images:list, preds:list)\n\nProcess detectron2 prediction to COCO-annotation polygon format. Returns a dict with COCO-style images and annotations\nAfter tiling the data, COCOProcessor can be used to convert it into a coco style dataset. You need to manually set the coco-info, categories and licenses.\n\ndeadwood_categories = [\n        {'supercategory':'deadwood', 'id':1, 'name': 'Standing'},\n        {'supercategory':'deadwood', 'id':2, 'name': 'Fallen'},\n    ]\n\nfrom datetime import date\n\ncoco_info = {'description': 'dummydataset for example purposes',\n             'version': 0.1,\n             'year': 2022,\n             'contributor': 'Janne Mäyrä',\n             'date_created': date.today().strftime(\"%Y/%m/%d\")\n}\n\ncoco_licenses = {}\n\nBase version with raw original bounding boxes.\n\noutpath = Path('example_data/tiles/')\ncoco_proc = COCOProcessor(outpath, outpath, coco_info, coco_licenses, deadwood_categories)\ncoco_proc.from_shp('label', outfile='coco_norm.json', rotated_bbox=False)\n\n\n\n\nYou can also do rotated bounding boxes.\n\noutpath = Path('example_data/tiles/')\ncoco_proc = COCOProcessor(outpath, outpath, coco_info, coco_licenses, deadwood_categories)\ncoco_proc.from_shp('label', outfile='coco_rot.json', rotated_bbox=True)\n\n\n\n\nUse detectron2 to visualize the differences.\n\nfrom detectron2.data.datasets import register_coco_instances\nfrom detectron2.utils.visualizer import Visualizer\nfrom detectron2.data import MetadataCatalog, DatasetCatalog, build_detection_train_loader, DatasetMapper\nimport random\nimport cv2\n\n\noutpath = Path('example_data/tiles/')\n\nregister_coco_instances('dummy_norm', {}, outpath/'coco_norm.json', outpath/'images')\nregister_coco_instances('dummy_rot', {}, outpath/'coco_rot.json', outpath/'images')\n\n\nfig, axs = plt.subplots(3,4, figsize=(8,4), dpi=150)\nmetadata = MetadataCatalog.get('dummy_norm')\ndataset_dicts = DatasetCatalog.get('dummy_norm')\nddicts = dataset_dicts[:12]\nfor d, ax in zip(ddicts, axs.flatten()):\n    ax.set_xticks([])\n    ax.set_yticks([])\n    img = cv2.imread(d['file_name'])\n    visualizer = Visualizer(img, metadata=metadata, scale=0.9)\n    truth = visualizer.draw_dataset_dict(d)\n    \n    ax.imshow(truth.get_image()[...,::-1])\nplt.tight_layout()\nplt.show()\n\n\n\n\ndetectron2 doesn’t yet support masks and rotated bounding boxes at the same time.\n\nfig, axs = plt.subplots(3,4, figsize=(8,4), dpi=150)\nmetadata = MetadataCatalog.get('dummy_rot')\ndataset_dicts = DatasetCatalog.get('dummy_rot')\nddicts = dataset_dicts[0:12]\nfor d, ax in zip(ddicts, axs.flatten()):\n    ax.set_xticks([])\n    ax.set_yticks([])\n    img = cv2.imread(d['file_name'])\n    visualizer = Visualizer(img, metadata=metadata, scale=0.9)\n    truth = visualizer.draw_dataset_dict(d)\n    \n    ax.imshow(truth.get_image()[...,::-1])\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "engines.detectron2.augmentations.html",
    "href": "engines.detectron2.augmentations.html",
    "title": "Data augmentation for detectron2 models",
    "section": "",
    "text": "RotationTransform.apply_rotated_box\n\n RotationTransform.apply_rotated_box (rotated_boxes)\n\nrotated_boxes should be a N*5 array-like, containing N couples of(x_center, y_center, width, height, angle) boxes\n\nsource\n\n\nVFlip_rotated_box\n\n VFlip_rotated_box (transform, rotated_boxes)\n\nApply the vertical flip transform on rotated boxes.\nArgs: rotated_boxes (ndarray): Nx5 floating point array of (x_center, y_center, width, height, angle_degrees) format in absolute coodinates\n\nsource\n\n\nbuild_aug_transforms\n\n build_aug_transforms (cfg:detectron2.config.config.CfgNode,\n                       flip_horiz:bool=True, flip_vert:bool=False,\n                       max_rotate:int=10,\n                       brightness_limits:Tuple[int,int]=(0.8, 1.4),\n                       contrast_limits:Tuple[int,int]=(0.8, 1.4),\n                       saturation_limits:Tuple[int,int]=(0.8, 1.4),\n                       p_lighting:float=0.75)\n\nBuild a list of detectron2 augmentations"
  },
  {
    "objectID": "examples.fastai.segmentation.html",
    "href": "examples.fastai.segmentation.html",
    "title": "Using fastai for segmentation",
    "section": "",
    "text": "from pathlib import Path\nfrom drone_detector.processing.tiling import *\nimport os\nfrom fastai.vision.all import *\nfrom drone_detector.engines.fastai.data import *\n\n/opt/conda/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\noutpath = Path('../data/historic_map/processed/raster_tiles/')\n\nfnames = [Path(outpath/f) for f in os.listdir(outpath)]\n\ndls = SegmentationDataLoaders.from_label_func('../data/historic_map/', bs=16,\n                                              codes=['Marshes'],\n                                              fnames=fnames,\n                                              label_func=partial(label_from_different_folder,\n                                                                 original_folder='raster_tiles',\n                                                                 new_folder='mask_tiles'),\n                                              batch_tfms = [\n                                                  *aug_transforms(max_rotate=0., max_warp=0.),\n                                                  Normalize.from_stats(*imagenet_stats)\n                                              ])\n\n/opt/conda/lib/python3.9/site-packages/torch/_tensor.py:1051: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n  ret = func(*args, **kwargs)\n\n\nlabel_from_different_folder is a helper located in drone_detector.engines.fastai.data. That module also contains helpers to use with for instance multispectral images or time series of images.\n\ndls.show_batch(max_n=16)\n\n\n\n\nTrain basic U-Net, using pretrained Resnet50 as the encoder. to_fp16() tells our model to use half precision training, thus using less memory. Loss function is FocalLossFlat, and for segmentation we need to specify axis=1. Metrics are Dice and JaccardCoeff, fairly standard segmentation metrics.\n\nlearn = unet_learner(dls, resnet50, pretrained=True, n_in=3, n_out=2,\n                     metrics=[Dice(), JaccardCoeff()], loss_func=FocalLossFlat(axis=1)\n                    ).to_fp16()\n\nSearch for a suitable learning rate.\n\nlearn.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=3.630780702224001e-05)\n\n\n\n\n\nTrain the model for 2 epochs with encoder layers frozen and 10 epochs with all layers unfrozen.\n\nfrom fastai.callback.progress import ShowGraphCallback\n\n\nlearn.fine_tune(10, freeze_epochs=2, base_lr=3e-5, cbs=ShowGraphCallback)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ndice\njaccard_coeff\ntime\n\n\n\n\n0\n0.086749\n0.066611\n0.050679\n0.025998\n00:13\n\n\n1\n0.070000\n0.036666\n0.721976\n0.564916\n00:13\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ndice\njaccard_coeff\ntime\n\n\n\n\n0\n0.031274\n0.025833\n0.810985\n0.682064\n00:13\n\n\n1\n0.030273\n0.021505\n0.839143\n0.722865\n00:13\n\n\n2\n0.029289\n0.019079\n0.852387\n0.742747\n00:13\n\n\n3\n0.027417\n0.048365\n0.708193\n0.548218\n00:13\n\n\n4\n0.030767\n0.020879\n0.826265\n0.703961\n00:13\n\n\n5\n0.027386\n0.018578\n0.867091\n0.765366\n00:13\n\n\n6\n0.025325\n0.017101\n0.863993\n0.760552\n00:13\n\n\n7\n0.022994\n0.016151\n0.879036\n0.784179\n00:13\n\n\n8\n0.021048\n0.015779\n0.878420\n0.783198\n00:13\n\n\n9\n0.019957\n0.015841\n0.876572\n0.780265\n00:13\n\n\n\n\n\n\n\n\nReturn to full precision.\n\nlearn.to_fp32()\n\n&lt;fastai.learner.Learner&gt;\n\n\nCheck results.\n\nlearn.show_results(max_n=8)\n\n\n\n\n\n\n\n\n\n\n\n\npreds = learn.get_preds(with_input=False, with_decoded=False)\n\n\n\n\n\n\n\n\nExport the model to use later.\n\nlearn.path = Path('../data/historic_map/models')\nlearn.export('resnet50_focalloss_swamps.pkl')\n\nSome helper functions for inference, such as removing all resizing transforms.\n\ndef label_func(fn):\n    return str(fn).replace('raster_tiles', 'mask_tiles')\n\n@patch \ndef remove(self:Pipeline, t):\n    for i,o in enumerate(self.fs):\n        if isinstance(o, t.__class__): self.fs.pop(i)\n            \n@patch\ndef set_base_transforms(self:DataLoader):\n    attrs = ['after_item', 'after_batch']\n    for i, attr in enumerate(attrs):\n        tfms = getattr(self, attr)\n        for j, o in enumerate(tfms):\n            if hasattr(o, 'size'):\n                tfms.remove(o)\n            setattr(self, attr, tfms)\n\nLoad learners and remove all resizing transforms. If you run out of memory just restart the kernel.\n\ntestlearn = load_learner('../data/historic_map/models/resnet50_focalloss_swamps.pkl', cpu=False)\ntestlearn.dls.valid.set_base_transforms()\n\nThe model is tested with 3 different map patches from different areas and sizes. Two of the images are from 1965 and two from 1984. Image sizes vary between 600x600 and 1500x1500 pixels.\n\nimport PIL\n\n\ndef unet_predict(fn):\n    image = np.array(PIL.Image.open(fn))\n    mask = testlearn.predict(PILImage.create(image))[0].numpy()\n    img = image\n    img[:,:,0][mask==0] = 0\n    img[:,:,1][mask==0] = 0\n    img[:,:,2][mask==0] = 0\n    img = PIL.Image.fromarray(img.astype(np.uint8))\n    return img\n\n\ntest_images = [f'../data/historic_map/test_patches/{f}' for f in os.listdir('../data/historic_map/test_patches/')]\n\nFirst result.\n\npatch_pred = unet_predict(test_images[0])\n\nfig, axs = plt.subplots(1,2, figsize=(10,5),dpi=300)\nfor a in axs:\n    a.set_yticks([])\n    a.set_xticks([])\naxs[0].imshow(PIL.Image.open(test_images[0]))\naxs[1].imshow(patch_pred)\naxs[0].set_title(test_images[0].split('/')[-1])\naxs[1].set_title('Predicted marshes')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nSecond result\n\npatch_pred = unet_predict(test_images[1])\n\nfig, axs = plt.subplots(1,2, figsize=(10,5),dpi=300)\nfor a in axs:\n    a.set_yticks([])\n    a.set_xticks([])\naxs[0].imshow(PIL.Image.open(test_images[1]))\naxs[1].imshow(patch_pred)\naxs[0].set_title(test_images[1].split('/')[-1])\naxs[1].set_title('Predicted marshes')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThird result\n\npatch_pred = unet_predict(test_images[3])\n\nfig, axs = plt.subplots(1,2, figsize=(10,5),dpi=300)\nfor a in axs:\n    a.set_yticks([])\n    a.set_xticks([])\naxs[0].imshow(PIL.Image.open(test_images[3]))\naxs[1].imshow(patch_pred)\naxs[0].set_title(test_images[3].split('/')[-1])\naxs[1].set_title('Predicted marshes')\nplt.show()"
  },
  {
    "objectID": "engines.fastai.predict.html",
    "href": "engines.fastai.predict.html",
    "title": "CLI for inference using fastai models",
    "section": "",
    "text": "Remove all transformations so predict_segmentation works with any image size and returns same sized output than input.\n\nsource\n\nDataLoader.set_base_transforms\n\n DataLoader.set_base_transforms ()\n\nRemoves all transforms with a size parameter\n\nsource\n\n\nPipeline.remove\n\n Pipeline.remove (t)\n\nRemove an instance of t from self if present\n\nsource\n\n\npredict_segmentation\n\n predict_segmentation (path_to_model:str, path_to_image:str, outfile:str,\n                       processing_dir:str='temp', tile_size:int=400,\n                       tile_overlap:int=100, use_tta:bool=False)\n\nSegment image into land cover classes with a pretrained models TODO save also information about label and class TODO add test-time augmentations\n\nsource\n\n\npredict_segmentation_fastai\n\n predict_segmentation_fastai (path_to_model:str, path_to_image:str,\n                              outfile:str, processing_dir:str='temp',\n                              tile_size:int=400, tile_overlap:int=100,\n                              use_tta:bool=False)\n\nCLI for semantic segmentation with fastai\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath_to_model\nstr\n\nPath to pretrained model file\n\n\npath_to_image\nstr\n\nPath to image to annotate\n\n\noutfile\nstr\n\nPath and filename for output raster\n\n\nprocessing_dir\nstr\ntemp\nDirectory to save the intermediate tiles. Deleted after use\n\n\ntile_size\nint\n400\nTile size to use. Default 400x400px tiles\n\n\ntile_overlap\nint\n100\nTile overlap to use. Default 100px\n\n\nuse_tta\nbool\nFalse\nUse test-time augmentation"
  },
  {
    "objectID": "engines.detectron2.predict.html",
    "href": "engines.detectron2.predict.html",
    "title": "CLI for using Detectron2 models for inference",
    "section": "",
    "text": "source\n\npredict_bboxes\n\n predict_bboxes (path_to_model_files:str, path_to_image:str, outfile:str,\n                 processing_dir:str='temp', tile_size:int=400,\n                 tile_overlap:int=100, coco_set:str=None,\n                 use_tta:bool=True, postproc_results:bool=True)\n\nDetect bounding boxes from a new image using a pretrained model\n\nsource\n\n\npredict_bboxes_detectron2\n\n predict_bboxes_detectron2 (path_to_model_files:str, path_to_image:str,\n                            outfile:str, processing_dir:str='temp',\n                            tile_size:int=400, tile_overlap:int=100,\n                            coco_set:str=None, use_tta:bool=False,\n                            postproc_results:bool=False)\n\nCLI for bbox prediction with detectron2\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath_to_model_files\nstr\n\nPath to the folder containing config and weights\n\n\npath_to_image\nstr\n\nPath to image to annotate\n\n\noutfile\nstr\n\nPath and filename for output raster\n\n\nprocessing_dir\nstr\ntemp\nDirectory to save the intermediate tiles. Deleted after use.\n\n\ntile_size\nint\n400\nTile size to use. Default 400x400px tiles\n\n\ntile_overlap\nint\n100\nTile overlap to use. Default 100px\n\n\ncoco_set\nstr\nNone\nPath to json file for the coco dataset the model was trained on. None defaults to dummy classes\n\n\nuse_tta\nbool\nFalse\nUse test-time augmentation?\n\n\npostproc_results\nbool\nFalse\nFilter predicted masks\n\n\n\n\nsource\n\n\npredict_instance_masks\n\n predict_instance_masks (path_to_model_files:str, path_to_image:str,\n                         outfile:str, processing_dir:str='temp',\n                         tile_size:int=400, tile_overlap:int=100,\n                         coco_set:str=None, use_tta:bool=False,\n                         postproc_results:bool=False,\n                         smooth_preds:bool=False)\n\nSegment instance masks from a new image using a pretrained model\n\nsource\n\n\npredict_instance_masks_detectron2\n\n predict_instance_masks_detectron2 (path_to_model_files:str,\n                                    path_to_image:str, outfile:str,\n                                    processing_dir:str='temp',\n                                    tile_size:int=400,\n                                    tile_overlap:int=100,\n                                    coco_set:str=None, use_tta:bool=False,\n                                    postproc_results:bool=False,\n                                    smooth_preds:bool=False)\n\nCLI for instance segmentation with detectron2\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath_to_model_files\nstr\n\nPath to the folder containing config and weights\n\n\npath_to_image\nstr\n\nPath to image to annotate\n\n\noutfile\nstr\n\nPath and filename for output raster\n\n\nprocessing_dir\nstr\ntemp\nDirectory to save the intermediate tiles. Deleted after use\n\n\ntile_size\nint\n400\nTile size to use. Default 400x400px tiles\n\n\ntile_overlap\nint\n100\nTile overlap to use. Default 100px\n\n\ncoco_set\nstr\nNone\nPath to json file for the coco dataset the model was trained on. None defaults to dummy classes\n\n\nuse_tta\nbool\nFalse\nUse test-time augmentation?\n\n\npostproc_results\nbool\nFalse\nFilter predicted masks\n\n\nsmooth_preds\nbool\nFalse\nRun fill_holes and dilate_erode to masks. Not implemented yet"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "GIS-data and deep learning",
    "section": "",
    "text": "drone_detector was originally a python package for automatic deadwood detection or segmentation from RGB UAV imagery. It contains functions and helpers to use various GIS data with fastai and detectron2."
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "GIS-data and deep learning",
    "section": "Installation",
    "text": "Installation\n\nInstalling locally\nInstalling the required packages is fairly tricky, because some of them are easiest to install via conda (geopandas and GDAL), some via pip (pytorch) and for detectron2 it is ofrequired to specify which prebuilt package to use.\nRepository contains two installation scripts, one for development environment which contains packages that are often needed and other for generating the deploy-environment.\nInstall miniconda and run bash -i install_dev_env.sh for dev environment and bash -i install_run_env.sh for deploy-env. Both scripts install all dependencies, create an editable install for this package and test all relevant code aside from examples.\n\n\nRunning with Apptainer/Singularity\nUse provided dronecontainer.def definition file to build Singularity container. Follow instructions on https://cloud.sylabs.io/builder and build the image with\nsingularity build --remote dronecontainer.sif dronecontainer.def"
  },
  {
    "objectID": "index.html#cli-usage",
    "href": "index.html#cli-usage",
    "title": "GIS-data and deep learning",
    "section": "CLI Usage",
    "text": "CLI Usage\n\nfastai\npredict_segmentation_fastai runs pretrained U-Net model for larger image. So far we support only models saved with learner.export().\n\n\nDetectron2\npredict_bboxes_detectron2 and predict_instance_masks_detectron2 can be used to run batch-predictions on new images."
  },
  {
    "objectID": "index.html#publications-using-this-repository",
    "href": "index.html#publications-using-this-repository",
    "title": "GIS-data and deep learning",
    "section": "Publications using this repository",
    "text": "Publications using this repository\n\nDeadwood detection from RGB UAV imagery using Mask R-CNN, manuscript almost ready"
  },
  {
    "objectID": "index.html#other-peoples-work-applied-in-this-repository",
    "href": "index.html#other-peoples-work-applied-in-this-repository",
    "title": "GIS-data and deep learning",
    "section": "Other people’s work applied in this repository",
    "text": "Other people’s work applied in this repository\nThis repository contains parts from\n\nSolaris by CosmiQ Works\npycococreator by waspinator, https://doi.org/10.5281/zenodo.4627206"
  },
  {
    "objectID": "examples.tiling.html",
    "href": "examples.tiling.html",
    "title": "Examples on how to use tiling utilities",
    "section": "",
    "text": "from pathlib import Path\nfrom drone_detector.processing.tiling import *\nimport os, sys\nimport geopandas as gpd\nimport rasterio as rio\nfrom rasterio import plot as rioplot\nimport random\nimport matplotlib.pyplot as plt\n\n/opt/conda/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\nThis example uses deadwood data from Hiidenportti. For this purpose, we use only 5 tiles to speed things up.\n\ntile_folder = Path('../../data/raw/hiidenportti/virtual_plots/train/images/')\nvector_folder = Path('../../data/raw/hiidenportti/virtual_plots/train/vectors/')\n\noutpath = Path('../data/hiidenportti/processed_example/')\n\ntiles = sorted(os.listdir(tile_folder))[:5]\nvectors = sorted([f for f in os.listdir(vector_folder) if f.endswith('geojson')])[:5]\nassert len(tiles) == len(vectors)\n\nThese virtual plots are tiled into 512 times 512 pixel grid. Then, the vector files are tiled using the same grid. By setting min_area_pct to 0.25, we discard all polygons that are cut so that their area is less than 25% of the original area. Tiler.tile_vector discards all grid cells that don’t contain any polygons\n\nfor t in tiles:\n    if not os.path.exists(outpath/t[:-4]): os.makedirs(outpath/t[:-4])\n    shp_fname = t.replace('tif', 'geojson')\n    tilesize = 512\n    tiler = Tiler(outpath=outpath/t[:-4], gridsize_x=tilesize, gridsize_y=tilesize, overlap=(0,0))\n    tiler.tile_raster(str(tile_folder/t))\n    tiler.tile_vector(vector_folder/shp_fname, min_area_pct=0.25)\n\n25it [00:00, 73.64it/s]\n16it [00:00, 38.62it/s]\n25it [00:00, 76.25it/s]\n16it [00:00, 31.12it/s]\n54it [00:00, 62.43it/s]\n40it [00:00, 43.92it/s]\n132it [00:02, 54.29it/s]\n110it [00:02, 41.22it/s]\n25it [00:00, 75.48it/s]\n16it [00:00, 41.42it/s]\n\n\n\nfor d in os.listdir(outpath):\n    print(f\"\"\"{d} was split into {len(os.listdir(outpath/d/'raster_tiles'))} raster cells and {len(os.listdir(outpath/d/\"vector_tiles\"))} vector cells\"\"\")\n\n104_111_Hiidenportti_Chunk4_orto was split into 16 raster cells and 16 vector cells\n104_18_Hiidenportti_Chunk5_orto was split into 16 raster cells and 15 vector cells\n104_118_Hiidenportti_Chunk6_orto was split into 40 raster cells and 35 vector cells\n104_16_Hiidenportti_Chunk5_orto was split into 110 raster cells and 108 vector cells\n104_102_Hiidenportti_Chunk9_orto was split into 16 raster cells and 16 vector cells\n\n\n\nex_file = random.sample(os.listdir(outpath/d/'vector_tiles'), 1)[0]\nfig, axs = plt.subplots(1,2, figsize=(11,5))\nfor a in axs:\n    a.set_xticks([])\n    a.set_yticks([])\nwith rio.open(outpath/d/f\"raster_tiles/{ex_file.replace('geojson', 'tif')}\") as im:\n    rioplot.show(im, ax=axs[0])\n    mask = gpd.read_file(outpath/d/'vector_tiles'/ex_file)\n    rioplot.show(im, ax=axs[1])\n    mask.plot(ax=axs[1], column='groundwood')"
  },
  {
    "objectID": "examples.tiling.html#tiling-dataset-for-object-detection-or-instance-segmentation",
    "href": "examples.tiling.html#tiling-dataset-for-object-detection-or-instance-segmentation",
    "title": "Examples on how to use tiling utilities",
    "section": "",
    "text": "from pathlib import Path\nfrom drone_detector.processing.tiling import *\nimport os, sys\nimport geopandas as gpd\nimport rasterio as rio\nfrom rasterio import plot as rioplot\nimport random\nimport matplotlib.pyplot as plt\n\n/opt/conda/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\nThis example uses deadwood data from Hiidenportti. For this purpose, we use only 5 tiles to speed things up.\n\ntile_folder = Path('../../data/raw/hiidenportti/virtual_plots/train/images/')\nvector_folder = Path('../../data/raw/hiidenportti/virtual_plots/train/vectors/')\n\noutpath = Path('../data/hiidenportti/processed_example/')\n\ntiles = sorted(os.listdir(tile_folder))[:5]\nvectors = sorted([f for f in os.listdir(vector_folder) if f.endswith('geojson')])[:5]\nassert len(tiles) == len(vectors)\n\nThese virtual plots are tiled into 512 times 512 pixel grid. Then, the vector files are tiled using the same grid. By setting min_area_pct to 0.25, we discard all polygons that are cut so that their area is less than 25% of the original area. Tiler.tile_vector discards all grid cells that don’t contain any polygons\n\nfor t in tiles:\n    if not os.path.exists(outpath/t[:-4]): os.makedirs(outpath/t[:-4])\n    shp_fname = t.replace('tif', 'geojson')\n    tilesize = 512\n    tiler = Tiler(outpath=outpath/t[:-4], gridsize_x=tilesize, gridsize_y=tilesize, overlap=(0,0))\n    tiler.tile_raster(str(tile_folder/t))\n    tiler.tile_vector(vector_folder/shp_fname, min_area_pct=0.25)\n\n25it [00:00, 73.64it/s]\n16it [00:00, 38.62it/s]\n25it [00:00, 76.25it/s]\n16it [00:00, 31.12it/s]\n54it [00:00, 62.43it/s]\n40it [00:00, 43.92it/s]\n132it [00:02, 54.29it/s]\n110it [00:02, 41.22it/s]\n25it [00:00, 75.48it/s]\n16it [00:00, 41.42it/s]\n\n\n\nfor d in os.listdir(outpath):\n    print(f\"\"\"{d} was split into {len(os.listdir(outpath/d/'raster_tiles'))} raster cells and {len(os.listdir(outpath/d/\"vector_tiles\"))} vector cells\"\"\")\n\n104_111_Hiidenportti_Chunk4_orto was split into 16 raster cells and 16 vector cells\n104_18_Hiidenportti_Chunk5_orto was split into 16 raster cells and 15 vector cells\n104_118_Hiidenportti_Chunk6_orto was split into 40 raster cells and 35 vector cells\n104_16_Hiidenportti_Chunk5_orto was split into 110 raster cells and 108 vector cells\n104_102_Hiidenportti_Chunk9_orto was split into 16 raster cells and 16 vector cells\n\n\n\nex_file = random.sample(os.listdir(outpath/d/'vector_tiles'), 1)[0]\nfig, axs = plt.subplots(1,2, figsize=(11,5))\nfor a in axs:\n    a.set_xticks([])\n    a.set_yticks([])\nwith rio.open(outpath/d/f\"raster_tiles/{ex_file.replace('geojson', 'tif')}\") as im:\n    rioplot.show(im, ax=axs[0])\n    mask = gpd.read_file(outpath/d/'vector_tiles'/ex_file)\n    rioplot.show(im, ax=axs[1])\n    mask.plot(ax=axs[1], column='groundwood')"
  },
  {
    "objectID": "examples.tiling.html#tiling-dataset-for-semantic-segmentation",
    "href": "examples.tiling.html#tiling-dataset-for-semantic-segmentation",
    "title": "Examples on how to use tiling utilities",
    "section": "Tiling dataset for semantic segmentation",
    "text": "Tiling dataset for semantic segmentation\n\noutpath = Path('../data/hiidenportti/processed_unet_example/')\n\ntiles = sorted(os.listdir(tile_folder))[:5]\nvectors = sorted([f for f in os.listdir(vector_folder) if f.endswith('geojson')])[:5]\nassert len(tiles) == len(vectors)\n\nFor semantic segmentation, we split the data into 256 times 256 pixel grid. Vector files are then tiled and rasterized to the same grid, in such way that raster images are saved. This method saves target mask for each cell even if they don’t contain any masks.\n\nfor t in tiles:\n    if not os.path.exists(outpath/t[:-4]): os.makedirs(outpath/t[:-4])\n    shp_fname = t.replace('tif', 'geojson')\n    tilesize = 256\n    tiler = Tiler(outpath=outpath/t[:-4], gridsize_x=tilesize, gridsize_y=tilesize, overlap=(0,0))\n    tiler.tile_raster(str(tile_folder/t))\n    tiler.tile_and_rasterize_vector(tile_folder/t, vector_folder/shp_fname, column='groundwood')\n\n81it [00:00, 137.19it/s]\n64it [00:01, 54.68it/s]\n81it [00:00, 145.42it/s]\n64it [00:01, 43.21it/s]\n187it [00:03, 54.60it/s]\n160it [00:03, 51.94it/s]\n483it [00:03, 129.74it/s]\n440it [00:18, 24.27it/s]\n81it [00:00, 145.25it/s]\n64it [00:01, 56.46it/s]\n\n\n\nfor d in os.listdir(outpath):\n    print(f\"\"\"{d} was split into {len(os.listdir(outpath/d/'raster_tiles'))} raster cells and {len(os.listdir(outpath/d/\"rasterized_vector_tiles\"))} vector cells\"\"\")\n\n104_111_Hiidenportti_Chunk4_orto was split into 64 raster cells and 64 vector cells\n104_18_Hiidenportti_Chunk5_orto was split into 64 raster cells and 64 vector cells\n104_118_Hiidenportti_Chunk6_orto was split into 160 raster cells and 160 vector cells\n104_16_Hiidenportti_Chunk5_orto was split into 440 raster cells and 440 vector cells\n104_102_Hiidenportti_Chunk9_orto was split into 64 raster cells and 64 vector cells\n\n\n\nex_file = random.sample(os.listdir(outpath/d/'rasterized_vector_tiles'), 1)[0]\nfig, axs = plt.subplots(1,2, figsize=(11,5))\nfor a in axs:\n    a.set_xticks([])\n    a.set_yticks([])\nwith rio.open(outpath/d/\"raster_tiles\"/ex_file) as im:\n    rioplot.show(im, ax=axs[0])\nwith rio.open(outpath/d/\"rasterized_vector_tiles\"/ex_file) as mask:\n    rioplot.show(mask, ax=axs[1])"
  },
  {
    "objectID": "examples.tiling.html#tiling-non-geospatial-data",
    "href": "examples.tiling.html#tiling-non-geospatial-data",
    "title": "Examples on how to use tiling utilities",
    "section": "Tiling non-geospatial data",
    "text": "Tiling non-geospatial data\nSo far Tiler doesn’t really support data without sensible geotransform. It is, however, possible to work around and might be available in the future.\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nmap_tile = '../data/historic_map/raw/kartta.png'\nmask_tile = '../data/historic_map/raw/swampbin.png'\noutpath = Path('../data/historic_map/processed')\n\n\ntilesize = 224\nmap_tiler = Tiler(outpath=outpath, gridsize_x=tilesize, gridsize_y=tilesize, overlap=(0,0))\nmap_tiler.tile_raster(map_tile)\n\n378it [00:10, 37.79it/s]\n\n\n\nfrom osgeo import gdal\n\nWe have to manually set pixel coordinates as geocoordinates, provide GCPs and change the order of projWin\n\nraster = gdal.Open(map_tile)\nfor row in (map_tiler.grid.itertuples()):\n    coords = list(row.geometry.exterior.coords)[:-1]\n    gcp_list = []\n    gcp_list.append(gdal.GCP(coords[0][0],coords[0][1],1,0,-0))\n    gcp_list.append(gdal.GCP(coords[1][0],coords[1][1],1,224,-0))\n    gcp_list.append(gdal.GCP(coords[2][0],coords[2][1],1,224,-224))\n    gcp_list.append(gdal.GCP(coords[3][0],coords[3][1],1,0,-224))\n    translate_kwargs = {'GCPs': gcp_list}\n    tempraster = gdal.Translate(f'{map_tiler.raster_path}/{row.cell}.tif', raster,\n                                projWin=[row.geometry.bounds[0], row.geometry.bounds[1], \n                                         row.geometry.bounds[2], row.geometry.bounds[3]],\n                                **translate_kwargs\n                               )\n    tempraster = None\nraster = None\n\n\nmap_tiler.raster_path = outpath/'mask_tiles'\nmap_tiler.tile_raster(mask_tile)\n\n378it [00:04, 93.63it/s] \n\n\n\nraster = gdal.Open(mask_tile)\nfor row in (map_tiler.grid.itertuples()):\n    coords = list(row.geometry.exterior.coords)[:-1]\n    gcp_list = []\n    gcp_list.append(gdal.GCP(coords[0][0],coords[0][1],1,0,-0))\n    gcp_list.append(gdal.GCP(coords[1][0],coords[1][1],1,224,-0))\n    gcp_list.append(gdal.GCP(coords[2][0],coords[2][1],1,224,-224))\n    gcp_list.append(gdal.GCP(coords[3][0],coords[3][1],1,0,-224))\n    translate_kwargs = {'GCPs': gcp_list,\n                        'outputType': gdal.GDT_Byte}\n    tempraster = gdal.Translate(f'{map_tiler.raster_path}/{row.cell}.tif', raster,\n                                projWin=[row.geometry.bounds[0], row.geometry.bounds[1], \n                                         row.geometry.bounds[2], row.geometry.bounds[3]],\n                                **translate_kwargs\n                               )\n    tempraster = None\nraster = None\n\nExample data here is historical map from Evo area, and the target mask is for marshes and swamps.\n\nex_file = random.sample(os.listdir(outpath/'raster_tiles'), 1)[0]\nfig, axs = plt.subplots(1,2, figsize=(11,5))\nfor a in axs:\n    a.set_xticks([])\n    a.set_yticks([])\nwith rio.open(outpath/\"raster_tiles\"/ex_file) as im:\n    rioplot.show(im, ax=axs[0])\nwith rio.open(outpath/\"mask_tiles\"/ex_file) as mask:\n    rioplot.show(mask, ax=axs[1])"
  }
]