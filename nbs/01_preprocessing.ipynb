{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "> Tiling and such"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from pathlib import Path\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from osgeo import gdal\n",
    "from shapely.geometry import Point, Polygon, box\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from typing import Tuple\n",
    "from tqdm import tqdm\n",
    "from fastcore.basics import *\n",
    "from drone_detector.core import *\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Tiler():\n",
    "    \"Similar functions than Â´solaris.tile.raster_tile' but with suitable dependencies\"\n",
    "    def __init__(self, outpath=None, gridsize_x:int=640, gridsize_y:int=480, \n",
    "                 overlap:Tuple[int, int]=(100, 100)):\n",
    "        store_attr()\n",
    "        self.grid = None\n",
    "        if outpath is not None and not os.path.exists(outpath):\n",
    "            os.makedirs(outpath)\n",
    "            os.makedirs(f'{outpath}/raster_tiles')\n",
    "            os.makedirs(f'{outpath}/vector_tiles')\n",
    "        \n",
    "    def tile_raster(self, path_to_raster:str):\n",
    "        self.grid = make_grid(str(path_to_raster), gridsize_x=self.gridsize_x,\n",
    "                              gridsize_y=self.gridsize_y, overlap=self.overlap)\n",
    "        raster = gdal.Open(path_to_raster)\n",
    "        for row in tqdm(self.grid.itertuples()):\n",
    "            tempraster = gdal.Translate(f'{self.outpath}/raster_tiles/{row.cell}.tif', raster,\n",
    "                                        projWin=[row.geometry.bounds[0], row.geometry.bounds[3], \n",
    "                                                 row.geometry.bounds[2], row.geometry.bounds[1]]\n",
    "                                       )\n",
    "            tempraster = None\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def tile_vector(self, path_to_vector:str):\n",
    "        if self.grid is None:\n",
    "            print('No raster grid specified, use Tiler.tile_raster to determine grid limits')\n",
    "            return\n",
    "        vector = gpd.read_file(path_to_vector)\n",
    "        # God bless spatial index\n",
    "        sindex = vector.sindex\n",
    "        for row in tqdm(self.grid.itertuples()):\n",
    "            possible_matches_index = list(sindex.intersection(row.geometry.bounds))\n",
    "            tempvector = vector.iloc[possible_matches_index].copy()\n",
    "            tempvector = gpd.clip(tempvector, row.geometry, keep_geom_type=True)\n",
    "            # No annotations -> no shapefile\n",
    "            if len(tempvector) == 0: continue\n",
    "            \n",
    "            # Filter too small geometries TODO\n",
    "            tempvector.to_file(f'{self.outpath}/vector_tiles/{row.cell}.shp')\n",
    "        return\n",
    "    \n",
    "def untile_vector(path_to_targets:str, outpath:str, non_max_suppression_thresh:float=0.0):\n",
    "    \"Create single shapefile from predictions\"\n",
    "    pred_files = [f for f in os.listdir(path_to_targets) if f.endswith('.shp')]\n",
    "    gdf = None\n",
    "    for p in tqdm(pred_files):\n",
    "        temp_gdf = gpd.read_file(f'{path_to_targets}/{p}')\n",
    "        if gdf is None: gdf = temp_gdf\n",
    "        else: gdf = gdf.append(temp_gdf)\n",
    "    print(f'{len(gdf)} polygons before non-max suppression')\n",
    "    if non_max_suppression_thresh != 0:\n",
    "        np_bounding_boxes = np.array([b.bounds for b in gdf.geometry])\n",
    "        idxs = non_max_suppression_fast(np_bounding_boxes, overlap_thresh = non_max_suppression_thresh)\n",
    "        gdf = gdf.iloc[idxs]\n",
    "    print(f'{len(gdf)} polygons after non-max suppression')\n",
    "    gdf.to_file(outpath)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COCOProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class COCOProcessor():\n",
    "    \"Handles Transformations from shapefiles to COCO-format and backwards\"\n",
    "    \n",
    "    def __init__(self, data_path:str, outpath:str, coco_info:dict, coco_licenses:list,\n",
    "                 coco_categories:list):\n",
    "        store_attr()\n",
    "        self.raster_path = f'{self.data_path}/raster_tiles'\n",
    "        self.vector_path = f'{self.data_path}/vector_tiles'\n",
    "        \n",
    "        self.coco_dict = {\n",
    "            'info': coco_info,\n",
    "            'licenses': coco_licenses,\n",
    "            'images': [],\n",
    "            'annotations': [],\n",
    "            'categories': coco_categories,\n",
    "            'segment_info': []\n",
    "        }\n",
    "        self.categories = {c['name']:c['id'] for c in self.coco_dict['categories']}\n",
    "        \n",
    "        \n",
    "    def shp_to_coco(self, outfile:str='coco.json'):\n",
    "        \"Process shapefiles from self.vector_path to coco-format and save to self.outpath/outfile\"\n",
    "        vector_tiles = [f for f in os.listdir(self.vector_path) if f.endswith('.shp')]\n",
    "        # If no annotations are in found in raster tile then there is no shapefile for that\n",
    "        raster_tiles = [f'{fname[:-4]}.tif' for fname in vector_tiles]\n",
    "        self.coco_dict['images'] = [{'file_name': raster_tiles[i],\n",
    "                                     'id': i} for i in rangeof(raster_tiles)]\n",
    "        ann_id = 0\n",
    "        for i in tqdm(rangeof(raster_tiles)):\n",
    "            gdf = gpd.read_file(f'{self.vector_path}/{vector_tiles[i]}')\n",
    "            tfmd_gdf = gdf_to_px(gdf, f'{self.raster_path}/{raster_tiles[i]}')\n",
    "            for row in tfmd_gdf.itertuples():\n",
    "                category_id = self.categories[row.label]\n",
    "                self.coco_dict['annotations'].append(_process_shp_to_coco(i, category_id, ann_id, row.geometry))\n",
    "                ann_id += 1\n",
    "        with open(f'{self.outpath}/{outfile}', 'w') as f: json.dump(self.coco_dict, f)\n",
    "\n",
    "        return\n",
    "    \n",
    "    def coco_to_shp(self, coco_path:str=None, outdir:str='predicted_vectors'):\n",
    "        \"TODO handle multipolygons\"\n",
    "        if not os.path.exists(f'{self.outpath}/{outdir}'): os.makedirs(f'{self.outpath}/{outdir}')\n",
    "        if coco_path is None: coco_path = f'{self.outpath}/coco.json'\n",
    "        with open(coco_path) as f:\n",
    "            coco_data = json.load(f)\n",
    "        \n",
    "        annotations = coco_data['annotations']\n",
    "        images = coco_data['images']\n",
    "        categories = coco_data['categories']\n",
    "        for i in tqdm(images):\n",
    "            anns_in_image = [a for a in annotations if a['image_id'] == i['id']]\n",
    "            if len(anns_in_image) == 0: continue\n",
    "            cats = []\n",
    "            polys = []\n",
    "            for a in anns_in_image:\n",
    "                # Single polygon\n",
    "                if len(a['segmentation']) == 1:\n",
    "                    cats.append(a['category_id'])\n",
    "                    xy_coords = [(a['segmentation'][0][i], a['segmentation'][0][i+1]) for i in range(0,len(a['segmentation'][0]),2)]\n",
    "                    xy_coords.append(xy_coords[-1])\n",
    "                    polys.append(Polygon(xy_coords))\n",
    "                # Multipolygon TODO handle better\n",
    "                else: \n",
    "                    for p in rangeof(a['segmentation']):\n",
    "                        cats.append(a['category_id'])\n",
    "                        xy_coords = [(a['segmentation'][p][i], a['segmentation'][p][i+1]) \n",
    "                                     for i in range(0,len(a['segmentation'][p]),2)]\n",
    "                        xy_coords.append(xy_coords[-1])\n",
    "                        polys.append(Polygon(xy_coords))\n",
    "            gdf = gpd.GeoDataFrame({'label':cats, 'geometry':polys})\n",
    "            tfmd_gdf = georegister_px_df(gdf, f'{self.raster_path}/{i[\"file_name\"]}')\n",
    "            tfmd_gdf.to_file(f'{self.outpath}/{outdir}/{i[\"file_name\"][:-4]}.shp')\n",
    "        return\n",
    "\n",
    "def _process_shp_to_coco(image_id, category_id, ann_id, poly:Polygon):\n",
    "    \"TODO handle multipolygons\"\n",
    "    ann_dict = {\n",
    "        'segmentation': [],\n",
    "        'area': None, \n",
    "        'bbox': [],\n",
    "        'category_id': category_id,\n",
    "        'id' : ann_id,\n",
    "        'image_id': image_id,\n",
    "        'iscrowd': 0,\n",
    "    }\n",
    "    ann_dict['bbox'] = [(poly.bounds[0]), \n",
    "                        (poly.bounds[1]), \n",
    "                        (poly.bounds[2]-poly.bounds[0]), \n",
    "                        (poly.bounds[3]-poly.bounds[1])]\n",
    "    ann_dict['area'] = poly.area\n",
    "    if poly.type == 'Polygon':\n",
    "        ann_dict['segmentation'] = [list(sum(poly.exterior.coords[:-1], ()))]\n",
    "    elif poly.type == 'MultiPolygon':\n",
    "        ann_dict['segmentation'] = [list(sum(p.exterior.coords[:-1], ())) for p in list(poly)]\n",
    "    return ann_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Icevision predictions to coco-json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_preds_to_coco(samples:list, preds:list, categories:list=None) -> dict:\n",
    "    coco_dict = {\n",
    "            'info': None,\n",
    "            'licenses': None,\n",
    "            'images': [],\n",
    "            'annotations': [],\n",
    "            'categories': categories,\n",
    "            'segment_info': []\n",
    "        }    \n",
    "    \n",
    "    coco_dict['images'] =  [{'file_name': s['filepath'], 'id': s['imageid']} for s in samples]\n",
    "    \n",
    "    for i, (s, p) in enumerate(zip(samples, preds)):\n",
    "        coco_dict['annotations'].append(_process_pred_to_coco(p, s['imageid'], i))\n",
    "    return coco_dict\n",
    "\n",
    "def _process_pred_to_coco(pred:dict, imageid:int, ann_id:int) -> dict:\n",
    "    anns = []\n",
    "    for i in rangeof(pred['masks']):\n",
    "        ann_dict = {\n",
    "            'segmentation': [],\n",
    "            'area': None,\n",
    "            'iscrowd': 0,\n",
    "            'category_id': None,\n",
    "            'id': ann_id,\n",
    "            'image_id': imageid,\n",
    "            'bbox': []\n",
    "        }\n",
    "        \n",
    "        anns.append([ann_dict])\n",
    "        \n",
    "    return anns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
