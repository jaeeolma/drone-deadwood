{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom losses and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mayrajeo/miniconda3/envs/dronedetector/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729047590/work/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "# export\n",
    "from drone_detector.imports import *\n",
    "from fastai.learner import Metric\n",
    "from fastai.torch_core import *\n",
    "from fastai.metrics import *\n",
    "from fastai.losses import BaseLoss\n",
    "import sklearn.metrics as skm\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "mk_class('ActivationType', **{o:o.lower() for o in ['No', 'Sigmoid', 'Softmax', 'BinarySoftmax']},\n",
    "         doc=\"All possible activation classes for `AccumMetric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def adjusted_R2Score(r2_score, n, k):\n",
    "    \"Calculates adjusted_R2Score based on r2_score, number of observations (n) and number of predictor variables(k)\"\n",
    "    return 1 - (((n-1)/(n-k-1)) * (1 - r2_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def _rrmse(inp, targ):\n",
    "    \"RMSE normalized with mean of the target\"\n",
    "    return torch.sqrt(F.mse_loss(inp, targ)) / targ.mean() * 100\n",
    "\n",
    "rrmse = AccumMetric(_rrmse)\n",
    "rrmse.__doc__ = \"Target mean weighted rmse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _bias(inp, targ):\n",
    "    \"Average bias of predictions\"\n",
    "    inp, targ = flatten_check(inp, targ)\n",
    "    return (inp - targ).sum() / len(targ)\n",
    "\n",
    "bias = AccumMetric(_bias)\n",
    "bias.__doc__ = \"Average bias of predictions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _bias_pct(inp, targ):\n",
    "    \"Mean weighted bias\"\n",
    "    inp, targ = flatten_check(inp, targ)\n",
    "    return 100 * ((inp-targ).sum()/len(targ)) / targ.mean()\n",
    "\n",
    "bias_pct = AccumMetric(_bias_pct)\n",
    "bias_pct.__doc__ = 'Mean weighted bias'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BigEarthNet metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def label_ranking_average_precision_score(sigmoid=True, sample_weight=None):\n",
    "    \"\"\"Label ranking average precision (LRAP) is the average over each ground truth label assigned to each sample, \n",
    "    of the ratio of true vs. total labels with lower score.\"\"\"\n",
    "    activation = ActivationType.Sigmoid if sigmoid else ActivationType.No\n",
    "    return skm_to_fastai(skm.label_ranking_average_precision_score, sample_weight=None, flatten=False, thresh=None, \n",
    "                         activation=activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def label_ranking_loss(sigmoid=True, sample_weight=None):\n",
    "    \"\"\"Compute the average number of label pairs that are incorrectly ordered given y_score \n",
    "    weighted by the size of the label set and the number of labels not in the label set.\"\"\"\n",
    "    activation = ActivationType.Sigmoid if sigmoid else ActivationType.No\n",
    "    return skm_to_fastai(skm.label_ranking_loss, sample_weight=None, flatten=False, thresh=None, \n",
    "                         activation=activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def _one_error(inp, targ):\n",
    "    max_ranks = inp.argmax(axis=1)\n",
    "    faults = 0\n",
    "    for i in range_of(max_ranks):\n",
    "        faults += targ[i,max_ranks[i]]\n",
    "    return 1 - faults/len(max_ranks)\n",
    "    \n",
    "one_error = AccumMetric(_one_error, flatten=False)\n",
    "one_error.__doc__ = \"Rate for which the top ranked label is not among ground truth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def coverage_error(sigmoid=True, sample_weight=None):\n",
    "    \"\"\"Compute how far we need to go through the ranked scores to cover all true labels. \n",
    "    The best value is equal to the average number of labels in y_true per sample.\"\"\"\n",
    "    \n",
    "    activation = ActivationType.Sigmoid if sigmoid else ActivationType.No\n",
    "    return skm_to_fastai(skm.coverage_error, sample_weight=None, flatten=False, thresh=None, activation=activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.learner import Learner\n",
    "class TstLearner(Learner):\n",
    "    def __init__(self,dls=None,model=None,**kwargs): self.pred,self.xb,self.yb = None,None,None\n",
    "\n",
    "def compute_val(met, x1, x2):\n",
    "    met.reset()\n",
    "    vals = [0,6,15,20]\n",
    "    learn = TstLearner()\n",
    "    for i in range(3):\n",
    "        learn.pred,learn.yb = x1[vals[i]:vals[i+1]],(x2[vals[i]:vals[i+1]],)\n",
    "        met.accumulate(learn)\n",
    "    return met.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrap = label_ranking_average_precision_score()\n",
    "lrl = label_ranking_loss()\n",
    "cov = coverage_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.7139, -1.1197,  1.3219, -0.8929, -0.0864, -1.0797, -1.1980, -1.1462,\n",
       "           0.7752, -0.3072],\n",
       "         [ 0.0160,  0.4422,  0.7312, -0.2738, -1.8118, -1.0051, -0.8846,  1.3535,\n",
       "           0.4079,  1.8772],\n",
       "         [-1.4298,  0.9177, -0.1495,  0.8309, -0.4060, -1.6582, -1.7383,  1.2100,\n",
       "          -0.4028,  0.3403],\n",
       "         [ 0.3563, -0.3367, -0.4472,  1.3416,  1.7071,  0.1720,  0.6515, -0.9848,\n",
       "           1.3077, -0.0549],\n",
       "         [-1.3293,  0.7442,  1.0438,  0.8183, -0.0366,  0.5047, -0.6310, -1.1718,\n",
       "           0.1838,  0.2767],\n",
       "         [-0.2497, -2.2051, -1.1139,  0.0787,  0.7320, -0.4195, -0.9570,  2.4100,\n",
       "          -1.0918,  0.7197],\n",
       "         [-0.5669,  1.1043,  0.7297, -0.3731,  1.2886, -0.6209, -0.7776,  1.3258,\n",
       "           0.4056, -1.2151],\n",
       "         [ 0.5525,  0.0561, -0.5651, -0.9875,  0.4355,  0.5439,  1.4667, -0.0607,\n",
       "           0.5053, -0.5647],\n",
       "         [ 0.0292, -2.3633, -0.6284,  0.9104, -0.5364, -0.8675, -0.3276, -0.5341,\n",
       "           0.5528, -0.4312],\n",
       "         [ 0.0761, -0.8560,  1.3815, -1.7832,  0.0498, -1.4050,  1.9722, -0.7927,\n",
       "          -2.4473,  0.2011]]),\n",
       " tensor([[0.3287, 0.2461, 0.7895, 0.2905, 0.4784, 0.2536, 0.2318, 0.2412, 0.6846,\n",
       "          0.4238],\n",
       "         [0.5040, 0.6088, 0.6751, 0.4320, 0.1404, 0.2679, 0.2922, 0.7947, 0.6006,\n",
       "          0.8673],\n",
       "         [0.1931, 0.7146, 0.4627, 0.6966, 0.3999, 0.1600, 0.1495, 0.7703, 0.4006,\n",
       "          0.5843],\n",
       "         [0.5881, 0.4166, 0.3900, 0.7928, 0.8465, 0.5429, 0.6573, 0.2719, 0.7871,\n",
       "          0.4863],\n",
       "         [0.2093, 0.6779, 0.7396, 0.6939, 0.4909, 0.6236, 0.3473, 0.2365, 0.5458,\n",
       "          0.5687],\n",
       "         [0.4379, 0.0993, 0.2472, 0.5197, 0.6752, 0.3966, 0.2775, 0.9176, 0.2513,\n",
       "          0.6725],\n",
       "         [0.3619, 0.7511, 0.6747, 0.4078, 0.7839, 0.3496, 0.3148, 0.7902, 0.6000,\n",
       "          0.2288],\n",
       "         [0.6347, 0.5140, 0.3624, 0.2714, 0.6072, 0.6327, 0.8126, 0.4848, 0.6237,\n",
       "          0.3624],\n",
       "         [0.5073, 0.0860, 0.3479, 0.7131, 0.3690, 0.2958, 0.4188, 0.3696, 0.6348,\n",
       "          0.3938],\n",
       "         [0.5190, 0.2982, 0.7992, 0.1439, 0.5125, 0.1970, 0.8778, 0.3116, 0.0796,\n",
       "          0.5501]]),\n",
       " tensor([[1, 1, 1, 0, 0, 0, 0, 0, 0, 1],\n",
       "         [1, 1, 1, 0, 0, 1, 1, 1, 1, 0],\n",
       "         [1, 0, 0, 1, 1, 1, 1, 1, 1, 0],\n",
       "         [1, 1, 0, 0, 1, 1, 1, 0, 1, 0],\n",
       "         [0, 0, 1, 0, 1, 1, 0, 0, 1, 0],\n",
       "         [1, 1, 1, 1, 0, 1, 0, 1, 0, 1],\n",
       "         [0, 1, 0, 1, 1, 0, 0, 1, 1, 1],\n",
       "         [0, 1, 1, 0, 0, 1, 0, 0, 0, 1],\n",
       "         [0, 0, 1, 1, 1, 0, 0, 0, 1, 0],\n",
       "         [1, 0, 1, 1, 1, 1, 0, 1, 0, 0]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_1 = torch.randn(10,10)\n",
    "x_2 = torch.randint(2,(10,10))\n",
    "x_1, torch.sigmoid(x_1), x_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4375"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_val(lrl, x_1, x_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6836692176870748"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_val(lrap, x_1, x_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.8"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_val(cov, x_1, x_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3000)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_error(x_1, x_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
