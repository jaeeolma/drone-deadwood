{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom losses and metrics\n",
    "\n",
    "> Collection of various loss and metric functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from drone_detector.imports import *\n",
    "from fastai.learner import Metric\n",
    "from fastai.torch_core import *\n",
    "from fastai.metrics import *\n",
    "from fastai.losses import BaseLoss\n",
    "import sklearn.metrics as skm\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "mk_class('ActivationType', **{o:o.lower() for o in ['No', 'Sigmoid', 'Softmax', 'BinarySoftmax']},\n",
    "         doc=\"All possible activation classes for `AccumMetric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def adjusted_R2Score(r2_score, n, k):\n",
    "    \"Calculates adjusted_R2Score based on r2_score, number of observations (n) and number of predictor variables(k)\"\n",
    "    return 1 - (((n-1)/(n-k-1)) * (1 - r2_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def _rrmse(inp, targ):\n",
    "    \"RMSE normalized with mean of the target\"\n",
    "    return torch.sqrt(F.mse_loss(inp, targ)) / targ.mean() * 100\n",
    "\n",
    "rrmse = AccumMetric(_rrmse)\n",
    "rrmse.__doc__ = \"Relative RMSE. Normalized with mean of the target\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### rrmse\n",
       "\n",
       ">      rrmse (preds, targs)\n",
       "\n",
       "Target mean weighted rmse"
      ],
      "text/plain": [
       "<nbdev.showdoc.BasicMarkdownRenderer>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(rrmse, name='rrmse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def _bias(inp, targ):\n",
    "    \"Average bias of predictions\"\n",
    "    inp, targ = flatten_check(inp, targ)\n",
    "    return (inp - targ).sum() / len(targ)\n",
    "\n",
    "bias = AccumMetric(_bias)\n",
    "bias.__doc__ = \"Average bias of predictions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### bias\n",
       "\n",
       ">      bias (preds, targs)\n",
       "\n",
       "Average bias of predictions"
      ],
      "text/plain": [
       "<nbdev.showdoc.BasicMarkdownRenderer>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(bias, name='bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def _bias_pct(inp, targ):\n",
    "    \"Mean weighted bias\"\n",
    "    inp, targ = flatten_check(inp, targ)\n",
    "    return 100 * ((inp-targ).sum()/len(targ)) / targ.mean()\n",
    "\n",
    "bias_pct = AccumMetric(_bias_pct)\n",
    "bias_pct.__doc__ = 'Mean weighted bias, normalized with mean of the target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### bias_pct\n",
       "\n",
       ">      bias_pct (preds, targs)\n",
       "\n",
       "Mean weighted bias, normalized with mean of the target"
      ],
      "text/plain": [
       "<nbdev.showdoc.BasicMarkdownRenderer>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(bias_pct, name='bias_pct')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BigEarthNet metrics\n",
    "\n",
    "> Metrics used in BigEarthNet paper to evaluate multi-label classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def label_ranking_average_precision_score(sigmoid=True, sample_weight=None):\n",
    "    \"\"\"Label ranking average precision (LRAP) is the average over each ground truth label assigned to each sample, \n",
    "    of the ratio of true vs. total labels with lower score.\"\"\"\n",
    "    activation = ActivationType.Sigmoid if sigmoid else ActivationType.No\n",
    "    return skm_to_fastai(skm.label_ranking_average_precision_score, sample_weight=None, flatten=False, thresh=None, \n",
    "                         activation=activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.learner import Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TstLearner(Learner):\n",
    "    def __init__(self,dls=None,model=None,**kwargs): self.pred,self.xb,self.yb = None,None,None\n",
    "\n",
    "def compute_val(met, x1, x2):\n",
    "    met.reset()\n",
    "    vals = [0,6,15,20]\n",
    "    learn = TstLearner()\n",
    "    for i in range(3):\n",
    "        learn.pred,learn.yb = x1[vals[i]:vals[i+1]],(x2[vals[i]:vals[i+1]],)\n",
    "        met.accumulate(learn)\n",
    "    return met.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0175, -1.1692,  0.4717],\n",
       "         [-0.1019,  1.7057,  0.8773],\n",
       "         [-0.6633,  0.0593,  1.3823],\n",
       "         [-0.6155, -0.4767, -1.4756],\n",
       "         [-1.0687,  0.0246, -0.3407],\n",
       "         [ 0.8646, -0.5164,  0.7274],\n",
       "         [-0.7253,  0.6507,  0.2994],\n",
       "         [-0.8993,  0.8574,  0.3334],\n",
       "         [ 0.5395,  0.5471,  0.7207],\n",
       "         [ 0.5087, -0.7967,  0.5395]]),\n",
       " tensor([[0.4956, 0.2370, 0.6158],\n",
       "         [0.4745, 0.8463, 0.7063],\n",
       "         [0.3400, 0.5148, 0.7994],\n",
       "         [0.3508, 0.3830, 0.1861],\n",
       "         [0.2557, 0.5061, 0.4156],\n",
       "         [0.7036, 0.3737, 0.6742],\n",
       "         [0.3262, 0.6572, 0.5743],\n",
       "         [0.2892, 0.7021, 0.5826],\n",
       "         [0.6317, 0.6335, 0.6728],\n",
       "         [0.6245, 0.3107, 0.6317]]),\n",
       " tensor([[1, 0, 0],\n",
       "         [1, 1, 0],\n",
       "         [1, 1, 1],\n",
       "         [0, 0, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 1, 0],\n",
       "         [1, 0, 1],\n",
       "         [0, 1, 0]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_1 = torch.randn(10,3)\n",
    "x_2 = torch.randint(2,(10,3))\n",
    "x_1, torch.sigmoid(x_1), x_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7833333333333332"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrap = label_ranking_average_precision_score()\n",
    "compute_val(lrap, x_1, x_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def label_ranking_loss(sigmoid=True, sample_weight=None):\n",
    "    \"\"\"Compute the average number of label pairs that are incorrectly ordered given y_score \n",
    "    weighted by the size of the label set and the number of labels not in the label set.\"\"\"\n",
    "    activation = ActivationType.Sigmoid if sigmoid else ActivationType.No\n",
    "    return skm_to_fastai(skm.label_ranking_loss, sample_weight=None, flatten=False, thresh=None, \n",
    "                         activation=activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrl = label_ranking_loss()\n",
    "compute_val(lrl, x_1, x_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def _one_error(inp, targ):\n",
    "    max_ranks = inp.argmax(axis=1)\n",
    "    faults = 0\n",
    "    for i in range_of(max_ranks):\n",
    "        faults += targ[i,max_ranks[i]]\n",
    "    return 1 - torch.true_divide(faults, len(max_ranks))\n",
    "    \n",
    "one_error = AccumMetric(_one_error, flatten=False)\n",
    "one_error.__doc__ = \"Rate for which the top ranked label is not among ground truth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### one_error\n",
       "\n",
       ">      one_error (preds, targs)\n",
       "\n",
       "Rate for which the top ranked label is not among ground truth"
      ],
      "text/plain": [
       "<nbdev.showdoc.BasicMarkdownRenderer>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(one_error, name='one_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4000)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_error(x_1, x_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def coverage_error(sigmoid=True, sample_weight=None):\n",
    "    \"\"\"Compute how far we need to go through the ranked scores to cover all true labels. \n",
    "    The best value is equal to the average number of labels in y_true per sample.\"\"\"\n",
    "    \n",
    "    activation = ActivationType.Sigmoid if sigmoid else ActivationType.No\n",
    "    return skm_to_fastai(skm.coverage_error, sample_weight=None, flatten=False, thresh=None, activation=activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov = coverage_error()\n",
    "compute_val(cov, x_1, x_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class JaccardCoeffMulti(DiceMulti):\n",
    "    \"Averaged Jaccard coefficient for multiclass target in segmentation. Excludes background class\"\n",
    "    @property\n",
    "    def value(self):\n",
    "        binary_jaccard_scores = np.array([])\n",
    "        for c in self.inter:\n",
    "            if c > 0:\n",
    "                binary_jaccard_scores = np.append(binary_jaccard_scores, self.inter[c]/(self.union[c] - self.inter[c]) if self.union[c] > 0 else np.nan)\n",
    "        return np.nanmean(binary_jaccard_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.randn(20,6,3,3)\n",
    "x2 = torch.randint(0, 6, (20, 3, 3))\n",
    "pred = x1.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08437537158034053"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_val(JaccardCoeffMulti(), x1, x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object detection metrics and evaluation for shapefiles MOSTLY OBSOLETE, USE GisCOCOEval instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate our collection of predicted masks, we'll compare each of our predicted masks with each of the available target masks for a given input. \n",
    "\n",
    "* A **true positive**, when a prediction-target mask pair has an IoU score which exceeds some predefined threshold\n",
    "* A **false positive**, when a predicted object had no associated ground truth object mask\n",
    "* A **false negative** indicates a ground truth object mask had no associated predicted object mask\n",
    "\n",
    "In the case of multiple detections, the one with the highest confidence is considered to be \"correct\" and others are FP.\n",
    "\n",
    "From these, we can get **Precision** and **Recall**\n",
    "\n",
    "$Precision = \\frac{TP}{TP + FP} = \\frac{TP}{all \\: detections}, Recall = \\frac{TP}{TP+FN} = \\frac{TP}{all \\: ground \\: truths}$\n",
    "\n",
    "And use these to derive other metrics.\n",
    "\n",
    "Typical metrics include **Average Precision** (AP) and **mean Average Precision** (mAP). From these several metrics can be derived:\n",
    "\n",
    "* AP50, AP75, AP[.50:.05:.95] are the most common, with AP[.50:.05:.95] being the primary challenge metric in COCO\n",
    "* AP Across scales: AP<sub>small</sub>, AP<sub>medium</sub>, AP<sub>large</sub>, where small, medium and large have specified areas\n",
    "    * Scales for COCO are less than 32² for small, between 32² and 96² for medium and more than 96² for large, sizes in pixels\n",
    "    * Our data has variable resolution sizes, but on average the resolution is around 0.05m, so small is less than 2.56m², medium is between 2.56m² and 23.04m², and large is more than 23.04m²\n",
    "* **Average Recall** (AR) is also sometimes used similarly, but with restrictions for the number of detections per image   \n",
    "    * It is computed as the area under Recall-IoU -curve for IoU thresholds from [0.5, 1]\n",
    "* All of these can be applied to bounding boxes and masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the following functions assume that you have two `GeoDataFrame`s that have same CRS and matching column `label`. Usage example is the following: \n",
    "\n",
    "```python\n",
    "\n",
    "ground_truth = gpd.read_file(<path_to_ground_truth>)\n",
    "results = gpd.read_file(<path_to_results>)\n",
    "\n",
    "# clip the geodataframes to have same extent\n",
    "results = gpd.clip(results, box(*ground_truth.total_bounds), keep_geom_type=True)\n",
    "ground_truth = gpd.clip(ground_truth, box(*results.total_bounds), keep_geom_type=True)\n",
    "\n",
    "# create spatial index for faster queries                         \n",
    "res_sindex = results.sindex\n",
    "gt_sindex = ground_truth.sindex\n",
    "\n",
    "# TP/FN check with different thresholds, applied to ground truth\n",
    "tp_cols = [f'TP_{np.round(i, 2)}' for i in np.arange(0.5, 1.04, 0.05)]\n",
    "ground_truth[tp_cols] = ground_truth.apply(lambda row: is_true_positive(row, results, res_sindex), \n",
    "                                           axis=1, result_type='expand')\n",
    "\n",
    "# TP/FP check with different thresholds, applied to predictions\n",
    "fp_cols = [f'FP_{np.round(i, 2)}' for i in np.arange(0.5, 1.01, 0.05)]\n",
    "results[fp_cols] = results.apply(lambda row: is_false_positive(row, ground_truth, gt_sindex, results, res_sindex), \n",
    "                                 axis=1, result_type='expand')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def poly_IoU(poly_1:Polygon, poly_2:Polygon) -> float:\n",
    "    \"IoU for polygons\"\n",
    "    area_intersection = poly_1.intersection(poly_2).area\n",
    "    area_union = poly_1.union(poly_2).area\n",
    "    iou = area_intersection / area_union\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def poly_dice(poly_1:Polygon, poly_2:Polygon):\n",
    "    \"Dice for polygons\"\n",
    "    area_intersection  = poly_1.intersection(poly_2).area\n",
    "    area_union = poly_1.union(poly_2).area\n",
    "    return (2 * area_intersection) / (poly_1.area + poly_2.area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def is_true_positive(row, results:gpd.GeoDataFrame, res_sindex:gpd.sindex):\n",
    "    \"Check if a single ground truth mask is TP or FN with 11 different IoU thresholds\"\n",
    "    iou_threshs = np.arange(0.5, 1.04, 0.05)\n",
    "    \n",
    "    # Matching predictions using spatial index\n",
    "    c = list(res_sindex.intersection(row.geometry.bounds))\n",
    "    possible_matches = results.iloc[c].copy()\n",
    "    \n",
    "    # No masks -> False negative\n",
    "    if len(possible_matches) == 0: return ['FN'] * len(iou_threshs)\n",
    "\n",
    "    possible_matches['iou'] = possible_matches.apply(lambda pred: poly_IoU(pred.geometry, row.geometry), axis=1)\n",
    "    \n",
    "    retvals = []\n",
    "    \n",
    "    for i, iou_thresh in enumerate(iou_threshs):\n",
    "        iou_thresh = np.round(iou_thresh, 2)\n",
    "        possible_matches = possible_matches[possible_matches.iou >= iou_thresh]\n",
    "        if len(possible_matches) == 0: return retvals + ['FN'] * (len(iou_threshs)-len(retvals))\n",
    "        \n",
    "        possible_matches.reset_index(inplace=True, drop=True)\n",
    "        max_iou_ix = possible_matches['iou'].idxmax()\n",
    "        max_score_id = possible_matches['score'].idxmax()\n",
    "        \n",
    "        if possible_matches.iloc[max_iou_ix].iou < iou_thresh: return ['FN'] * (len(iou_threshs) - len(retvals))\n",
    "        \n",
    "        if possible_matches.iloc[max_iou_ix].label != row.label: return ['FN'] * (len(iou_threshs) - len(retvals))\n",
    "        \n",
    "        retvals.append('TP')\n",
    "    return retvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def is_false_positive(row, ground_truth:gpd.GeoDataFrame, gt_sindex:gpd.sindex, \n",
    "                      results:gpd.GeoDataFrame, res_sindex:gpd.sindex):\n",
    "    \"Check if prediction is FP or TP for 11 different IoU thresholds\"\n",
    "    \n",
    "    iou_threshs = np.arange(0.5, 1.04, 0.05)\n",
    "    \n",
    "    # First find out the matching ground truth masks\n",
    "    c = list(gt_sindex.intersection(row.geometry.bounds))\n",
    "    possible_gt_matches = ground_truth.iloc[c].copy()\n",
    "    #possible_gt_matches = possible_matches[possible_matches.label == row.label].copy()\n",
    "    possible_gt_matches = possible_gt_matches.loc[possible_gt_matches.intersects(row.geometry)]\n",
    "    possible_gt_matches.reset_index(inplace=True)\n",
    "    \n",
    "    # No ground truth masks -> false positive\n",
    "    if len(possible_gt_matches) == 0: return ['FP'] * len(iou_threshs)\n",
    "    \n",
    "    retvals = []\n",
    "    \n",
    "    # Count IoU for all possible_gt_matches\n",
    "    possible_gt_matches['iou'] = possible_gt_matches.apply(lambda gt: poly_IoU(gt.geometry, row.geometry), axis=1)\n",
    "    \n",
    "    # Assume that largest IoU is the corresponding label\n",
    "    gt_ix = possible_gt_matches['iou'].idxmax()\n",
    "    \n",
    "    for i, iou_thresh in enumerate(iou_threshs):\n",
    "        iou_thresh = np.round(iou_thresh, 2)\n",
    "        # If IoU-threshold is too low -> false positive\n",
    "        if possible_gt_matches.iloc[gt_ix].iou < iou_thresh: \n",
    "            return retvals + ['FP'] * (len(iou_threshs)-len(retvals))\n",
    "\n",
    "\n",
    "        # If labels don't match -> false positive:\n",
    "        if possible_gt_matches.iloc[gt_ix].label != row.label: \n",
    "            return retvals + ['FP'] * (len(iou_threshs)-len(retvals))\n",
    "\n",
    "        # Then check whether there are other predictions\n",
    "        c = list(res_sindex.intersection(row.geometry.bounds))\n",
    "        possible_pred_matches = results.iloc[c].copy()\n",
    "\n",
    "        # Remove examined row from possible_matches. Assume that scores are always different (spoiler: they are not)\n",
    "        possible_pred_matches = possible_pred_matches[possible_pred_matches.score != row.score]\n",
    "\n",
    "        # No other possibilities -> not FP\n",
    "        if len(possible_pred_matches) == 0: \n",
    "            retvals.append('TP')\n",
    "            continue\n",
    "\n",
    "        possible_pred_matches['iou'] = possible_pred_matches.apply(lambda pred: poly_IoU(pred.geometry, \n",
    "                                                                                         possible_gt_matches.iloc[gt_ix].geometry), \n",
    "                                                                   axis=1)\n",
    "\n",
    "        possible_pred_matches = possible_pred_matches[possible_pred_matches.iou > iou_thresh]\n",
    "        possible_pred_matches.reset_index(inplace=True)\n",
    "\n",
    "        if len(possible_pred_matches) == 0: \n",
    "            retvals.append('TP')\n",
    "            continue\n",
    "\n",
    "        pred_max_iou_ix = possible_pred_matches['iou'].idxmax()\n",
    "        pred_max_score_ix = possible_pred_matches['score'].idxmax()\n",
    "\n",
    "        # Do any other possible predictions have larger score? If yes -> FP\n",
    "        if possible_pred_matches.iloc[pred_max_score_ix].score > row.score: \n",
    "            return retvals + ['FP'] * (len(iou_threshs)-len(retvals))\n",
    "        \n",
    "        retvals.append('TP')\n",
    "    \n",
    "    return retvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`average_precision` and `average_recall` both return `dict` of the results, with each label and each IoU threshold separately. Each item is 11 item list where each item correspond to a different recall threshold in the range of [0:.1:1] in the case of `average_precision`, or IoU threshold in the range of [.50:.05:1] in for `average_recall`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def average_precision(ground_truth:gpd.GeoDataFrame, preds:gpd.GeoDataFrame) -> dict:\n",
    "    \"Get 11-point AP score for each label separately and with all iou_thresholds\"\n",
    "    \n",
    "    # Clip geodataframes so that they cover the same area\n",
    "    preds = gpd.clip(preds, box(*ground_truth.total_bounds), keep_geom_type=True)\n",
    "    ground_truth = gpd.clip(ground_truth, box(*preds.total_bounds), keep_geom_type=True)\n",
    "    \n",
    "    gt_sindex = ground_truth.sindex\n",
    "    pred_sindex = preds.sindex\n",
    "    fp_cols = [f'FP_{np.round(i, 2)}' for i in np.arange(0.5, 1.04, 0.05)]\n",
    "    preds[fp_cols] = preds.apply(lambda row: is_false_positive(row, ground_truth, gt_sindex, preds, pred_sindex), \n",
    "                                 axis=1, result_type='expand')\n",
    "    iou_threshs = np.arange(0.5, 1.04, 0.05)\n",
    "    \n",
    "    res_dict = {}\n",
    "    for l in preds.label.unique():\n",
    "        for iou_thresh in iou_threshs:\n",
    "            iou_thresh = np.round(iou_thresh, 2)\n",
    "            res_dict[f'{l}_pre_{iou_thresh}'] = []\n",
    "            temp_preds = preds[preds.label == l].copy()\n",
    "            num_correct = len(ground_truth[ground_truth.label == l])\n",
    "            temp_preds.sort_values(by='score', ascending=False, inplace=True)\n",
    "            temp_preds.reset_index(inplace=True)\n",
    "            temp_preds['cumul_TP'] = 0.\n",
    "            temp_preds['precision'] = 0. \n",
    "            temp_preds['recall'] = 0.\n",
    "            temp_preds.loc[0, 'cumul_TP'] = 0 if temp_preds.loc[0, f'FP_{iou_thresh}'] == 'FP' else 1\n",
    "            temp_preds.loc[0, 'precision'] = temp_preds.loc[0,'cumul_TP'] / 1\n",
    "            temp_preds.loc[0, 'recall'] = temp_preds.loc[0,'cumul_TP'] / num_correct\n",
    "            for i in range(1, len(temp_preds)):\n",
    "                row_tp = 0 if temp_preds.loc[i, f'FP_{iou_thresh}'] == 'FP' else 1\n",
    "                temp_preds.loc[i, 'cumul_TP'] = temp_preds.loc[i-1, 'cumul_TP'] + row_tp\n",
    "                temp_preds.loc[i, 'precision'] = temp_preds.loc[i,'cumul_TP'] / (i+1)\n",
    "                temp_preds.loc[i, 'recall'] = temp_preds.loc[i,'cumul_TP'] / num_correct\n",
    "            recall_threshs = np.arange(0,1.04, 0.1)\n",
    "            for rec_thresh in recall_threshs:\n",
    "                pre = temp_preds[temp_preds.recall >= rec_thresh].precision.max()\n",
    "                res_dict[f'{l}_pre_{iou_thresh}'].append(0 if not np.isfinite(pre) else pre)  \n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "def average_recall(ground_truth:gpd.GeoDataFrame, preds:gpd.GeoDataFrame, max_detections:int=None) -> dict:\n",
    "    \"\"\"Get 11-point AR score for each label separately and with all iou_thresholds. \n",
    "    If `max_detections` is not `None` evaluate with only that most confident predictions\n",
    "    Seems to be still bugged, needs fixing\n",
    "    \"\"\"\n",
    "    \n",
    "    # Clip geodataframes so that they cover the same area\n",
    "    preds = gpd.clip(preds, box(*ground_truth.total_bounds), keep_geom_type=True)\n",
    "    ground_truth = gpd.clip(ground_truth, box(*preds.total_bounds), keep_geom_type=True)\n",
    "    \n",
    "    tp_cols = [f'TP_{np.round(i, 2)}' for i in np.arange(0.5, 1.03, 0.05)]\n",
    "    if max_detections is not None:\n",
    "        preds.sort_values(by='score', ascending=False, inplace=True)\n",
    "        preds = preds[:max_detections]\n",
    "        preds.reset_index(inplace=True)\n",
    "    pred_sindex = preds.sindex\n",
    "    ground_truth[tp_cols] = ground_truth.apply(lambda row: is_true_positive(row, preds, pred_sindex), \n",
    "                                               axis=1, result_type='expand')\n",
    "    iou_threshs = np.arange(0.5, 1.04, 0.05)\n",
    "    res_dict = {}\n",
    "    for l in ground_truth.label.unique():\n",
    "        res_dict[f'{l}_rec'] = []\n",
    "        for iou_thresh in iou_threshs:\n",
    "            iou_thresh = np.round(iou_thresh, 2)\n",
    "            temp_gt = ground_truth[ground_truth.label == l].copy()\n",
    "            res_dict[f'{l}_rec'].append(len(temp_gt[temp_gt[f'TP_{iou_thresh}'] == 'TP']) / len(temp_gt))\n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object detection metrics with pycocotools and gis-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `predict_instance_masks` or `predict_bboxes` for each scene separately, and save the resulting files in `data_path` containing \n",
    "* folder `raster_tiles` that contain the corresponding raster data. Required for transforming shapefiles to pixel coordinates\n",
    "* folder `vector_tiles` that contain ground truth masks\n",
    "* folder `predicted_vectors` that contain predictions\n",
    "\n",
    "All files corresponding to the same scene should have the same name, e.g. `raster_tiles/1053_Hiidenportti_Chunk9_orto.tif` for raster image, `vector_tiles/1053_Hiidenportti_Chunk9_orto.geojson` for ground truth and `predicted_vectors/1053_Hiidenportti_Chunk9_orto.geojson` for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from pycocotools.mask import decode\n",
    "from drone_detector.processing.coco import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class GisCOCOeval():\n",
    "    \n",
    "    def __init__(self, data_path:str, outpath:str, coco_info:dict, coco_licenses:list, coco_categories:list):\n",
    "        \"Initialize evaluator with data path and coco information\"\n",
    "        store_attr()\n",
    "        self.iou_threshs = [0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]\n",
    "        self.coco_proc = COCOProcessor(data_path=self.data_path, outpath=self.outpath, coco_info=self.coco_info,\n",
    "                                       coco_licenses=self.coco_licenses, coco_categories=self.coco_categories)\n",
    "        \n",
    "    def prepare_data(self, gt_label_col:str='label', res_label_col:str='label', rotated_bbox:bool=False):\n",
    "        \"Convert GIS-data predictions to COCO-format for evaluation, and save resulting files to self.outpath\"\n",
    "        self.coco_proc.shp_to_coco(label_col=gt_label_col, rotated_bbox=rotated_bbox)\n",
    "        self.coco_proc.to_coco_results(label_col=res_label_col, rotated_bbox=rotated_bbox)\n",
    "    \n",
    "    def prepare_eval(self, eval_type:str='segm'):\n",
    "        \"\"\"\n",
    "        Prepare COCOeval to evaluate predictions with 100 and 1000 detections. AP metrics are evaluated with 1000 detections and AR with 100\n",
    "        \"\"\"\n",
    "        self.coco = COCO(f'{self.outpath}/coco.json')\n",
    "        self.coco_res = self.coco.loadRes(f'{self.outpath}/coco_res.json')\n",
    "        self.coco_eval = COCOeval(self.coco, self.coco_res, eval_type)\n",
    "        self.coco_eval.params.maxDets = [100, 1000]\n",
    "        \n",
    "    def evaluate(self):\n",
    "        \"Run evaluation and print metrics\"\n",
    "        \n",
    "        for cat in self.coco_categories:\n",
    "            print(f'\\nEvaluating for category {cat[\"name\"]}')\n",
    "            self.coco_eval.params.catIds = [cat['id']]\n",
    "            self.coco_eval.evaluate()\n",
    "            self.coco_eval.accumulate()\n",
    "            _summarize_coco(self.coco_eval)\n",
    "        \n",
    "        self.coco_eval.params.catIds = self.coco.getCatIds()\n",
    "        print('\\nEvaluating for full data...')\n",
    "\n",
    "        self.coco_eval.evaluate()\n",
    "        self.coco_eval.accumulate()\n",
    "        _summarize_coco(self.coco_eval)\n",
    "    \n",
    "    def save_results(self, outpath, iou_thresh:float=0.5):\n",
    "        \"\"\"Saves correctly detected ground truths, correct detections missed ground truths and misclassifications with specified iou_threshold in separate files for each scene\"\"\"\n",
    "        \n",
    "        if not os.path.exists(f'{self.coco_proc.outpath}/{outpath}'):\n",
    "            os.makedirs(f'{self.coco_proc.outpath}/{outpath}')\n",
    "            os.makedirs(f'{self.coco_proc.outpath}/{outpath}/cor_gts')\n",
    "            os.makedirs(f'{self.coco_proc.outpath}/{outpath}/cor_dts')\n",
    "            os.makedirs(f'{self.coco_proc.outpath}/{outpath}/miss_gts')\n",
    "            os.makedirs(f'{self.coco_proc.outpath}/{outpath}/miss_dts')\n",
    "        \n",
    "        else: \n",
    "            print('Output directory exists')\n",
    "            return\n",
    "        \n",
    "        # Index from which get the Iou\n",
    "        iou_ix = self.iou_threshs.index(iou_thresh)\n",
    "\n",
    "        im_ids = self.coco.getImgIds()\n",
    "        cat_ids = self.coco.getCatIds()\n",
    "        anns = self.coco.anns\n",
    "        \n",
    "        cor_gt_res = {'images': self.coco.dataset['images'],\n",
    "                      'categories': self.coco.cats,\n",
    "                      'annotations': []}\n",
    "        \n",
    "        miss_gt_res = {'images': self.coco.dataset['images'],\n",
    "                       'categories': self.coco.cats,\n",
    "                       'annotations': []}\n",
    "        \n",
    "        cor_dt_res = {'images': self.coco.dataset['images'],\n",
    "                      'categories': self.coco.cats,\n",
    "                      'annotations': []}\n",
    "        \n",
    "        miss_dt_res = {'images': self.coco.dataset['images'],\n",
    "                       'categories': self.coco.cats,\n",
    "                       'annotations': []}\n",
    "        \n",
    "        # self.cocoeval.evalImgs has lenght of 4 * n_images * n_cats, and full results are in the ranges of \n",
    "        # [(4*n_images*(cat_id-1)):(4*n_images*(cat_id-1)+9)] \n",
    "        \n",
    "        for im_id, cat_id in tqdm(itertools.product(im_ids, cat_ids)):\n",
    "            eval_ix = 4*len(im_ids)*(cat_id-1) + im_id\n",
    "            res_dict = self.coco_eval.evalImgs[eval_ix]\n",
    "            \n",
    "            if res_dict is None:\n",
    "                continue\n",
    "                \n",
    "            gt_matches = np.unique(res_dict['dtMatches'][iou_ix]) # Detected ground truth ids in specified iou level\n",
    "            dt_matches = np.unique(res_dict['gtMatches'][iou_ix]) # Correct detection ids in specified iou level\n",
    "            gt_matches = gt_matches[gt_matches>0]\n",
    "            dt_matches = dt_matches[dt_matches>0]\n",
    "            \n",
    "            if gt_matches is None:\n",
    "                gt_matches = []\n",
    "                \n",
    "            gt_misses = [i for i in res_dict['gtIds'] if i not in gt_matches] # Missed ground truths\n",
    "            gt_match_anns = [self.coco.anns[i] for i in gt_matches]\n",
    "            gt_miss_anns = [self.coco.anns[i] for i in gt_misses]\n",
    "\n",
    "                \n",
    "            if dt_matches is None: \n",
    "                dt_matches = []\n",
    "                \n",
    "            dt_misses = [i for i in res_dict['dtIds'] if i not in dt_matches] # Misdetections\n",
    "            dt_match_anns = [self.coco_res.anns[i] for i in dt_matches]\n",
    "            dt_miss_anns = [self.coco_res.anns[i] for i in dt_misses]\n",
    "            \n",
    "        \n",
    "            for a in gt_match_anns:\n",
    "                ann = a.copy()\n",
    "                ann['segmentation'] = binary_mask_to_polygon(decode(a['segmentation']))\n",
    "                cor_gt_res['annotations'].append(ann)\n",
    "                \n",
    "            for a in dt_match_anns:\n",
    "                ann = a.copy()\n",
    "                ann['segmentation'] = binary_mask_to_polygon(decode(a['segmentation']))\n",
    "                cor_dt_res['annotations'].append(ann)\n",
    "                \n",
    "            for a in gt_miss_anns:\n",
    "                ann = a.copy()\n",
    "                ann['segmentation'] = binary_mask_to_polygon(decode(a['segmentation']))\n",
    "                miss_gt_res['annotations'].append(ann)\n",
    "                \n",
    "            for a in dt_miss_anns:\n",
    "                ann = a.copy()\n",
    "                ann['segmentation'] = binary_mask_to_polygon(decode(a['segmentation']))\n",
    "                miss_dt_res['annotations'].append(ann)\n",
    "               \n",
    "        self.coco_proc.coco_to_shp(cor_gt_res, f'{outpath}/cor_gts/')\n",
    "        self.coco_proc.coco_to_shp(cor_dt_res, f'{outpath}/cor_dts/')\n",
    "        self.coco_proc.coco_to_shp(miss_gt_res, f'{outpath}/miss_gts/')\n",
    "        self.coco_proc.coco_to_shp(miss_dt_res, f'{outpath}/miss_dts/')\n",
    "    \n",
    "def _summarize_coco(cocoeval:COCOeval): \n",
    "    \"\"\"\n",
    "    Compute and display summary metrics for evaluation results.\n",
    "    Note this functin can *only* be applied on the default parameter setting\n",
    "    \"\"\"\n",
    "    def _summarize(ap=1, iouThr=None, areaRng='all', maxDets=100):\n",
    "        p = cocoeval.params\n",
    "        iStr = ' {:<18} {} @[ IoU={:<9} | area={:>6s} | maxDets={:>3d} ] = {:0.3f}'\n",
    "        titleStr = 'Average Precision' if ap == 1 else 'Average Recall'\n",
    "        typeStr = '(AP)' if ap==1 else '(AR)'\n",
    "        iouStr = '{:0.2f}:{:0.2f}'.format(p.iouThrs[0], p.iouThrs[-1]) \\\n",
    "            if iouThr is None else '{:0.2f}'.format(iouThr)\n",
    "\n",
    "        aind = [i for i, aRng in enumerate(p.areaRngLbl) if aRng == areaRng]\n",
    "        mind = [i for i, mDet in enumerate(p.maxDets) if mDet == maxDets]\n",
    "        if ap == 1:\n",
    "            # dimension of precision: [TxRxKxAxM]\n",
    "            s = cocoeval.eval['precision']\n",
    "            # IoU\n",
    "            if iouThr is not None:\n",
    "                t = np.where(iouThr == p.iouThrs)[0]\n",
    "                s = s[t]\n",
    "            s = s[:,:,:,aind,mind]\n",
    "        else:\n",
    "            # dimension of recall: [TxKxAxM]\n",
    "            s = cocoeval.eval['recall']\n",
    "            if iouThr is not None:\n",
    "                t = np.where(iouThr == p.iouThrs)[0]\n",
    "                s = s[t]\n",
    "            s = s[:,:,aind,mind]\n",
    "        if len(s[s>-1])==0:\n",
    "            mean_s = -1\n",
    "        else:\n",
    "            mean_s = np.mean(s[s>-1])\n",
    "        print(iStr.format(titleStr, typeStr, iouStr, areaRng, maxDets, mean_s))\n",
    "        return mean_s\n",
    "    \n",
    "    def _summarizeDets():\n",
    "        stats = np.zeros((12,))\n",
    "        stats[0] = _summarize(1, maxDets=cocoeval.params.maxDets[1])\n",
    "        stats[1] = _summarize(1, iouThr=.5, maxDets=cocoeval.params.maxDets[1])\n",
    "        stats[2] = _summarize(1, iouThr=.75, maxDets=cocoeval.params.maxDets[1])\n",
    "        stats[3] = _summarize(1, areaRng='small', maxDets=cocoeval.params.maxDets[1])\n",
    "        stats[4] = _summarize(1, areaRng='medium', maxDets=cocoeval.params.maxDets[1])\n",
    "        stats[5] = _summarize(1, areaRng='large', maxDets=cocoeval.params.maxDets[1])\n",
    "        stats[6] = _summarize(0, maxDets=cocoeval.params.maxDets[0])\n",
    "        stats[9] = _summarize(0, areaRng='small', maxDets=cocoeval.params.maxDets[0])\n",
    "        stats[10] = _summarize(0, areaRng='medium', maxDets=cocoeval.params.maxDets[0])\n",
    "        stats[11] = _summarize(0, areaRng='large', maxDets=cocoeval.params.maxDets[0])\n",
    "        return stats\n",
    "    \n",
    "    def _summarizeKps():\n",
    "        stats = np.zeros((10,))\n",
    "        stats[0] = _summarize(1, maxDets=20)\n",
    "        stats[1] = _summarize(1, maxDets=20, iouThr=.5)\n",
    "        stats[2] = _summarize(1, maxDets=20, iouThr=.75)\n",
    "        stats[3] = _summarize(1, maxDets=20, areaRng='medium')\n",
    "        stats[4] = _summarize(1, maxDets=20, areaRng='large')\n",
    "        stats[5] = _summarize(0, maxDets=20)\n",
    "        stats[6] = _summarize(0, maxDets=20, iouThr=.5)\n",
    "        stats[7] = _summarize(0, maxDets=20, iouThr=.75)\n",
    "        stats[8] = _summarize(0, maxDets=20, areaRng='medium')\n",
    "        stats[9] = _summarize(0, maxDets=20, areaRng='large')\n",
    "        return stats\n",
    "    \n",
    "    if not cocoeval.eval:\n",
    "        raise Exception('Please run accumulate() first')\n",
    "        \n",
    "    iouType = cocoeval.params.iouType\n",
    "    \n",
    "    if iouType == 'segm' or iouType == 'bbox':\n",
    "        summarize = _summarizeDets\n",
    "    elif iouType == 'keypoints':\n",
    "        summarize = _summarizeKps\n",
    "    cocoeval.stats = summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "#### GisCOCOeval.prepare_data\n",
       "\n",
       ">      GisCOCOeval.prepare_data (gt_label_col:str='label',\n",
       ">                                res_label_col:str='label')\n",
       "\n",
       "Convert GIS-data predictions to COCO-format for evaluation, and save resulting files to self.outpath"
      ],
      "text/plain": [
       "<nbdev.showdoc.BasicMarkdownRenderer>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(GisCOCOeval.prepare_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "#### GisCOCOeval.prepare_eval\n",
       "\n",
       ">      GisCOCOeval.prepare_eval (eval_type:str='segm')\n",
       "\n",
       "Prepare COCOeval to evaluate predictions with 100 and 1000 detections. AP metrics are evaluated with 1000 detections and AR with 100"
      ],
      "text/plain": [
       "<nbdev.showdoc.BasicMarkdownRenderer>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(GisCOCOeval.prepare_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "#### GisCOCOeval.evaluate\n",
       "\n",
       ">      GisCOCOeval.evaluate ()\n",
       "\n",
       "Run evaluation and print metrics"
      ],
      "text/plain": [
       "<nbdev.showdoc.BasicMarkdownRenderer>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(GisCOCOeval.evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "#### GisCOCOeval.save_results\n",
       "\n",
       ">      GisCOCOeval.save_results (outpath, iou_thresh:float=0.5)\n",
       "\n",
       "Saves correctly detected ground truths, correct detections missed ground truths and misclassifications with specified iou_threshold in separate files for each scene"
      ],
      "text/plain": [
       "<nbdev.showdoc.BasicMarkdownRenderer>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(GisCOCOeval.save_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml-env]",
   "language": "python",
   "name": "conda-env-ml-env-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
