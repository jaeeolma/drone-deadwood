{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0471a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp engines.detectron2.tta\n",
    "#| nbflags skip_exec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b7c60b",
   "metadata": {},
   "source": [
    "# Custom TTA modifications for detectron2\n",
    "\n",
    "> Implement both HFlip and VFlip as TTA transforms, add support for RotatedBoxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f4a7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from detectron2.data.transforms import RandomFlip, ResizeShortestEdge, ResizeTransform, apply_augmentations \n",
    "from detectron2.config import configurable\n",
    "from copy import deepcopy\n",
    "from fvcore.transforms import VFlipTransform, HFlipTransform, NoOpTransform\n",
    "from detectron2.modeling import GeneralizedRCNNWithTTA, build_model\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "import torch\n",
    "from drone_detector.imports import *\n",
    "\n",
    "from detectron2.modeling.roi_heads.rotated_fast_rcnn import fast_rcnn_inference_single_image_rotated\n",
    "from detectron2.modeling.roi_heads.fast_rcnn import fast_rcnn_inference_single_image\n",
    "from detectron2.structures import Boxes, Instances\n",
    "\n",
    "from itertools import count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af599bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class DatasetMapperTTAFlip:\n",
    "    \"\"\"\n",
    "    Implement test-time augmentation for detection data. Modified to implement both horizontal and vertical flip\n",
    "    It is a callable which takes a dataset dict from a detection dataset,\n",
    "    and returns a list of dataset dicts where the images\n",
    "    are augmented from the input image by the transformations defined in the config.\n",
    "    This is used for test-time augmentation.\n",
    "    \"\"\"\n",
    "\n",
    "    @configurable\n",
    "    def __init__(self, min_sizes: List[int], max_size: int, flip: bool):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            min_sizes: list of short-edge size to resize the image to\n",
    "            max_size: maximum height or width of resized images\n",
    "            flip: whether to apply flipping augmentation\n",
    "        \"\"\"\n",
    "        self.min_sizes = min_sizes\n",
    "        self.max_size = max_size\n",
    "        self.flip = flip\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, cfg):\n",
    "        return {\n",
    "            \"min_sizes\": cfg.TEST.AUG.MIN_SIZES,\n",
    "            \"max_size\": cfg.TEST.AUG.MAX_SIZE,\n",
    "            \"flip\": cfg.TEST.AUG.FLIP,\n",
    "        }\n",
    "\n",
    "    def __call__(self, dataset_dict):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dict: a dict in standard model input format. See tutorials for details.\n",
    "        Returns:\n",
    "            list[dict]:\n",
    "                a list of dicts, which contain augmented version of the input image.\n",
    "                The total number of dicts is ``len(min_sizes) * (2 if flip else 1)``.\n",
    "                Each dict has field \"transforms\" which is a TransformList,\n",
    "                containing the transforms that are used to generate this image.\n",
    "        \"\"\"\n",
    "        numpy_image = dataset_dict[\"image\"].permute(1, 2, 0).numpy()\n",
    "        shape = numpy_image.shape\n",
    "        orig_shape = (dataset_dict[\"height\"], dataset_dict[\"width\"])\n",
    "        if shape[:2] != orig_shape:\n",
    "            # It transforms the \"original\" image in the dataset to the input image\n",
    "            pre_tfm = ResizeTransform(orig_shape[0], orig_shape[1], shape[0], shape[1])\n",
    "        else:\n",
    "            pre_tfm = NoOpTransform()\n",
    "\n",
    "        # Create all combinations of augmentations to use\n",
    "        aug_candidates = []  # each element is a list[Augmentation]\n",
    "        for min_size in self.min_sizes:\n",
    "            resize = ResizeShortestEdge(min_size, self.max_size)\n",
    "            aug_candidates.append([resize])\n",
    "            if self.flip:\n",
    "                hflip = RandomFlip(prob=1.0, horizontal=True, vertical=False)\n",
    "                aug_candidates.append([resize, hflip])\n",
    "                vflip =  RandomFlip(prob=1.0, horizontal=False, vertical=True)\n",
    "                aug_candidates.append([resize, vflip])\n",
    "\n",
    "        # Apply all the augmentations\n",
    "        ret = []\n",
    "        for aug in aug_candidates:\n",
    "            new_image, tfms = apply_augmentations(aug, np.copy(numpy_image))\n",
    "            torch_image = torch.from_numpy(np.ascontiguousarray(new_image.transpose(2, 0, 1)))\n",
    "\n",
    "            dic = deepcopy(dataset_dict)\n",
    "            dic[\"transforms\"] = pre_tfm + tfms\n",
    "            dic[\"image\"] = torch_image\n",
    "            ret.append(dic)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb70c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@patch_to(GeneralizedRCNNWithTTA)\n",
    "def _reduce_pred_masks(self, outputs, tfms):\n",
    "    \"Invert vflip and hflip transforms\"\n",
    "    for output, tfm in zip(outputs, tfms):\n",
    "        if any(isinstance(t, HFlipTransform) for t in tfm.transforms):\n",
    "            output.pred_masks = output.pred_masks.flip(dims=[3])\n",
    "        if any(isinstance(t, VFlipTransform) for t in tfm.transforms):\n",
    "            output.pred_masks = output.pred_masks.flip(dims=[2])\n",
    "    all_pred_masks = torch.stack([o.pred_masks for o in outputs], dim=0)\n",
    "    avg_pred_masks = torch.mean(all_pred_masks, dim=0)\n",
    "    return avg_pred_masks\n",
    "\n",
    "@patch_to(GeneralizedRCNNWithTTA)\n",
    "def _merge_detections(self, all_boxes, all_scores, all_classes, shape_hw):\n",
    "    # select from the union of all results\n",
    "    num_boxes = len(all_boxes)\n",
    "    num_classes = self.cfg.MODEL.ROI_HEADS.NUM_CLASSES\n",
    "    # +1 because fast_rcnn_inference expects background scores as well\n",
    "    all_scores_2d = torch.zeros(num_boxes, num_classes + 1, device=all_boxes.device)\n",
    "    for idx, cls, score in zip(count(), all_classes, all_scores):\n",
    "        all_scores_2d[idx, cls] = score\n",
    "\n",
    "    if all_boxes.size()[-1] == 5:\n",
    "        merged_instances, _ = fast_rcnn_inference_single_image_rotated(\n",
    "            all_boxes,\n",
    "            all_scores_2d,\n",
    "            shape_hw,\n",
    "            1e-8,\n",
    "            self.cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST,\n",
    "            self.cfg.TEST.DETECTIONS_PER_IMAGE,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        merged_instances, _ = fast_rcnn_inference_single_image(\n",
    "            all_boxes,\n",
    "            all_scores_2d,\n",
    "            shape_hw,\n",
    "            1e-8,\n",
    "            self.cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST,\n",
    "            self.cfg.TEST.DETECTIONS_PER_IMAGE,\n",
    "        )\n",
    "\n",
    "    return merged_instances\n",
    "\n",
    "@patch_to(GeneralizedRCNNWithTTA)\n",
    "def _rescale_detected_boxes(self, augmented_inputs, merged_instances, tfms):\n",
    "    augmented_instances = []\n",
    "    for input, tfm in zip(augmented_inputs, tfms):\n",
    "        # Transform the target box to the augmented image's coordinate space\n",
    "        pred_boxes = merged_instances.pred_boxes.tensor.cpu().numpy()\n",
    "        if pred_boxes.shape[-1] == 5:\n",
    "            pred_boxes = torch.from_numpy(tfm.apply_rotated_box(pred_boxes))\n",
    "        else:\n",
    "            pred_boxes = torch.from_numpy(tfm.apply_box(pred_boxes))\n",
    "\n",
    "        aug_instances = Instances(\n",
    "            image_size=input[\"image\"].shape[1:3],\n",
    "            pred_boxes=Boxes(pred_boxes),\n",
    "            pred_classes=merged_instances.pred_classes,\n",
    "            scores=merged_instances.scores,\n",
    "        )\n",
    "        augmented_instances.append(aug_instances)\n",
    "    return augmented_instances\n",
    "\n",
    "@patch_to(GeneralizedRCNNWithTTA)\n",
    "def _get_augmented_boxes(self, augmented_inputs, tfms):\n",
    "    # 1: forward with all augmented images\n",
    "    outputs = self._batch_inference(augmented_inputs)\n",
    "    # 2: union the results\n",
    "    all_boxes = []\n",
    "    all_scores = []\n",
    "    all_classes = []\n",
    "    for output, tfm in zip(outputs, tfms):\n",
    "        # Need to inverse the transforms on boxes, to obtain results on original image\n",
    "        pred_boxes = output.pred_boxes.tensor\n",
    "        if pred_boxes.size()[-1] == 5:\n",
    "            original_pred_boxes = tfm.inverse.apply_rotated_box(pred_boxes.cpu().numpy())\n",
    "        else:\n",
    "            original_pred_boxes = tfm.inverse().apply_box(pred_boxes.cpu().numpy())\n",
    "        all_boxes.append(torch.from_numpy(original_pred_boxes).to(pred_boxes.device))\n",
    "\n",
    "        all_scores.extend(output.scores)\n",
    "        all_classes.extend(output.pred_classes)\n",
    "    all_boxes = torch.cat(all_boxes, dim=0)\n",
    "    return all_boxes, all_scores, all_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cf5927",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class TTAPredictor:\n",
    "    \"\"\"DefaultPredictor that implements TTA\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg.clone()  # cfg can be modified by model\n",
    "        self.model = build_model(self.cfg)\n",
    "        checkpointer = DetectionCheckpointer(self.model)\n",
    "        checkpointer.load(cfg.MODEL.WEIGHTS)\n",
    "        self.model = GeneralizedRCNNWithTTA(cfg, self.model, tta_mapper=DatasetMapperTTAFlip(cfg))\n",
    "        self.model.eval()\n",
    "        if len(cfg.DATASETS.TEST):\n",
    "            self.metadata = MetadataCatalog.get(cfg.DATASETS.TEST[0])\n",
    "\n",
    "        self.aug = ResizeShortestEdge(\n",
    "            [cfg.INPUT.MIN_SIZE_TEST, cfg.INPUT.MIN_SIZE_TEST], cfg.INPUT.MAX_SIZE_TEST\n",
    "        )\n",
    "\n",
    "        self.input_format = cfg.INPUT.FORMAT\n",
    "        assert self.input_format in [\"RGB\", \"BGR\"], self.input_format\n",
    "\n",
    "    def __call__(self, original_image):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            original_image (np.ndarray): an image of shape (H, W, C) (in BGR order).\n",
    "\n",
    "        Returns:\n",
    "            predictions (dict):\n",
    "                the output of the model for one image only.\n",
    "                See :doc:`/tutorials/models` for details about the format.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():  # https://github.com/sphinx-doc/sphinx/issues/4258\n",
    "            # Apply pre-processing to image.\n",
    "            if self.input_format == \"RGB\":\n",
    "                # whether the model expects BGR inputs or RGB\n",
    "                original_image = original_image[:, :, ::-1]\n",
    "            height, width = original_image.shape[:2]\n",
    "            image = self.aug.get_transform(original_image).apply_image(original_image)\n",
    "            image = torch.as_tensor(image.astype(\"float32\").transpose(2, 0, 1))\n",
    "\n",
    "            inputs = {\"image\": image, \"height\": height, \"width\": width}\n",
    "            predictions = self.model([inputs])[0]\n",
    "            return predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
