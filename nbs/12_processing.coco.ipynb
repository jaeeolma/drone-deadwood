{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp processing.coco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COCO utilities\n",
    "\n",
    "> Make coco annotations from shapefiles and transform predictions to shapefiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from drone_detector.imports import *\n",
    "from drone_detector.utils import *\n",
    "from drone_detector.processing.coordinates import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import datetime\n",
    "from skimage import measure\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary masks to polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# From https://github.com/waspinator/pycococreator/blob/master/pycococreatortools/pycococreatortools.py\n",
    "\n",
    "def resize_binary_mask(array, new_size):\n",
    "    image = Image.fromarray(array.astype(np.uint8)*255)\n",
    "    image = image.resize(new_size)\n",
    "    return np.asarray(image).astype(np.bool_)\n",
    "\n",
    "def close_contour(contour):\n",
    "    if not np.array_equal(contour[0], contour[-1]):\n",
    "        contour = np.vstack((contour, contour[0]))\n",
    "    return contour\n",
    "\n",
    "def binary_mask_to_polygon(binary_mask, tolerance=0):\n",
    "    \"\"\"Converts a binary mask to COCO polygon representation\n",
    "    Args:\n",
    "        binary_mask: a 2D binary numpy array where '1's represent the object\n",
    "        tolerance: Maximum distance from original points of polygon to approximated\n",
    "            polygonal chain. If tolerance is 0, the original coordinate array is returned.\n",
    "    \"\"\"\n",
    "    polygons = []\n",
    "    # pad mask to close contours of shapes which start and end at an edge\n",
    "    padded_binary_mask = np.pad(binary_mask, pad_width=1, mode='constant', constant_values=0)\n",
    "    contours = measure.find_contours(padded_binary_mask, 0.5)\n",
    "    contours = np.subtract(contours, 1)\n",
    "    for contour in contours:\n",
    "        contour = close_contour(contour)\n",
    "        contour = measure.approximate_polygon(contour, tolerance)\n",
    "        if len(contour) < 3:\n",
    "            continue\n",
    "        contour = np.flip(contour, axis=1)\n",
    "        segmentation = contour.ravel().tolist()\n",
    "        # after padding and subtracting 1 we may get -0.5 points in our segmentation \n",
    "        segmentation = [0 if i < 0 else i for i in segmentation]\n",
    "        polygons.append(segmentation)\n",
    "\n",
    "    return polygons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COCOProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility to transform geospatial data to different COCO formats.\n",
    "\n",
    "Notes:\n",
    "* It is possible to specify `min_bbox_area` to `shp_to_coco` function to exclude too small polygons. Default value is 16 pixels\n",
    "* If a detection is a multipart polygon, only the polygon with the largest area is converted to a shapefile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "from pycocotools.mask import frPyObjects\n",
    "from shapely.geometry import MultiPolygon\n",
    "\n",
    "class COCOProcessor():\n",
    "    \"Handles Transformations from shapefiles to COCO-format and backwards\"\n",
    "    \n",
    "    def __init__(self, data_path:str, outpath:str, coco_info:dict, coco_licenses:list,\n",
    "                 coco_categories:list):\n",
    "        store_attr()\n",
    "        self.raster_path = f'{self.data_path}/raster_tiles'\n",
    "        self.vector_path = f'{self.data_path}/vector_tiles'\n",
    "        self.prediction_path = f'{self.data_path}/predicted_vectors'\n",
    "        \n",
    "        self.coco_dict = {\n",
    "            'info': coco_info,\n",
    "            'licenses': coco_licenses,\n",
    "            'images': [],\n",
    "            'annotations': [],\n",
    "            'categories': coco_categories,\n",
    "            'segment_info': []\n",
    "        }\n",
    "        self.categories = {c['name']:c['id'] for c in self.coco_dict['categories']}\n",
    "        \n",
    "        \n",
    "    def shp_to_coco(self, label_col:str='label', outfile:str='coco.json', min_bbox_area:int=16):\n",
    "        \"Process shapefiles from self.vector_path to coco-format and save to self.outpath/outfile\"\n",
    "        vector_tiles = [f for f in os.listdir(self.vector_path) if f.endswith(('.shp', '.geojson'))]\n",
    "        # If no annotations are in found in raster tile then there is no shapefile for that\n",
    "        raster_tiles = [f'{fname.split(\".\")[0]}.tif' for fname in vector_tiles]\n",
    "        ann_id = 1\n",
    "        for i, r in tqdm(enumerate(raster_tiles)):\n",
    "            tile_anns = []\n",
    "            gdf = gpd.read_file(f'{self.vector_path}/{vector_tiles[i]}')\n",
    "            tfmd_gdf = gdf_to_px(gdf, f'{self.raster_path}/{raster_tiles[i]}', precision=3)\n",
    "            for row in tfmd_gdf.itertuples():\n",
    "                category_id = self.categories[getattr(row, label_col)]\n",
    "                if box(*row.geometry.bounds).area < min_bbox_area: continue # if bounding box is smaller than 4Â² pixels then exclude it\n",
    "                tile_anns.append(_process_shp_to_coco(i, category_id, ann_id, row.geometry))\n",
    "                ann_id += 1\n",
    "            if len(tile_anns) > 0:\n",
    "                with rio.open(f'{self.raster_path}/{r}') as im: \n",
    "                    h, w = im.shape\n",
    "                self.coco_dict['images'].append({'file_name': raster_tiles[i],'id': i, 'height':h, 'width':w})\n",
    "                self.coco_dict['annotations'].extend(tile_anns)\n",
    "\n",
    "        with open(f'{self.outpath}/{outfile}', 'w') as f: json.dump(self.coco_dict, f)\n",
    "\n",
    "        return\n",
    "    \n",
    "    def coco_to_shp(self, coco_data:dict=None, outdir:str='predicted_vectors', downsample_factor:int=1):\n",
    "        \"\"\"Generates shapefiles from a dictionary with coco annotations.\n",
    "        TODO handle multipolygons better\"\"\"\n",
    "        \n",
    "        if not os.path.exists(f'{self.outpath}/{outdir}'): os.makedirs(f'{self.outpath}/{outdir}')\n",
    "        \n",
    "        annotations = coco_data['annotations']\n",
    "        images = coco_data['images']\n",
    "        categories = coco_data['categories']\n",
    "        for i in tqdm(images):\n",
    "            anns_in_image = [a for a in annotations if a['image_id'] == i['id']]\n",
    "            if len(anns_in_image) == 0: continue\n",
    "            cats = []\n",
    "            polys = []\n",
    "            scores = []\n",
    "            for a in anns_in_image:\n",
    "                # No segmentations, only bounding boxes\n",
    "                if a['segmentation'] is None:\n",
    "                    cats.append(a['category_id'])\n",
    "                    # Bbox has format xmin, ymin, xdelta, ydelta\n",
    "                    polys.append(box(a['bbox'][0] / downsample_factor, \n",
    "                                     a['bbox'][1] / downsample_factor, \n",
    "                                     (a['bbox'][2] + a['bbox'][0]) / downsample_factor, \n",
    "                                     (a['bbox'][3]+a['bbox'][1]) / downsample_factor))\n",
    "                    if 'score' in a.keys():\n",
    "                        scores.append(a['score'])\n",
    "                # Single polygon\n",
    "                elif len(a['segmentation']) == 1:\n",
    "                    cats.append(a['category_id'])\n",
    "                    xy_coords = [(a['segmentation'][0][i]  / downsample_factor, \n",
    "                                  a['segmentation'][0][i+1] / downsample_factor) \n",
    "                                 for i in range(0,len(a['segmentation'][0]),2)]\n",
    "                    xy_coords.append(xy_coords[-1])\n",
    "                    polys.append(Polygon(xy_coords))\n",
    "                    if 'score' in a.keys():\n",
    "                        scores.append(a['score'])\n",
    "                # Multipolygon \n",
    "                else: \n",
    "                    temp_poly = None\n",
    "                    max_area = 0\n",
    "                    cats.append(a['category_id'])\n",
    "                    for p in rangeof(a['segmentation']):\n",
    "                        xy_coords = [(a['segmentation'][p][i] / downsample_factor, \n",
    "                                      a['segmentation'][p][i+1] / downsample_factor) \n",
    "                                     for i in range(0,len(a['segmentation'][p]),2)]\n",
    "                        xy_coords.append(xy_coords[-1])\n",
    "                        if Polygon(xy_coords).area > max_area:\n",
    "                            temp_poly = Polygon(xy_coords)\n",
    "                            max_area = temp_poly.area\n",
    "                    polys.append(temp_poly)\n",
    "                    if 'score' in a.keys():\n",
    "                        scores.append(a['score'])\n",
    "            gdf = gpd.GeoDataFrame({'label':cats, 'geometry':polys})\n",
    "            if len(scores) != 0: gdf['score'] = scores\n",
    "            tfmd_gdf = georegister_px_df(gdf, f'{self.raster_path}/{i[\"file_name\"]}')\n",
    "            tfmd_gdf.to_file(f'{self.outpath}/{outdir}/{i[\"file_name\"][:-4]}.geojson', driver='GeoJSON')\n",
    "        return\n",
    "\n",
    "    def results_to_coco_res(self, label_col:str='label_id', outfile:str='coco_res.json'):\n",
    "        result_tiles = [f for f in os.listdir(self.prediction_path) if f.endswith(('.shp', '.geojson'))]\n",
    "        # If no annotations are in found in raster tile then there is no shapefile for that\n",
    "        raster_tiles = [f'{fname.split(\".\")[0]}.tif' for fname in result_tiles]\n",
    "        results = []\n",
    "        for i in tqdm(rangeof(raster_tiles)):\n",
    "            for im_id, im in enumerate(self.coco_dict['images']):\n",
    "                if im['file_name'] == raster_tiles[i]:\n",
    "                    break\n",
    "            image_id = self.coco_dict['images'][im_id]['id']\n",
    "            h = self.coco_dict['images'][im_id]['height']\n",
    "            w = self.coco_dict['images'][im_id]['width']\n",
    "            gdf = gpd.read_file(f'{self.prediction_path}/{result_tiles[i]}')\n",
    "            tfmd_gdf = gdf_to_px(gdf, f'{self.raster_path}/{raster_tiles[i]}', precision=3)\n",
    "            for row in tfmd_gdf.itertuples():\n",
    "                res = {'image_id': image_id,\n",
    "                       'category_id': getattr(row, label_col),\n",
    "                       'segmentation': None,\n",
    "                       'score': np.round(getattr(row, 'score'), 5)}\n",
    "                ann = _process_shp_to_coco(image_id, getattr(row, label_col), 0, row.geometry)\n",
    "                res['segmentation'] = frPyObjects(ann['segmentation'], h, w)[0]\n",
    "                res['segmentation']['counts'] = res['segmentation']['counts'].decode('ascii')\n",
    "                results.append(res)\n",
    "        \n",
    "        with open(f'{self.outpath}/{outfile}', 'w') as f: \n",
    "            json.dump(results, f)\n",
    "        \n",
    "def icevision_mask_preds_to_coco_anns(preds:list) -> dict:\n",
    "    \"\"\"Process list of IceVision `samples` and `preds` to COCO-annotation polygon format. \n",
    "    Returns a dict with Coco-style `images` and `annotations`\n",
    "    \n",
    "    TODO replace these with functions from icevision somehow\"\"\"\n",
    "    outdict = {}\n",
    "    outdict['annotations'] = []\n",
    "    outdict['images'] = [{'file_name': str(f'{p.ground_truth.filepath.stem}{p.ground_truth.filepath.suffix}'), 'id': p.record_id} for p in preds]\n",
    "    anns = []\n",
    "    for i, p in tqdm(enumerate(preds)): \n",
    "        for j in rangeof(p.pred.detection.label_ids):\n",
    "            anns = []\n",
    "            ann_dict = {\n",
    "                'segmentation': binary_mask_to_polygon(p.pred.detection.mask_array.to_mask(p.height,p.width).data[j]),\n",
    "                'area': None,  \n",
    "                'iscrowd': 0,\n",
    "                'category_id': p.pred.detection.label_ids[j].item(),\n",
    "                'id': i,\n",
    "                'image_id': p.record_id,\n",
    "                'bbox': [p.pred.detection.bboxes[j].xmin.item(), \n",
    "                         p.pred.detection.bboxes[j].ymin.item(),\n",
    "                         p.pred.detection.bboxes[j].xmax.item() - p.pred.detection.bboxes[j].xmin.item(),\n",
    "                         p.pred.detection.bboxes[j].ymax.item() - p.pred.detection.bboxes[j].ymin.item()],\n",
    "                'score': p.pred.detection.scores[j]\n",
    "            }\n",
    "\n",
    "            if len(ann_dict['segmentation']) == 0:\n",
    "                # Quickhack, find reason for empty annotation masks later\n",
    "                continue\n",
    "            anns.append(ann_dict)\n",
    "            outdict['annotations'].extend(anns)\n",
    "\n",
    "    return outdict\n",
    "\n",
    "def icevision_bbox_preds_to_coco_anns(preds:list) -> dict:\n",
    "    \"\"\"Process list of IceVision `samples` and `preds` to COCO-annotation polygon format. \n",
    "    Returns a dict with Coco-style `images` and `annotations`\"\"\"\n",
    "    outdict = {}\n",
    "    outdict['annotations'] = []\n",
    "    outdict['images'] = [{'file_name': str(f'{p.ground_truth.filepath.stem}{p.ground_truth.filepath.suffix}'), 'id': p.record_id} for p in preds]\n",
    "\n",
    "    anns = []\n",
    "    for i, p in tqdm(enumerate(preds)): \n",
    "        for j in rangeof(p.pred.detection.bboxes):\n",
    "            anns = []\n",
    "            ann_dict = {\n",
    "                'segmentation': None,\n",
    "                'area': None,  \n",
    "                'iscrowd': 0,\n",
    "                'category_id': p.pred.detection.label_ids[j].item(),\n",
    "                'id': i,\n",
    "                'image_id': p.record_id,\n",
    "                'bbox': [p.pred.detection.bboxes[j].xmin.item(), \n",
    "                         p.pred.detection.bboxes[j].ymin.item(),\n",
    "                         p.pred.detection.bboxes[j].xmax.item() - p.pred.detection.bboxes[j].xmin.item(),\n",
    "                         p.pred.detection.bboxes[j].ymax.item() - p.pred.detection.bboxes[j].ymin.item()],\n",
    "                'score': p.pred.detection.scores[j]\n",
    "            }\n",
    "\n",
    "            anns.append(ann_dict)\n",
    "            outdict['annotations'].extend(anns)\n",
    "\n",
    "    return outdict\n",
    "\n",
    "def detectron2_bbox_preds_to_coco_anns(images:list, preds:list):\n",
    "    \"\"\"Process detectron2 prediction to COCO-annotation polygon format. \n",
    "    Returns a dict with COCO-style `images` and `annotations`\n",
    "    \"\"\"\n",
    "    outdict = {}\n",
    "    outdict['annotations'] = []\n",
    "    outdict['images'] = images\n",
    "\n",
    "    for i in tqdm(rangeof(preds)):\n",
    "        p = preds[i]['instances']\n",
    "        for j in rangeof(p.pred_classes):\n",
    "            anns = []\n",
    "            ann_dict = {\n",
    "                'segmentation': None,\n",
    "                'area': None,  \n",
    "                'iscrowd': 0,\n",
    "                'category_id': p.pred_classes[j].item(),\n",
    "                'id': i+1,\n",
    "                'image_id': images[i]['id'],\n",
    "                'bbox': [p.pred_boxes[j].tensor[0,0].item(), \n",
    "                         p.pred_boxes[j].tensor[0,1].item(),\n",
    "                         p.pred_boxes[j].tensor[0,2].item() - p.pred_boxes[j].tensor[0,0].item(),\n",
    "                         p.pred_boxes[j].tensor[0,3].item() - p.pred_boxes[j].tensor[0,1].item()],\n",
    "                'score': p.scores[j].item()\n",
    "            }\n",
    "            if len(ann_dict['segmentation']) == 0:\n",
    "                # Quickhack, find reason for empty annotation masks later\n",
    "                continue\n",
    "            anns.append(ann_dict)\n",
    "            outdict['annotations'].extend(anns)\n",
    "\n",
    "    return outdict\n",
    "\n",
    "\n",
    "def detectron2_mask_preds_to_coco_anns(images:list, preds:list):\n",
    "    \"\"\"Process detectron2 prediction to COCO-annotation polygon format. \n",
    "    Returns a dict with COCO-style `images` and `annotations`\n",
    "    \"\"\"\n",
    "    outdict = {}\n",
    "    outdict['annotations'] = []\n",
    "    outdict['images'] = images\n",
    "\n",
    "    for i in tqdm(rangeof(preds)):\n",
    "        p = preds[i]['instances']\n",
    "        for j in rangeof(p.pred_classes):\n",
    "            anns = []\n",
    "            ann_dict = {\n",
    "                'segmentation': binary_mask_to_polygon(p.pred_masks[j].cpu().numpy()),\n",
    "                'area': None,  \n",
    "                'iscrowd': 0,\n",
    "                'category_id': p.pred_classes[j].item(),\n",
    "                'id': i+1,\n",
    "                'image_id': images[i]['id'],\n",
    "                'bbox': [p.pred_boxes[j].tensor[0,0].item(), \n",
    "                         p.pred_boxes[j].tensor[0,1].item(),\n",
    "                         p.pred_boxes[j].tensor[0,2].item() - p.pred_boxes[j].tensor[0,0].item(),\n",
    "                         p.pred_boxes[j].tensor[0,3].item() - p.pred_boxes[j].tensor[0,1].item()],\n",
    "                'score': p.scores[j].item()\n",
    "            }\n",
    "            if len(ann_dict['segmentation']) == 0:\n",
    "                # Quickhack, find reason for empty annotation masks later\n",
    "                continue\n",
    "            anns.append(ann_dict)\n",
    "            outdict['annotations'].extend(anns)\n",
    "\n",
    "    return outdict\n",
    "    \n",
    "def _process_shp_to_coco(image_id, category_id, ann_id, poly:Polygon):\n",
    "    \"TODO handle multipolygons\"\n",
    "    ann_dict = {\n",
    "        'segmentation': [],\n",
    "        'area': None, \n",
    "        'bbox': [],\n",
    "        'category_id': category_id,\n",
    "        'id' : ann_id,\n",
    "        'image_id': image_id,\n",
    "        'iscrowd': 0,\n",
    "    }\n",
    "\n",
    "    if poly.type == 'Polygon':\n",
    "        ann_dict['segmentation'] = [list(sum(poly.exterior.coords[:-1], ()))]\n",
    "        ann_dict['bbox'] = [(poly.bounds[0]), \n",
    "                        (poly.bounds[1]), \n",
    "                        (poly.bounds[2]-poly.bounds[0]), \n",
    "                        (poly.bounds[3]-poly.bounds[1])]\n",
    "        ann_dict['area'] = poly.area\n",
    "    elif poly.type == 'MultiPolygon':\n",
    "        temp_poly = None\n",
    "        max_area = 0\n",
    "        # Take only the largest polygon\n",
    "        for p in poly.geoms:\n",
    "            area = p.area\n",
    "            if area > max_area:\n",
    "                max_area = area\n",
    "                temp_poly = p\n",
    "        ann_dict['segmentation'] = [list(sum(temp_poly.exterior.coords[:-1], ()))]\n",
    "        ann_dict['bbox'] = [(temp_poly.bounds[0]), \n",
    "                            (temp_poly.bounds[1]), \n",
    "                            (temp_poly.bounds[2]-temp_poly.bounds[0]), \n",
    "                            (temp_poly.bounds[3]-temp_poly.bounds[1])]\n",
    "        ann_dict['area'] = temp_poly.area\n",
    "    return ann_dict\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
