{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5265fdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb289666",
   "metadata": {},
   "source": [
    "# Custom losses for segmentation and object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1a68e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from drone_detector.imports import *\n",
    "from fastai.learner import Metric\n",
    "from fastai.torch_core import *\n",
    "from fastai.metrics import *\n",
    "from fastai.losses import BaseLoss\n",
    "from fastcore.meta import *\n",
    "import sklearn.metrics as skm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba67097",
   "metadata": {},
   "source": [
    "## LovÃ¡sz-losses, from [https://github.com/bermanmaxim/LovaszSoftmax](https://github.com/bermanmaxim/LovaszSoftmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1cd837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\"\"\"\n",
    "Lovasz-Softmax and Jaccard hinge loss in PyTorch\n",
    "Maxim Berman 2018 ESAT-PSI KU Leuven (MIT License)\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "try:\n",
    "    from itertools import  ifilterfalse\n",
    "except ImportError: # py3k\n",
    "    from itertools import  filterfalse as ifilterfalse\n",
    "\n",
    "\n",
    "def lovasz_grad(gt_sorted):\n",
    "    \"\"\"\n",
    "    Computes gradient of the Lovasz extension w.r.t sorted errors\n",
    "    See Alg. 1 in paper\n",
    "    \"\"\"\n",
    "    p = len(gt_sorted)\n",
    "    gts = gt_sorted.sum()\n",
    "    intersection = gts - gt_sorted.float().cumsum(0)\n",
    "    union = gts + (1 - gt_sorted).float().cumsum(0)\n",
    "    jaccard = 1. - intersection / union\n",
    "    if p > 1: # cover 1-pixel case\n",
    "        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n",
    "    return jaccard\n",
    "\n",
    "\n",
    "def iou_binary(preds, labels, EMPTY=1., ignore=None, per_image=True):\n",
    "    \"\"\"\n",
    "    IoU for foreground class\n",
    "    binary: 1 foreground, 0 background\n",
    "    \"\"\"\n",
    "    if not per_image:\n",
    "        preds, labels = (preds,), (labels,)\n",
    "    ious = []\n",
    "    for pred, label in zip(preds, labels):\n",
    "        intersection = ((label == 1) & (pred == 1)).sum()\n",
    "        union = ((label == 1) | ((pred == 1) & (label != ignore))).sum()\n",
    "        if not union:\n",
    "            iou = EMPTY\n",
    "        else:\n",
    "            iou = float(intersection) / float(union)\n",
    "        ious.append(iou)\n",
    "    iou = mean(ious)    # mean accross images if per_image\n",
    "    return 100 * iou\n",
    "\n",
    "\n",
    "def iou(preds, labels, C, EMPTY=1., ignore=None, per_image=False):\n",
    "    \"\"\"\n",
    "    Array of IoU for each (non ignored) class\n",
    "    \"\"\"\n",
    "    if not per_image:\n",
    "        preds, labels = (preds,), (labels,)\n",
    "    ious = []\n",
    "    for pred, label in zip(preds, labels):\n",
    "        iou = []    \n",
    "        for i in range(C):\n",
    "            if i != ignore: # The ignored label is sometimes among predicted classes (ENet - CityScapes)\n",
    "                intersection = ((label == i) & (pred == i)).sum()\n",
    "                union = ((label == i) | ((pred == i) & (label != ignore))).sum()\n",
    "                if not union:\n",
    "                    iou.append(EMPTY)\n",
    "                else:\n",
    "                    iou.append(float(intersection) / float(union))\n",
    "        ious.append(iou)\n",
    "    ious = [mean(iou) for iou in zip(*ious)] # mean accross images if per_image\n",
    "    return 100 * np.array(ious)\n",
    "\n",
    "\n",
    "# --------------------------- HELPER FUNCTIONS ---------------------------\n",
    "def isnan(x):\n",
    "    return x != x\n",
    "    \n",
    "    \n",
    "def mean(l, ignore_nan=False, empty=0):\n",
    "    \"\"\"\n",
    "    nanmean compatible with generators.\n",
    "    \"\"\"\n",
    "    l = iter(l)\n",
    "    if ignore_nan:\n",
    "        l = ifilterfalse(isnan, l)\n",
    "    try:\n",
    "        n = 1\n",
    "        acc = next(l)\n",
    "    except StopIteration:\n",
    "        if empty == 'raise':\n",
    "            raise ValueError('Empty mean')\n",
    "        return empty\n",
    "    for n, v in enumerate(l, 2):\n",
    "        acc += v\n",
    "    if n == 1:\n",
    "        return acc\n",
    "    return acc / n\n",
    "\n",
    "def lovasz_hinge(logits, labels, per_image=True, ignore=None):\n",
    "    \"\"\"\n",
    "    Binary Lovasz hinge loss\n",
    "      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n",
    "      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n",
    "      per_image: compute the loss per image instead of per batch\n",
    "      ignore: void class id\n",
    "    \"\"\"\n",
    "    if per_image:\n",
    "        loss = mean(lovasz_hinge_flat(*flatten_binary_scores(log.unsqueeze(0), lab.unsqueeze(0), ignore))\n",
    "                          for log, lab in zip(logits, labels))\n",
    "    else:\n",
    "        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def lovasz_hinge_flat(logits, labels):\n",
    "    \"\"\"\n",
    "    Binary Lovasz hinge loss\n",
    "      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n",
    "      labels: [P] Tensor, binary ground truth labels (0 or 1)\n",
    "      ignore: label to ignore\n",
    "    \"\"\"\n",
    "    if len(labels) == 0:\n",
    "        # only void pixels, the gradients should be 0\n",
    "        return logits.sum() * 0.\n",
    "    signs = 2. * labels.float() - 1.\n",
    "    errors = (1. - logits * Variable(signs))\n",
    "    errors_sorted, perm = torch.sort(errors, dim=0, descending=True)\n",
    "    perm = perm.data\n",
    "    gt_sorted = labels[perm]\n",
    "    grad = lovasz_grad(gt_sorted)\n",
    "    loss = torch.dot(F.relu(errors_sorted), Variable(grad))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def flatten_binary_scores(scores, labels, ignore=None):\n",
    "    \"\"\"\n",
    "    Flattens predictions in the batch (binary case)\n",
    "    Remove labels equal to 'ignore'\n",
    "    \"\"\"\n",
    "    scores = scores.view(-1)\n",
    "    labels = labels.view(-1)\n",
    "    if ignore is None:\n",
    "        return scores, labels\n",
    "    valid = (labels != ignore)\n",
    "    vscores = scores[valid]\n",
    "    vlabels = labels[valid]\n",
    "    return vscores, vlabels\n",
    "\n",
    "def lovasz_softmax(probas, labels, classes='present', per_image=False, ignore=None):\n",
    "    \"\"\"\n",
    "    Multi-class Lovasz-Softmax loss\n",
    "      probas: [B, C, H, W] Variable, class probabilities at each prediction (between 0 and 1).\n",
    "              Interpreted as binary (sigmoid) output with outputs of size [B, H, W].\n",
    "      labels: [B, H, W] Tensor, ground truth labels (between 0 and C - 1)\n",
    "      classes: 'all' for all, 'present' for classes present in labels, or a list of classes to average.\n",
    "      per_image: compute the loss per image instead of per batch\n",
    "      ignore: void class labels\n",
    "    \"\"\"\n",
    "    if per_image:\n",
    "        loss = mean(lovasz_softmax_flat(*flatten_probas(prob.unsqueeze(0), lab.unsqueeze(0), ignore), classes=classes)\n",
    "                          for prob, lab in zip(probas, labels))\n",
    "    else:\n",
    "        loss = lovasz_softmax_flat(*flatten_probas(probas, labels, ignore), classes=classes)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def lovasz_softmax_flat(probas, labels, classes='present'):\n",
    "    \"\"\"\n",
    "    Multi-class Lovasz-Softmax loss\n",
    "      probas: [P, C] Variable, class probabilities at each prediction (between 0 and 1)\n",
    "      labels: [P] Tensor, ground truth labels (between 0 and C - 1)\n",
    "      classes: 'all' for all, 'present' for classes present in labels, or a list of classes to average.\n",
    "    \"\"\"\n",
    "    if probas.numel() == 0:\n",
    "        # only void pixels, the gradients should be 0\n",
    "        return probas * 0.\n",
    "    C = probas.size(1)\n",
    "    losses = []\n",
    "    class_to_sum = list(range(C)) if classes in ['all', 'present'] else classes\n",
    "    for c in class_to_sum:\n",
    "        fg = (labels == c).float() # foreground for class c\n",
    "        if (classes == 'present' and fg.sum() == 0):\n",
    "            continue\n",
    "        if C == 1:\n",
    "            if len(classes) > 1:\n",
    "                raise ValueError('Sigmoid output possible only with 1 class')\n",
    "            class_pred = probas[:, 0]\n",
    "        else:\n",
    "            class_pred = probas[:, c]\n",
    "        errors = (Variable(fg) - class_pred).abs()\n",
    "        errors_sorted, perm = torch.sort(errors, 0, descending=True)\n",
    "        perm = perm.data\n",
    "        fg_sorted = fg[perm]\n",
    "        losses.append(torch.dot(errors_sorted, Variable(lovasz_grad(fg_sorted))))\n",
    "    return mean(losses)\n",
    "\n",
    "\n",
    "def flatten_probas(probas, labels, ignore=None):\n",
    "    \"\"\"\n",
    "    Flattens predictions in the batch\n",
    "    \"\"\"\n",
    "    if probas.dim() == 3:\n",
    "        # assumes output of a sigmoid layer\n",
    "        B, H, W = probas.size()\n",
    "        probas = probas.view(B, 1, H, W)\n",
    "    B, C, H, W = probas.size()\n",
    "    probas = probas.permute(0, 2, 3, 1).contiguous().view(-1, C)  # B * H * W, C = P, C\n",
    "    labels = labels.view(-1)\n",
    "    if ignore is None:\n",
    "        return probas, labels\n",
    "    valid = (labels != ignore)\n",
    "    vprobas = probas[valid.nonzero().squeeze()]\n",
    "    vlabels = labels[valid]\n",
    "    return vprobas, vlabels\n",
    "\n",
    "def xloss(logits, labels, ignore=None):\n",
    "    \"\"\"\n",
    "    Cross entropy loss\n",
    "    \"\"\"\n",
    "    return F.cross_entropy(logits, Variable(labels), ignore_index=255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9358da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "\n",
    "class LovaszHingeLossFlat(BaseLoss):\n",
    "    \"Same as `LovaszHingeLoss` but flattens input and target\"\n",
    "    y_int = True\n",
    "    @use_kwargs_dict(keep=True, ignore=None)\n",
    "    def __init__(self, *args, axis=-1, **kwargs): super().__init__(LovaszHingeLoss, *args, axis=axis, is_2d=False, **kwargs)\n",
    "    def decodes(self, x): return x>0\n",
    "    def activation(self, x): return x\n",
    "\n",
    "    \n",
    "class LovaszHingeLoss(Module):\n",
    "    \"\"\"\n",
    "    Lovasz-Hinge loss from https://arxiv.org/abs/1705.08790, with per_image=True\n",
    "    \n",
    "    Todo\n",
    "    \n",
    "    Binary Lovasz hinge loss\n",
    "      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n",
    "      labels: [P] Tensor, binary ground truth labels (0 or 1)\n",
    "      ignore: label to ignore\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ignore=None):\n",
    "        store_attr()\n",
    "        \n",
    "    def forward(self, outputs, targets):\n",
    "        if self.ignore is not None:\n",
    "            valid = (targets != ignore)\n",
    "            outputs = outputs[valid]\n",
    "            targets = targets[valid]\n",
    "        if len(targets) == 0:\n",
    "            # only void pixels, the gradiens should be 0\n",
    "            return outputs.sum() * 0.\n",
    "        signs = 2. * targets.float() - 1.\n",
    "        errors = (1. - outputs * Variable(signs))\n",
    "        errors_sorted, perm = torch.sort(errors, dim=0, descending=True)\n",
    "        perm = perm.data\n",
    "        gt_sorted = targets[perm]\n",
    "        grad = lovasz_grad(gt_sorted)\n",
    "        loss = torch.dot(F.relu(errors_sorted), Variable(grad))\n",
    "        return loss\n",
    "    \n",
    "    def decodes(self, x): x>0\n",
    "    def activation(self,x): return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6be678",
   "metadata": {},
   "outputs": [],
   "source": [
    "lov_hinge = LovaszHingeLossFlat()\n",
    "outp = torch.randn(4,1,128,128)\n",
    "target = torch.randint(0, 2, (4,1,128,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edb2c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorBase(1.4343)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lov_hinge(outp, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8598b04e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4343)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lovasz_hinge(outp, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e657a90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class LovaszSigmoidLossFlat(BaseLoss):\n",
    "    \"Same as `LovaszSigmoidLoss` but flattens input and target\"\n",
    "    y_int = True\n",
    "    @use_kwargs_dict(keep=True, ignore=None)\n",
    "    def __init__(self, *args, axis=-1, **kwargs): super().__init__(LovaszSigmoidLoss, *args, axis=axis, is_2d=False, **kwargs)\n",
    "    def decodes(self, x): return x>0\n",
    "    def activation(self, x): return x\n",
    "    \n",
    "class LovaszSigmoidLoss(Module):\n",
    "    \"\"\"\n",
    "    Lovasz-Sigmoid loss from https://arxiv.org/abs/1705.08790, with per_image=False\n",
    "    \n",
    "    Todo\n",
    "    \n",
    "      probas: [P, C] Variable, logits at each prediction (between -\\infty and +\\infty)\n",
    "      labels: [P] Tensor, binary ground truth labels (0 or 1)\n",
    "      ignore: label to ignore\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ignore=None):\n",
    "        store_attr()\n",
    "        \n",
    "    def forward(self, outputs, targets):\n",
    "        if self.ignore is not None:\n",
    "            valid = (targets != ignore)\n",
    "            outputs = outputs[valid]\n",
    "            targets = targets[valid]\n",
    "        if len(targets) == 0:\n",
    "            # only void pixels, the gradiens should be 0\n",
    "            return outputs.sum() * 0.\n",
    "        outputs = torch.sigmoid(outputs)\n",
    "        fg = (targets == 1).float() # foregroud pixels\n",
    "        errors = (Variable(fg) - outputs).abs()\n",
    "        errors_sorted, perm = torch.sort(errors, 0, descending=True)\n",
    "        perm = perm.data\n",
    "        fg_sorted = fg[perm]\n",
    "        loss = torch.dot(errors_sorted, Variable(lovasz_grad(fg_sorted)))\n",
    "        return loss\n",
    "    \n",
    "    def decodes(self, x): return x>0.5\n",
    "    def activation(self, x): return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f84ee17",
   "metadata": {},
   "outputs": [],
   "source": [
    "lov_sigmoid = LovaszSigmoidLossFlat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d433afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorBase(0.5826)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lov_sigmoid(outp, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247af006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5826)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lovasz_softmax(torch.sigmoid(outp), target, classes=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a6569d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class LovaszSoftmaxLossFlat(BaseLoss):\n",
    "    \"Same as `LovaszSigmoidLoss` but flattens input and target\"\n",
    "    y_int = True\n",
    "    @use_kwargs_dict(keep=True, classes='present', ignore=None)\n",
    "    def __init__(self, *args, axis=1, **kwargs): super().__init__(LovaszSoftmaxLoss, *args, axis=axis, is_2d=True, \n",
    "                                                                   flatten=True,**kwargs)\n",
    "    def activation(self, out): return F.softmax(out, dim=self.axis)\n",
    "    def decodes(self, out): return out.argmax(dim=self.axis)\n",
    "    \n",
    "    \n",
    "class LovaszSoftmaxLoss(Module):\n",
    "    \"\"\"\n",
    "    Lovasz-Sigmoid loss from https://arxiv.org/abs/1705.08790, with per_image=False\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, classes='present', ignore=None):\n",
    "        store_attr()    \n",
    "        \n",
    "    def forward(self, outputs, targets):\n",
    "        if outputs.numel() == 0:\n",
    "            # only void pixels, the gradients should be 0\n",
    "            return outputs * 0.\n",
    "        outputs = F.softmax(outputs, dim=-1)\n",
    "        C = outputs.size(1)\n",
    "        losses = []\n",
    "        class_to_sum = list(range(C)) if self.classes in ['all', 'present'] else self.classes\n",
    "        for c in class_to_sum:\n",
    "            fg = (targets == c).float() # foreground for class c\n",
    "            if (self.classes == 'present' and fg.sum() == 0):\n",
    "                continue\n",
    "            if C == 1:\n",
    "                if len(self.classes) > 1:\n",
    "                    raise ValueError('Sigmoid output possible only with 1 class')\n",
    "                class_pred = outputs[:, 0]\n",
    "            else:\n",
    "                class_pred = outputs[:, c]\n",
    "            errors = (Variable(fg) - class_pred).abs()\n",
    "            errors_sorted, perm = torch.sort(errors, 0, descending=True)\n",
    "            perm = perm.data\n",
    "            fg_sorted = fg[perm]\n",
    "            losses.append(torch.dot(errors_sorted, Variable(lovasz_grad(fg_sorted))))\n",
    "        return mean(losses)\n",
    "    \n",
    "    def activation(self, out): return F.softmax(out, dim=1)\n",
    "    def decodes(self, out): return out.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9358407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorBase(0.8593)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lov_softmax = LovaszSoftmaxLossFlat()\n",
    "outp_multi = torch.randn(4,7,128,128)\n",
    "target_multi = torch.randint(0, 7, (4,1,128,128))\n",
    "lov_softmax(outp_multi, target_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfdc601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8593)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lovasz_softmax(F.softmax(outp_multi, dim=1), target_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405a7e5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorBase(0.8599)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lov_softmax_subset = LovaszSoftmaxLossFlat(classes=[1,2])\n",
    "lov_softmax_subset(outp_multi, target_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76654884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8599)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lovasz_softmax(F.softmax(outp_multi, dim=1), target_multi, classes=[1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaab0684",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae389495",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
